{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2023 CITS4012 Assignment\n",
        "*Make sure you change the file name with your student id.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please check the bottom of the this ipynb file*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\New\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Necessary Library Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import torch\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from nltk.tag import pos_tag\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Read in data files\n",
        "test_df = pd.read_csv(\"WikiQA-test.tsv\", sep=\"\\t\")\n",
        "train_df = pd.read_csv(\"WikiQA-train.tsv\", sep=\"\\t\")\n",
        "\n",
        "# TODO: ask about Word Match Feature: check whether the word appears in the question by using decapitalisation or lemmatization - how can we extract that for the question inputs\n",
        "# TODO: ask about NER tagging 3g and 4g as (3, g) and (4, g)\n",
        "# TODO: solve if word not in GloVe model\n",
        "\n",
        "# notes: \n",
        "\n",
        "# spacy word tokenizer can be used for the case of tagging 3g and 4g as (3, g) and (4, g), however it is far less\n",
        "# efficient - the data wrangling function takes about 210 seconds to run instead of 5 seconds when using spacy tokenize\n",
        "\n",
        "# phrases like 1570–1638 get converted to 15701638 after punctuation removal \n",
        "\n",
        "# it would be better to predict which sentence is the answer to the question since that is how the data ia stored - sentences are tagged as the answer\n",
        "# our model removes the concept of sentences by removing full stops and representing the whole document as a continuous string of tokens\n",
        "# an improvement would be to keep the sentences and extract which sentence contains the answer\n",
        "\n",
        "# improvement could be to remove articles (a, an, the)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dictionary for Contraction Removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "contraction_dict = {\"ID\": \"identify\", \"Im\": \"i am\", \"im\": \"i am\", \"Dont\": \"do not\", \"dont\": \"do not\", \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
        "                    \"diff'rent\": \"different\", \"3g\": \"3 g\", \"4g\": \"4 g\"}\n",
        "\n",
        "\n",
        "def replace_contractions(sentence):\n",
        "    words = sentence.split()\n",
        "    replaced_words = []\n",
        "    for word in words:\n",
        "        if word in contraction_dict:\n",
        "            replaced_words.append(contraction_dict[word])\n",
        "        else:\n",
        "            replaced_words.append(word)\n",
        "    replaced_sentence = \" \".join(replaced_words).lower()\n",
        "    return replaced_sentence\n",
        "\n",
        "\n",
        "def clean(string):\n",
        "    '''\n",
        "    Removes puncutation and replaces contractions from a string\n",
        "\n",
        "    string: string to clean\n",
        "\n",
        "    return: cleaned string\n",
        "    '''\n",
        "    clean_string = re.sub(r'[^\\w\\s\\']', '', string)\n",
        "    clean_string = replace_contractions(clean_string)\n",
        "    clean_string = re.sub(r'[^\\w\\s]', '', string)\n",
        "    return clean_string"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vocab Handling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_vocab_list(dfs:list):\n",
        "    '''\n",
        "    Creates index2word and word2index dictionaries for encoding from inputted dataframes. Vocab list \n",
        "    contains words from dataframe sentences and questions.\n",
        "    \n",
        "    dfs: list of dataframes from dataset.\n",
        "\n",
        "    returns: word2index and index2word dictionaries\n",
        "    '''\n",
        "    \n",
        "    words = set(['[PAD]', '[OOV]'])\n",
        "\n",
        "    for df in dfs:\n",
        "        unique_sentences = list(df['Sentence'].unique())\n",
        "        for sentence in unique_sentences:\n",
        "            tokens = word_tokenize(clean(sentence.lower()))\n",
        "            for token in tokens:\n",
        "                words.add(token)\n",
        "\n",
        "        unique_questions = list(df['Question'].unique())\n",
        "        for question in unique_questions:\n",
        "            tokens = word_tokenize(clean(question.lower()))\n",
        "            for token in tokens:\n",
        "                words.add(token)\n",
        "\n",
        "    word2index = {w: i+2 for i, w in enumerate(words)}\n",
        "    word2index['[PAD]'] = 1  \n",
        "    word2index['[OOV]'] = 0  \n",
        "\n",
        "    index2word = {index:word for word,index in word2index.items()}\n",
        "\n",
        "    return word2index, index2word\n",
        "\n",
        "word2index, index2word = create_vocab_list([train_df, test_df])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Input Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(tokens, word2index):\n",
        "    '''\n",
        "    Converts tokens into respective idsby mapping ach words using word2index. \n",
        "\n",
        "    tokens: list of tokens\n",
        "    word2index: word to index mapping\n",
        "\n",
        "    returns: list of mapped indexes\n",
        "    '''\n",
        "    \n",
        "    ids = [word2index[word] for word in tokens]\n",
        "\n",
        "    return ids"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Wrangling Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "# Read in data files\n",
        "test_data = pd.read_csv(\"WikiQA-test.tsv\", sep=\"\\t\")\n",
        "train_data = pd.read_csv(\"WikiQA-train.tsv\", sep=\"\\t\")\n",
        "\n",
        "def wrangle_data(df):\n",
        "    \"\"\"Handles data and splits dataframe into question, document and answer set\n",
        "\n",
        "    df: dataframe to split\n",
        "\n",
        "    return: question, document, and answer dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise variables containing all Questions, Documents and Answers\n",
        "    questions = []\n",
        "    documents = []\n",
        "    answers = {}\n",
        "\n",
        "    # Get unique questions from the dataframe\n",
        "    unique_questions = df['Question'].drop_duplicates()\n",
        "\n",
        "    # Iterate through questions dataframe to form the Questions data\n",
        "    for row in unique_questions:\n",
        "        \n",
        "        # Remove punctuation (except apostrophes for contraction removal)\n",
        "        cleaned_row = re.sub(r'[^\\w\\s\\']','', row)\n",
        "\n",
        "        # Replace contracted words\n",
        "        cleaned_row = replace_contractions(cleaned_row)\n",
        "\n",
        "        # Tokenise the question\n",
        "        question_tokens = word_tokenize(cleaned_row.lower())\n",
        "\n",
        "        # Append the question to the list of questions\n",
        "        questions.append(question_tokens)\n",
        "\n",
        "    # Iterate through dataframe and form Documents and Answer data\n",
        "    for index, row in df.iterrows():\n",
        "\n",
        "        # Initalise current document pairs list and previous doc number (only for first iteration)\n",
        "        if index == 0:\n",
        "            previous_doc_num = row['DocumentID']\n",
        "            current_doc_pairs = []\n",
        "\n",
        "        # If the current doc number is different to the previous doc number, then a new  \n",
        "        # document has been found so append the current document pairs to the list of documents     \n",
        "        # and reinitilise the current document pairs list\n",
        "        current_doc_num = row['DocumentID']\n",
        "        if current_doc_num != previous_doc_num:\n",
        "            documents.append(current_doc_pairs)\n",
        "            current_doc_pairs = []\n",
        "        \n",
        "        # Replace contracted words\n",
        "        cleaned_row = replace_contractions(row['Sentence'])\n",
        "\n",
        "        # Remove punctuation\n",
        "        cleaned_row = re.sub(r'[^\\w\\s]','', cleaned_row)\n",
        "\n",
        "        # Tokenise the current row's sentence\n",
        "        sent_tokens = word_tokenize(cleaned_row.lower())\n",
        "\n",
        "        # Assign token types according to 'Label' field (1 means the corresponding sentence\n",
        "        # is the answer/one of the answers)\n",
        "        # Token types:\n",
        "        #   0: token a part of the answer\n",
        "        #   1: start token of the answer\n",
        "        #   2: inner token of the answer\n",
        "        #   3: end token of the answer\n",
        "        if row['Label']:\n",
        "            # Answer to question is found\n",
        "            token_types = np.full(shape=len(sent_tokens), fill_value=2)\n",
        "            token_types[0] = 1\n",
        "            token_types[-1] = 3\n",
        "\n",
        "            # Add the tokenised answer to the answers dictionary. If an answer has already been found.\n",
        "            # add the tokens to the current answer.\n",
        "            if row['QuestionID'] in answers:\n",
        "                answers[row['QuestionID']] += sent_tokens \n",
        "            else:\n",
        "                answers[row['QuestionID']] = sent_tokens\n",
        "        else:\n",
        "            # Answer not found \n",
        "            token_types = np.zeros(len(sent_tokens), dtype=int)\n",
        "\n",
        "        # Combine token and corresponding types and append to current document list\n",
        "        token_type_pairs = list(zip(sent_tokens, token_types))\n",
        "        current_doc_pairs.append(token_type_pairs)\n",
        "\n",
        "        # Update previous document number \n",
        "        previous_doc_num = current_doc_num\n",
        "\n",
        "        # If the iterated row is the final row in the dataframe, append the current document\n",
        "        # to the documents array\n",
        "        if index == len(df)-1:\n",
        "            documents.append(current_doc_pairs)\n",
        "\n",
        "    return questions, documents, answers\n",
        "\n",
        "train_questions, train_documents, train_answers = wrangle_data(train_data)\n",
        "test_questions, test_documents, test_answers = wrangle_data(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wrangle_data(df):\n",
        "    \"\"\"\n",
        "    Handles data wrangling and returns a dataframe giving each question the corresponding document and\n",
        "    answer span set. Encodes the words into their index of the vocab list as well.\n",
        "\n",
        "    Args:\n",
        "        df (pandas dataframe): dataframe to wrangle\n",
        "\n",
        "    Returns: \n",
        "        dataframe containing questions with the corresponding document and answer span set (encoded).\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise variable containing Answer Spans (questions can have 0, 1, or multiple answers)\n",
        "    answer_spans = [[0,0]]\n",
        "\n",
        "    # Intialise df object that will be returned after wrangling\n",
        "    wrangled_df = pd.DataFrame(columns=['question_id', 'question', 'document', 'answer span'])\n",
        "\n",
        "\n",
        "    # Iterate through dataframe and form Documents and Answer Span data\n",
        "    for index, row in df.iterrows():\n",
        "        # print(index)\n",
        "\n",
        "        # Initalise current document tokens list, length and previous doc number (only for first iteration)\n",
        "        if index == 0:\n",
        "            previous_doc_num = row['DocumentID']\n",
        "            current_doc_length = 0\n",
        "            current_doc_tokens = []\n",
        "\n",
        "\n",
        "        # If the current doc number is different to the previous doc number, then a new  \n",
        "        # document has been found \n",
        "        current_doc_num = row['DocumentID']\n",
        "        if current_doc_num != previous_doc_num:\n",
        "\n",
        "            # Remove punctuation from question and replace contractions\n",
        "            cleaned_qn = clean(previous_qn)\n",
        "\n",
        "            # Tokenize the question \n",
        "            qn_tokens = word_tokenize(cleaned_qn.lower())\n",
        "                \n",
        "            for span in answer_spans:\n",
        "                # Create a new row with multiple values\n",
        "                # print(current_doc_tokens)\n",
        "                new_row = {'question_id': previous_qn_id, \n",
        "                           'question': [encode(qn_tokens, word2index)],\n",
        "                           'document':[encode(current_doc_tokens, word2index)],\n",
        "                           'answer span': [span]}\n",
        "                \n",
        "                # Convert the new row to a DataFrame\n",
        "                new_row_df = pd.DataFrame(new_row)\n",
        "\n",
        "                # Append the new row DataFrame to the original DataFrame\n",
        "                wrangled_df = pd.concat([wrangled_df, new_row_df], ignore_index=True)\n",
        "\n",
        "            # Reinitialise document tokens list, current document length, and answer spans\n",
        "            current_doc_tokens = []\n",
        "            current_doc_length = 0\n",
        "            answer_spans = [[0,0]]\n",
        "\n",
        "\n",
        "        # Remove punctuation from document sentence and replace contracted words from document sentence \n",
        "        cleaned_sent = clean(row['Sentence'])\n",
        "\n",
        "        # Tokenise the current row's sentence and append to the document tokens list\n",
        "        sent_tokens = word_tokenize(cleaned_sent.lower())\n",
        "        current_doc_tokens.extend(sent_tokens)\n",
        "\n",
        "\n",
        "        if row['Label']:\n",
        "            # Answer to question is found\n",
        "\n",
        "            # Find the span of the answer\n",
        "            span_start = current_doc_length\n",
        "            span_end = span_start + len(sent_tokens)\n",
        "\n",
        "            # Add answer span to current document's answer spans\n",
        "            if answer_spans[0] == [0,0]:\n",
        "                answer_spans[0] = [span_start, span_end]\n",
        "            else:\n",
        "                answer_spans.append([span_start, span_end])\n",
        "            \n",
        "            # Increase length of current document\n",
        "            current_doc_length += len(sent_tokens)\n",
        "\n",
        "        else:\n",
        "            # Answer not found \n",
        "\n",
        "            # Increase length of current document\n",
        "            current_doc_length += len(sent_tokens)\n",
        "\n",
        "        # Update previous document number and question\n",
        "        previous_doc_num = current_doc_num\n",
        "        previous_qn_id = row['QuestionID']\n",
        "        previous_qn = row['Question']\n",
        "\n",
        "        # If the iterated row is the final row in the dataframe, add the final question and \n",
        "        # document to the wrangled df\n",
        "        if index == len(df)-1:\n",
        "\n",
        "            # Remove punctuation from question and replaces contractions\n",
        "            cleaned_qn = clean(previous_qn)\n",
        "\n",
        "            # Tokenize the question \n",
        "            qn_tokens = word_tokenize(cleaned_qn.lower())\n",
        "\n",
        "            for span in answer_spans:\n",
        "                # Create a new row with multiple values\n",
        "                new_row = {'question_id': previous_qn_id,\n",
        "                           'question': [encode(qn_tokens, word2index)],\n",
        "                           'document': [encode(current_doc_tokens, word2index)],\n",
        "                           'answer span': [span]}\n",
        "\n",
        "                # Convert the new row to a DataFrame\n",
        "                new_row_df = pd.DataFrame(new_row)\n",
        "\n",
        "                # Append the new row DataFrame to the original DataFrame\n",
        "                wrangled_df = pd.concat([wrangled_df, new_row_df], ignore_index=True)\n",
        "\n",
        "    return wrangled_df\n",
        "\n",
        "wrangled_train_data = wrangle_data(train_df)\n",
        "wrangled_test_data = wrangle_data(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "      <th>document</th>\n",
              "      <th>answer span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q1</td>\n",
              "      <td>[12365, 15581, 18831, 36162, 14077]</td>\n",
              "      <td>[5820, 17206, 17579, 18831, 33492, 6930, 6430,...</td>\n",
              "      <td>[24, 37]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q2</td>\n",
              "      <td>[12365, 15581, 36119, 36206, 10682, 36119, 231...</td>\n",
              "      <td>[13851, 22737, 21061, 17360, 14472, 5820, 1866...</td>\n",
              "      <td>[0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q5</td>\n",
              "      <td>[12365, 38089, 11551, 1859, 3856]</td>\n",
              "      <td>[11551, 1859, 14472, 5820, 20933, 17017, 36213...</td>\n",
              "      <td>[0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q6</td>\n",
              "      <td>[12365, 32302, 14472, 36119, 5478, 20328, 1640...</td>\n",
              "      <td>[13851, 36119, 10231, 7959, 36119, 11658, 1068...</td>\n",
              "      <td>[0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q7</td>\n",
              "      <td>[12365, 5820, 14435, 25246, 4683, 39107, 19458...</td>\n",
              "      <td>[36119, 14435, 30015, 17267, 14472, 5820, 3245...</td>\n",
              "      <td>[0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2279</th>\n",
              "      <td>Q3042</td>\n",
              "      <td>[39440, 35936, 21824, 35709, 1739]</td>\n",
              "      <td>[21824, 1529, 28091, 21824, 35709, 1529, 14472...</td>\n",
              "      <td>[76, 94]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2280</th>\n",
              "      <td>Q3043</td>\n",
              "      <td>[38123, 14472, 32742, 10182, 30157]</td>\n",
              "      <td>[32742, 17732, 30157, 13851, 36119, 17278, 248...</td>\n",
              "      <td>[7, 42]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2281</th>\n",
              "      <td>Q3043</td>\n",
              "      <td>[38123, 14472, 32742, 10182, 30157]</td>\n",
              "      <td>[32742, 17732, 30157, 13851, 36119, 17278, 248...</td>\n",
              "      <td>[42, 70]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2282</th>\n",
              "      <td>Q3044</td>\n",
              "      <td>[38123, 14472, 36119, 695, 9313, 10682, 25140]</td>\n",
              "      <td>[37906, 380, 32898, 9313, 26246, 6488, 3431, 1...</td>\n",
              "      <td>[0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2283</th>\n",
              "      <td>Q3046</td>\n",
              "      <td>[38123, 14472, 11477, 18938, 37713, 18844, 6930]</td>\n",
              "      <td>[11477, 1640, 15621, 11360, 13851, 36119, 3757...</td>\n",
              "      <td>[0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2284 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     question_id                                           question  \\\n",
              "0             Q1                [12365, 15581, 18831, 36162, 14077]   \n",
              "1             Q2  [12365, 15581, 36119, 36206, 10682, 36119, 231...   \n",
              "2             Q5                  [12365, 38089, 11551, 1859, 3856]   \n",
              "3             Q6  [12365, 32302, 14472, 36119, 5478, 20328, 1640...   \n",
              "4             Q7  [12365, 5820, 14435, 25246, 4683, 39107, 19458...   \n",
              "...          ...                                                ...   \n",
              "2279       Q3042                 [39440, 35936, 21824, 35709, 1739]   \n",
              "2280       Q3043                [38123, 14472, 32742, 10182, 30157]   \n",
              "2281       Q3043                [38123, 14472, 32742, 10182, 30157]   \n",
              "2282       Q3044     [38123, 14472, 36119, 695, 9313, 10682, 25140]   \n",
              "2283       Q3046   [38123, 14472, 11477, 18938, 37713, 18844, 6930]   \n",
              "\n",
              "                                               document answer span  \n",
              "0     [5820, 17206, 17579, 18831, 33492, 6930, 6430,...    [24, 37]  \n",
              "1     [13851, 22737, 21061, 17360, 14472, 5820, 1866...      [0, 0]  \n",
              "2     [11551, 1859, 14472, 5820, 20933, 17017, 36213...      [0, 0]  \n",
              "3     [13851, 36119, 10231, 7959, 36119, 11658, 1068...      [0, 0]  \n",
              "4     [36119, 14435, 30015, 17267, 14472, 5820, 3245...      [0, 0]  \n",
              "...                                                 ...         ...  \n",
              "2279  [21824, 1529, 28091, 21824, 35709, 1529, 14472...    [76, 94]  \n",
              "2280  [32742, 17732, 30157, 13851, 36119, 17278, 248...     [7, 42]  \n",
              "2281  [32742, 17732, 30157, 13851, 36119, 17278, 248...    [42, 70]  \n",
              "2282  [37906, 380, 32898, 9313, 26246, 6488, 3431, 1...      [0, 0]  \n",
              "2283  [11477, 1640, 15621, 11360, 13851, 36119, 3757...      [0, 0]  \n",
              "\n",
              "[2284 rows x 4 columns]"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Have a look at the wrangled data\n",
        "wrangled_train_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Loader**\n",
        "\n",
        "A class for loading the during training and testing is created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class DataLoader:\n",
        "    '''\n",
        "    -Divides the dataframe in batches.\n",
        "    -Pads the contexts and questions dynamically for each batch by padding \n",
        "     the examples to the maximum-length sequence in that batch.\n",
        "    -Calculates masks for context and question.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, data, batch_size):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        '''\n",
        "        Creates batches of data and yields them.\n",
        "        \n",
        "        Each yield comprises of:\n",
        "        :padded_context: padded tensor of contexts for each batch \n",
        "        :padded_question: padded tensor of questions for each batch \n",
        "        :context_mask & question_mask: zero-mask for question and context\n",
        "        :answer_span: start and end index wrt context_ids\n",
        "        :context_text,answer_text: used while validation to calculate metrics\n",
        "        :ids: question_ids used in evaluation\n",
        "        '''\n",
        "        \n",
        "        for batch in self.data:\n",
        "\n",
        "            context_text = []\n",
        "            \n",
        "            max_context_len = max([len(document) for document in batch['document']])\n",
        "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
        "            \n",
        "            for ctx in batch['document']:\n",
        "                context_text.append(ctx)\n",
        "                \n",
        "            for i, ctx in enumerate(batch['document']):\n",
        "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
        "            \n",
        "            max_question_len = max([len(ques) for ques in batch['question']])\n",
        "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
        "            \n",
        "            for i, ques in enumerate(batch['question']):\n",
        "                padded_question[i,: len(ques)] = torch.LongTensor(ques)\n",
        "                \n",
        "            \n",
        "            answer_span = torch.LongTensor(list(batch['answer span']))\n",
        "            context_mask = torch.eq(padded_context, 1)\n",
        "            question_mask = torch.eq(padded_question, 1)\n",
        "            \n",
        "            qn_ids = list(batch['question_id'])\n",
        "             \n",
        "            yield (padded_context, padded_question, context_mask, \n",
        "                   question_mask, answer_span, context_text, qn_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(wrangled_train_data, 32)\n",
        "test_loader = DataLoader(wrangled_test_data, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14725\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "total = 0\n",
        "for qn in train_questions:\n",
        "    total += len(qn)\n",
        "print(total)\n",
        "train_questions\n",
        "\n",
        "docs = []\n",
        "total_2 = 0\n",
        "\n",
        "for sentence in test_questions:\n",
        "    text = \" \".join(sentence)\n",
        "    doc = nlp(text)\n",
        "    docs.append(doc)\n",
        "\n",
        "for i in range(len(test_questions)):\n",
        "    # for x in doc:\n",
        "    #     total_2 += 1\n",
        "    #     print(total_2, x)\n",
        "    if len(test_questions[i]) != len(docs[i]):\n",
        "        print(test_questions[i], len(test_questions[i]))\n",
        "        print([x for x in docs[i]], len(docs[i]))\n",
        "    # total_2 += len(train_questions[i])\n",
        "    # print(total_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QuestionID</th>\n",
              "      <th>Question</th>\n",
              "      <th>DocumentID</th>\n",
              "      <th>DocumentTitle</th>\n",
              "      <th>SentenceID</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q1</td>\n",
              "      <td>how are glacier caves formed?</td>\n",
              "      <td>D1</td>\n",
              "      <td>Glacier cave</td>\n",
              "      <td>D1-3</td>\n",
              "      <td>A glacier cave is a cave formed within the ice...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>Q16</td>\n",
              "      <td>how much is 1 tablespoon of water</td>\n",
              "      <td>D16</td>\n",
              "      <td>Tablespoon</td>\n",
              "      <td>D16-0</td>\n",
              "      <td>This tablespoon has a capacity of about 15 mL.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>Q16</td>\n",
              "      <td>how much is 1 tablespoon of water</td>\n",
              "      <td>D16</td>\n",
              "      <td>Tablespoon</td>\n",
              "      <td>D16-8</td>\n",
              "      <td>In the USA one tablespoon (measurement unit) i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>Q16</td>\n",
              "      <td>how much is 1 tablespoon of water</td>\n",
              "      <td>D16</td>\n",
              "      <td>Tablespoon</td>\n",
              "      <td>D16-9</td>\n",
              "      <td>In Australia one tablespoon (measurement unit)...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Q17</td>\n",
              "      <td>how much are the harry potter movies worth</td>\n",
              "      <td>D17</td>\n",
              "      <td>Harry Potter</td>\n",
              "      <td>D17-13</td>\n",
              "      <td>The series also originated much tie-in merchan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20292</th>\n",
              "      <td>Q3037</td>\n",
              "      <td>What is an economic feature?</td>\n",
              "      <td>D2802</td>\n",
              "      <td>Economics</td>\n",
              "      <td>D2802-9</td>\n",
              "      <td>At the turn of the 21st century, the expanding...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20307</th>\n",
              "      <td>Q3039</td>\n",
              "      <td>what is the average american income</td>\n",
              "      <td>D1876</td>\n",
              "      <td>Household income in the United States</td>\n",
              "      <td>D1876-6</td>\n",
              "      <td>U.S. median household income fell from $51,144...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20325</th>\n",
              "      <td>Q3042</td>\n",
              "      <td>When was Apple Computer founded</td>\n",
              "      <td>D2806</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>D2806-3</td>\n",
              "      <td>The company was founded on April 1, 1976, and ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20335</th>\n",
              "      <td>Q3043</td>\n",
              "      <td>what is section eight housing</td>\n",
              "      <td>D2807</td>\n",
              "      <td>Section 8 (housing)</td>\n",
              "      <td>D2807-1</td>\n",
              "      <td>Section 8 of the Housing Act of 1937 (), often...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20336</th>\n",
              "      <td>Q3043</td>\n",
              "      <td>what is section eight housing</td>\n",
              "      <td>D2807</td>\n",
              "      <td>Section 8 (housing)</td>\n",
              "      <td>D2807-2</td>\n",
              "      <td>It operates through several programs, the larg...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1039 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      QuestionID                                    Question DocumentID  \\\n",
              "3             Q1               how are glacier caves formed?         D1   \n",
              "75           Q16           how much is 1 tablespoon of water        D16   \n",
              "83           Q16           how much is 1 tablespoon of water        D16   \n",
              "84           Q16           how much is 1 tablespoon of water        D16   \n",
              "98           Q17  how much are the harry potter movies worth        D17   \n",
              "...          ...                                         ...        ...   \n",
              "20292      Q3037                What is an economic feature?      D2802   \n",
              "20307      Q3039         what is the average american income      D1876   \n",
              "20325      Q3042             When was Apple Computer founded      D2806   \n",
              "20335      Q3043               what is section eight housing      D2807   \n",
              "20336      Q3043               what is section eight housing      D2807   \n",
              "\n",
              "                               DocumentTitle SentenceID  \\\n",
              "3                               Glacier cave       D1-3   \n",
              "75                                Tablespoon      D16-0   \n",
              "83                                Tablespoon      D16-8   \n",
              "84                                Tablespoon      D16-9   \n",
              "98                              Harry Potter     D17-13   \n",
              "...                                      ...        ...   \n",
              "20292                              Economics    D2802-9   \n",
              "20307  Household income in the United States    D1876-6   \n",
              "20325                             Apple Inc.    D2806-3   \n",
              "20335                    Section 8 (housing)    D2807-1   \n",
              "20336                    Section 8 (housing)    D2807-2   \n",
              "\n",
              "                                                Sentence  Label  \n",
              "3      A glacier cave is a cave formed within the ice...      1  \n",
              "75        This tablespoon has a capacity of about 15 mL.      1  \n",
              "83     In the USA one tablespoon (measurement unit) i...      1  \n",
              "84     In Australia one tablespoon (measurement unit)...      1  \n",
              "98     The series also originated much tie-in merchan...      1  \n",
              "...                                                  ...    ...  \n",
              "20292  At the turn of the 21st century, the expanding...      1  \n",
              "20307  U.S. median household income fell from $51,144...      1  \n",
              "20325  The company was founded on April 1, 1976, and ...      1  \n",
              "20335  Section 8 of the Housing Act of 1937 (), often...      1  \n",
              "20336  It operates through several programs, the larg...      1  \n",
              "\n",
              "[1039 rows x 7 columns]"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for debugging - find all labels equal to 1\n",
        "train_data.loc[train_data['Label'] == 1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2.QA Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**TFIDF (Term Frequency Inverse Document Frequency)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_df(tokenized_data):\n",
        "    \"\"\" Calculate term frequencies for a given list of list of tokens\n",
        "\n",
        "    tokenized_data: tokenized dataset\n",
        "\n",
        "    return: dictionary of frequencies for each term in the dataset\n",
        "    \"\"\"\n",
        "    DF = {}\n",
        "\n",
        "    for token_vector in tokenized_data:\n",
        "        # get each unique word in the doc and count the number of occurences in the document\n",
        "        for term in np.unique(token_vector):\n",
        "            try:\n",
        "                DF[term] += 1\n",
        "            except:\n",
        "                DF[term] = 1\n",
        "\n",
        "    # print scores in descending order\n",
        "    sorted_dict = sorted(DF.items(), key = lambda x : x[1], reverse = True)\n",
        "    #print(sorted_dict)\n",
        "    \n",
        "    return DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate TFIDF scores\n",
        "\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def calculate_tf_idf(tokenized_data):\n",
        "\n",
        "    tf_idf = {}\n",
        "\n",
        "    # total number of documents\n",
        "    n = len(tokenized_data)\n",
        "\n",
        "    # calculate Document Frequencies\n",
        "    DF = calculate_df(tokenized_data)\n",
        "\n",
        "    doc_id = 0\n",
        "    # get each token vector\n",
        "    for token_vector in tokenized_data:\n",
        "        # initialise counter for the vector\n",
        "        counter = Counter(token_vector)\n",
        "        #calculate total number of words in the doc\n",
        "        total_num_words = len(token_vector)\n",
        "\n",
        "        # get each unique word in the doc\n",
        "        for term in np.unique(token_vector):\n",
        "\n",
        "            # calculate Term Frequency\n",
        "            tf = counter[term]/total_num_words\n",
        "\n",
        "            # calculate Document Frequency\n",
        "            df = DF[term]\n",
        "\n",
        "            # calculate Inverse Document Frequency\n",
        "            idf = math.log(n/(df+1))+1\n",
        "\n",
        "            # calcaulte TF-IDF\n",
        "            tf_idf[doc_id, term] = tf*idf\n",
        "\n",
        "        doc_id += 1\n",
        "\n",
        "    return tf_idf\n",
        "\n",
        "tf_idfs = calculate_tf_idf(train_questions)\n",
        "tf_idfs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**POS Tagging Dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\New\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tag import pos_tag_sents\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "pos2index = {}\n",
        "pos2index['NN'] = 0\n",
        "index = 1\n",
        "\n",
        "# Iterate through words in corpus and create POS tag lookup table\n",
        "for word in word2index.keys():\n",
        "\n",
        "    pos = pos_tag([word])[0][-1]\n",
        "\n",
        "    if pos not in pos2index:\n",
        "        pos2index[pos] = index\n",
        "        index += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pos2index"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Named Entity Recognition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! python -m spacy validate\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner2index = {}\n",
        "index = 0\n",
        "\n",
        "# Iterate through words in corpus and create NER tag lookup table\n",
        "for word in word2index.keys():\n",
        "\n",
        "    ner = nlp(word)[0].ent_type_\n",
        "\n",
        "    if ner not in ner2index:\n",
        "        ner2index[ner] = index\n",
        "        index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner2index"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**GloVe/Word2VeC word embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# model = api.load(\"word2vec-google-news-300\")\n",
        "embed_model = api.load(\"glove-wiki-gigaword-50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_input_embedding(embed_model, pos=False, ner=False):\n",
        "    '''\n",
        "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
        "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
        "    Also includes embeddings for NER and POS tags.\n",
        "    '''\n",
        "    word_dim = embed_model.vector_size\n",
        "    print(pos, ner)\n",
        "\n",
        "    if ner:\n",
        "        ner_dim = len(ner2index)\n",
        "    else:\n",
        "        ner_dim = 0\n",
        "\n",
        "    if pos:\n",
        "        pos_dim = len(pos2index)\n",
        "    else:\n",
        "        pos_dim = 0\n",
        "\n",
        "    weights_matrix = np.zeros((len(word2index) + 2, word_dim + ner_dim + pos_dim))\n",
        "\n",
        "    words_found = 0\n",
        "    for i, word in enumerate(word2index.keys()):\n",
        "        \n",
        "\n",
        "        if word in embed_model.key_to_index:\n",
        "            input_embedding = embed_model[word]\n",
        "            \n",
        "            if ner:\n",
        "                ner_tag_embedding = np.zeros(ner_dim)\n",
        "                if nlp(word)[0].ent_type_ in ner2index:\n",
        "                    ner_tag_embedding[ner2index[nlp(word)[0].ent_type_]] = 1.0\n",
        "                input_embedding = np.hstack((input_embedding, ner_tag_embedding))\n",
        "\n",
        "            if pos:\n",
        "                pos_tag_embedding = np.zeros(pos_dim)\n",
        "                if pos_tag([word])[0][-1] in pos2index:\n",
        "                    pos_tag_embedding[pos2index[pos_tag([word])[0][-1]]] = 1.0\n",
        "                input_embedding = np.hstack((input_embedding, pos_tag_embedding))\n",
        "\n",
        "            weights_matrix[i + 2] = input_embedding\n",
        "            words_found += 1\n",
        "\n",
        "    return weights_matrix, words_found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'the': 0,\n",
              " ',': 1,\n",
              " '.': 2,\n",
              " 'of': 3,\n",
              " 'to': 4,\n",
              " 'and': 5,\n",
              " 'in': 6,\n",
              " 'a': 7,\n",
              " '\"': 8,\n",
              " \"'s\": 9,\n",
              " 'for': 10,\n",
              " '-': 11,\n",
              " 'that': 12,\n",
              " 'on': 13,\n",
              " 'is': 14,\n",
              " 'was': 15,\n",
              " 'said': 16,\n",
              " 'with': 17,\n",
              " 'he': 18,\n",
              " 'as': 19,\n",
              " 'it': 20,\n",
              " 'by': 21,\n",
              " 'at': 22,\n",
              " '(': 23,\n",
              " ')': 24,\n",
              " 'from': 25,\n",
              " 'his': 26,\n",
              " \"''\": 27,\n",
              " '``': 28,\n",
              " 'an': 29,\n",
              " 'be': 30,\n",
              " 'has': 31,\n",
              " 'are': 32,\n",
              " 'have': 33,\n",
              " 'but': 34,\n",
              " 'were': 35,\n",
              " 'not': 36,\n",
              " 'this': 37,\n",
              " 'who': 38,\n",
              " 'they': 39,\n",
              " 'had': 40,\n",
              " 'i': 41,\n",
              " 'which': 42,\n",
              " 'will': 43,\n",
              " 'their': 44,\n",
              " ':': 45,\n",
              " 'or': 46,\n",
              " 'its': 47,\n",
              " 'one': 48,\n",
              " 'after': 49,\n",
              " 'new': 50,\n",
              " 'been': 51,\n",
              " 'also': 52,\n",
              " 'we': 53,\n",
              " 'would': 54,\n",
              " 'two': 55,\n",
              " 'more': 56,\n",
              " \"'\": 57,\n",
              " 'first': 58,\n",
              " 'about': 59,\n",
              " 'up': 60,\n",
              " 'when': 61,\n",
              " 'year': 62,\n",
              " 'there': 63,\n",
              " 'all': 64,\n",
              " '--': 65,\n",
              " 'out': 66,\n",
              " 'she': 67,\n",
              " 'other': 68,\n",
              " 'people': 69,\n",
              " \"n't\": 70,\n",
              " 'her': 71,\n",
              " 'percent': 72,\n",
              " 'than': 73,\n",
              " 'over': 74,\n",
              " 'into': 75,\n",
              " 'last': 76,\n",
              " 'some': 77,\n",
              " 'government': 78,\n",
              " 'time': 79,\n",
              " '$': 80,\n",
              " 'you': 81,\n",
              " 'years': 82,\n",
              " 'if': 83,\n",
              " 'no': 84,\n",
              " 'world': 85,\n",
              " 'can': 86,\n",
              " 'three': 87,\n",
              " 'do': 88,\n",
              " ';': 89,\n",
              " 'president': 90,\n",
              " 'only': 91,\n",
              " 'state': 92,\n",
              " 'million': 93,\n",
              " 'could': 94,\n",
              " 'us': 95,\n",
              " 'most': 96,\n",
              " '_': 97,\n",
              " 'against': 98,\n",
              " 'u.s.': 99,\n",
              " 'so': 100,\n",
              " 'them': 101,\n",
              " 'what': 102,\n",
              " 'him': 103,\n",
              " 'united': 104,\n",
              " 'during': 105,\n",
              " 'before': 106,\n",
              " 'may': 107,\n",
              " 'since': 108,\n",
              " 'many': 109,\n",
              " 'while': 110,\n",
              " 'where': 111,\n",
              " 'states': 112,\n",
              " 'because': 113,\n",
              " 'now': 114,\n",
              " 'city': 115,\n",
              " 'made': 116,\n",
              " 'like': 117,\n",
              " 'between': 118,\n",
              " 'did': 119,\n",
              " 'just': 120,\n",
              " 'national': 121,\n",
              " 'day': 122,\n",
              " 'country': 123,\n",
              " 'under': 124,\n",
              " 'such': 125,\n",
              " 'second': 126,\n",
              " 'then': 127,\n",
              " 'company': 128,\n",
              " 'group': 129,\n",
              " 'any': 130,\n",
              " 'through': 131,\n",
              " 'china': 132,\n",
              " 'four': 133,\n",
              " 'being': 134,\n",
              " 'down': 135,\n",
              " 'war': 136,\n",
              " 'back': 137,\n",
              " 'off': 138,\n",
              " 'south': 139,\n",
              " 'american': 140,\n",
              " 'minister': 141,\n",
              " 'police': 142,\n",
              " 'well': 143,\n",
              " 'including': 144,\n",
              " 'team': 145,\n",
              " 'international': 146,\n",
              " 'week': 147,\n",
              " 'officials': 148,\n",
              " 'still': 149,\n",
              " 'both': 150,\n",
              " 'even': 151,\n",
              " 'high': 152,\n",
              " 'part': 153,\n",
              " 'told': 154,\n",
              " 'those': 155,\n",
              " 'end': 156,\n",
              " 'former': 157,\n",
              " 'these': 158,\n",
              " 'make': 159,\n",
              " 'billion': 160,\n",
              " 'work': 161,\n",
              " 'our': 162,\n",
              " 'home': 163,\n",
              " 'school': 164,\n",
              " 'party': 165,\n",
              " 'house': 166,\n",
              " 'old': 167,\n",
              " 'later': 168,\n",
              " 'get': 169,\n",
              " 'another': 170,\n",
              " 'tuesday': 171,\n",
              " 'news': 172,\n",
              " 'long': 173,\n",
              " 'five': 174,\n",
              " 'called': 175,\n",
              " '1': 176,\n",
              " 'wednesday': 177,\n",
              " 'military': 178,\n",
              " 'way': 179,\n",
              " 'used': 180,\n",
              " 'much': 181,\n",
              " 'next': 182,\n",
              " 'monday': 183,\n",
              " 'thursday': 184,\n",
              " 'friday': 185,\n",
              " 'game': 186,\n",
              " 'here': 187,\n",
              " '?': 188,\n",
              " 'should': 189,\n",
              " 'take': 190,\n",
              " 'very': 191,\n",
              " 'my': 192,\n",
              " 'north': 193,\n",
              " 'security': 194,\n",
              " 'season': 195,\n",
              " 'york': 196,\n",
              " 'how': 197,\n",
              " 'public': 198,\n",
              " 'early': 199,\n",
              " 'according': 200,\n",
              " 'several': 201,\n",
              " 'court': 202,\n",
              " 'say': 203,\n",
              " 'around': 204,\n",
              " 'foreign': 205,\n",
              " '10': 206,\n",
              " 'until': 207,\n",
              " 'set': 208,\n",
              " 'political': 209,\n",
              " 'says': 210,\n",
              " 'market': 211,\n",
              " 'however': 212,\n",
              " 'family': 213,\n",
              " 'life': 214,\n",
              " 'same': 215,\n",
              " 'general': 216,\n",
              " '–': 217,\n",
              " 'left': 218,\n",
              " 'good': 219,\n",
              " 'top': 220,\n",
              " 'university': 221,\n",
              " 'going': 222,\n",
              " 'number': 223,\n",
              " 'major': 224,\n",
              " 'known': 225,\n",
              " 'points': 226,\n",
              " 'won': 227,\n",
              " 'six': 228,\n",
              " 'month': 229,\n",
              " 'dollars': 230,\n",
              " 'bank': 231,\n",
              " '2': 232,\n",
              " 'iraq': 233,\n",
              " 'use': 234,\n",
              " 'members': 235,\n",
              " 'each': 236,\n",
              " 'area': 237,\n",
              " 'found': 238,\n",
              " 'official': 239,\n",
              " 'sunday': 240,\n",
              " 'place': 241,\n",
              " 'go': 242,\n",
              " 'based': 243,\n",
              " 'among': 244,\n",
              " 'third': 245,\n",
              " 'times': 246,\n",
              " 'took': 247,\n",
              " 'right': 248,\n",
              " 'days': 249,\n",
              " 'local': 250,\n",
              " 'economic': 251,\n",
              " 'countries': 252,\n",
              " 'see': 253,\n",
              " 'best': 254,\n",
              " 'report': 255,\n",
              " 'killed': 256,\n",
              " 'held': 257,\n",
              " 'business': 258,\n",
              " 'west': 259,\n",
              " 'does': 260,\n",
              " 'own': 261,\n",
              " '%': 262,\n",
              " 'came': 263,\n",
              " 'law': 264,\n",
              " 'months': 265,\n",
              " 'women': 266,\n",
              " \"'re\": 267,\n",
              " 'power': 268,\n",
              " 'think': 269,\n",
              " 'service': 270,\n",
              " 'children': 271,\n",
              " 'bush': 272,\n",
              " 'show': 273,\n",
              " '/': 274,\n",
              " 'help': 275,\n",
              " 'chief': 276,\n",
              " 'saturday': 277,\n",
              " 'system': 278,\n",
              " 'john': 279,\n",
              " 'support': 280,\n",
              " 'series': 281,\n",
              " 'play': 282,\n",
              " 'office': 283,\n",
              " 'following': 284,\n",
              " 'me': 285,\n",
              " 'meeting': 286,\n",
              " 'expected': 287,\n",
              " 'late': 288,\n",
              " 'washington': 289,\n",
              " 'games': 290,\n",
              " 'european': 291,\n",
              " 'league': 292,\n",
              " 'reported': 293,\n",
              " 'final': 294,\n",
              " 'added': 295,\n",
              " 'without': 296,\n",
              " 'british': 297,\n",
              " 'white': 298,\n",
              " 'history': 299,\n",
              " 'man': 300,\n",
              " 'men': 301,\n",
              " 'became': 302,\n",
              " 'want': 303,\n",
              " 'march': 304,\n",
              " 'case': 305,\n",
              " 'few': 306,\n",
              " 'run': 307,\n",
              " 'money': 308,\n",
              " 'began': 309,\n",
              " 'open': 310,\n",
              " 'name': 311,\n",
              " 'trade': 312,\n",
              " 'center': 313,\n",
              " '3': 314,\n",
              " 'israel': 315,\n",
              " 'oil': 316,\n",
              " 'too': 317,\n",
              " 'al': 318,\n",
              " 'film': 319,\n",
              " 'win': 320,\n",
              " 'led': 321,\n",
              " 'east': 322,\n",
              " 'central': 323,\n",
              " '20': 324,\n",
              " 'air': 325,\n",
              " 'come': 326,\n",
              " 'chinese': 327,\n",
              " 'town': 328,\n",
              " 'leader': 329,\n",
              " 'army': 330,\n",
              " 'line': 331,\n",
              " 'never': 332,\n",
              " 'little': 333,\n",
              " 'played': 334,\n",
              " 'prime': 335,\n",
              " 'death': 336,\n",
              " 'companies': 337,\n",
              " 'least': 338,\n",
              " 'put': 339,\n",
              " 'forces': 340,\n",
              " 'past': 341,\n",
              " 'de': 342,\n",
              " 'half': 343,\n",
              " 'june': 344,\n",
              " 'saying': 345,\n",
              " 'know': 346,\n",
              " 'federal': 347,\n",
              " 'french': 348,\n",
              " 'peace': 349,\n",
              " 'earlier': 350,\n",
              " 'capital': 351,\n",
              " 'force': 352,\n",
              " 'great': 353,\n",
              " 'union': 354,\n",
              " 'near': 355,\n",
              " 'released': 356,\n",
              " 'small': 357,\n",
              " 'department': 358,\n",
              " 'every': 359,\n",
              " 'health': 360,\n",
              " 'japan': 361,\n",
              " 'head': 362,\n",
              " 'ago': 363,\n",
              " 'night': 364,\n",
              " 'big': 365,\n",
              " 'cup': 366,\n",
              " 'election': 367,\n",
              " 'region': 368,\n",
              " 'director': 369,\n",
              " 'talks': 370,\n",
              " 'program': 371,\n",
              " 'far': 372,\n",
              " 'today': 373,\n",
              " 'statement': 374,\n",
              " 'july': 375,\n",
              " 'although': 376,\n",
              " 'district': 377,\n",
              " 'again': 378,\n",
              " 'born': 379,\n",
              " 'development': 380,\n",
              " 'leaders': 381,\n",
              " 'council': 382,\n",
              " 'close': 383,\n",
              " 'record': 384,\n",
              " 'along': 385,\n",
              " 'county': 386,\n",
              " 'france': 387,\n",
              " 'went': 388,\n",
              " 'point': 389,\n",
              " 'must': 390,\n",
              " 'spokesman': 391,\n",
              " 'your': 392,\n",
              " 'member': 393,\n",
              " 'plan': 394,\n",
              " 'financial': 395,\n",
              " 'april': 396,\n",
              " 'recent': 397,\n",
              " 'campaign': 398,\n",
              " 'become': 399,\n",
              " 'troops': 400,\n",
              " 'whether': 401,\n",
              " 'lost': 402,\n",
              " 'music': 403,\n",
              " '15': 404,\n",
              " 'got': 405,\n",
              " 'israeli': 406,\n",
              " '30': 407,\n",
              " 'need': 408,\n",
              " '4': 409,\n",
              " 'lead': 410,\n",
              " 'already': 411,\n",
              " 'russia': 412,\n",
              " 'though': 413,\n",
              " 'might': 414,\n",
              " 'free': 415,\n",
              " 'hit': 416,\n",
              " 'rights': 417,\n",
              " '11': 418,\n",
              " 'information': 419,\n",
              " 'away': 420,\n",
              " '12': 421,\n",
              " '5': 422,\n",
              " 'others': 423,\n",
              " 'control': 424,\n",
              " 'within': 425,\n",
              " 'large': 426,\n",
              " 'economy': 427,\n",
              " 'press': 428,\n",
              " 'agency': 429,\n",
              " 'water': 430,\n",
              " 'died': 431,\n",
              " 'career': 432,\n",
              " 'making': 433,\n",
              " '...': 434,\n",
              " 'deal': 435,\n",
              " 'attack': 436,\n",
              " 'side': 437,\n",
              " 'seven': 438,\n",
              " 'better': 439,\n",
              " 'less': 440,\n",
              " 'september': 441,\n",
              " 'once': 442,\n",
              " 'clinton': 443,\n",
              " 'main': 444,\n",
              " 'due': 445,\n",
              " 'committee': 446,\n",
              " 'building': 447,\n",
              " 'conference': 448,\n",
              " 'club': 449,\n",
              " 'january': 450,\n",
              " 'decision': 451,\n",
              " 'stock': 452,\n",
              " 'america': 453,\n",
              " 'given': 454,\n",
              " 'give': 455,\n",
              " 'often': 456,\n",
              " 'announced': 457,\n",
              " 'television': 458,\n",
              " 'industry': 459,\n",
              " 'order': 460,\n",
              " 'young': 461,\n",
              " \"'ve\": 462,\n",
              " 'palestinian': 463,\n",
              " 'age': 464,\n",
              " 'start': 465,\n",
              " 'administration': 466,\n",
              " 'russian': 467,\n",
              " 'prices': 468,\n",
              " 'round': 469,\n",
              " 'december': 470,\n",
              " 'nations': 471,\n",
              " \"'m\": 472,\n",
              " 'human': 473,\n",
              " 'india': 474,\n",
              " 'defense': 475,\n",
              " 'asked': 476,\n",
              " 'total': 477,\n",
              " 'october': 478,\n",
              " 'players': 479,\n",
              " 'bill': 480,\n",
              " 'important': 481,\n",
              " 'southern': 482,\n",
              " 'move': 483,\n",
              " 'fire': 484,\n",
              " 'population': 485,\n",
              " 'rose': 486,\n",
              " 'november': 487,\n",
              " 'include': 488,\n",
              " 'further': 489,\n",
              " 'nuclear': 490,\n",
              " 'street': 491,\n",
              " 'taken': 492,\n",
              " 'media': 493,\n",
              " 'different': 494,\n",
              " 'issue': 495,\n",
              " 'received': 496,\n",
              " 'secretary': 497,\n",
              " 'return': 498,\n",
              " 'college': 499,\n",
              " 'working': 500,\n",
              " 'community': 501,\n",
              " 'eight': 502,\n",
              " 'groups': 503,\n",
              " 'despite': 504,\n",
              " 'level': 505,\n",
              " 'largest': 506,\n",
              " 'whose': 507,\n",
              " 'attacks': 508,\n",
              " 'germany': 509,\n",
              " 'august': 510,\n",
              " 'change': 511,\n",
              " 'church': 512,\n",
              " 'nation': 513,\n",
              " 'german': 514,\n",
              " 'station': 515,\n",
              " 'london': 516,\n",
              " 'weeks': 517,\n",
              " 'having': 518,\n",
              " '18': 519,\n",
              " 'research': 520,\n",
              " 'black': 521,\n",
              " 'services': 522,\n",
              " 'story': 523,\n",
              " '6': 524,\n",
              " 'europe': 525,\n",
              " 'sales': 526,\n",
              " 'policy': 527,\n",
              " 'visit': 528,\n",
              " 'northern': 529,\n",
              " 'lot': 530,\n",
              " 'across': 531,\n",
              " 'per': 532,\n",
              " 'current': 533,\n",
              " 'board': 534,\n",
              " 'football': 535,\n",
              " 'ministry': 536,\n",
              " 'workers': 537,\n",
              " 'vote': 538,\n",
              " 'book': 539,\n",
              " 'fell': 540,\n",
              " 'seen': 541,\n",
              " 'role': 542,\n",
              " 'students': 543,\n",
              " 'shares': 544,\n",
              " 'iran': 545,\n",
              " 'process': 546,\n",
              " 'agreement': 547,\n",
              " 'quarter': 548,\n",
              " 'full': 549,\n",
              " 'match': 550,\n",
              " 'started': 551,\n",
              " 'growth': 552,\n",
              " 'yet': 553,\n",
              " 'moved': 554,\n",
              " 'possible': 555,\n",
              " 'western': 556,\n",
              " 'special': 557,\n",
              " '100': 558,\n",
              " 'plans': 559,\n",
              " 'interest': 560,\n",
              " 'behind': 561,\n",
              " 'strong': 562,\n",
              " 'england': 563,\n",
              " 'named': 564,\n",
              " 'food': 565,\n",
              " 'period': 566,\n",
              " 'real': 567,\n",
              " 'authorities': 568,\n",
              " 'car': 569,\n",
              " 'term': 570,\n",
              " 'rate': 571,\n",
              " 'race': 572,\n",
              " 'nearly': 573,\n",
              " 'korea': 574,\n",
              " 'enough': 575,\n",
              " 'site': 576,\n",
              " 'opposition': 577,\n",
              " 'keep': 578,\n",
              " '25': 579,\n",
              " 'call': 580,\n",
              " 'future': 581,\n",
              " 'taking': 582,\n",
              " 'island': 583,\n",
              " '2008': 584,\n",
              " '2006': 585,\n",
              " 'road': 586,\n",
              " 'outside': 587,\n",
              " 'really': 588,\n",
              " 'century': 589,\n",
              " 'democratic': 590,\n",
              " 'almost': 591,\n",
              " 'single': 592,\n",
              " 'share': 593,\n",
              " 'leading': 594,\n",
              " 'trying': 595,\n",
              " 'find': 596,\n",
              " 'album': 597,\n",
              " 'senior': 598,\n",
              " 'minutes': 599,\n",
              " 'together': 600,\n",
              " 'congress': 601,\n",
              " 'index': 602,\n",
              " 'australia': 603,\n",
              " 'results': 604,\n",
              " 'hard': 605,\n",
              " 'hours': 606,\n",
              " 'land': 607,\n",
              " 'action': 608,\n",
              " 'higher': 609,\n",
              " 'field': 610,\n",
              " 'cut': 611,\n",
              " 'coach': 612,\n",
              " 'elections': 613,\n",
              " 'san': 614,\n",
              " 'issues': 615,\n",
              " 'executive': 616,\n",
              " 'february': 617,\n",
              " 'production': 618,\n",
              " 'areas': 619,\n",
              " 'river': 620,\n",
              " 'face': 621,\n",
              " 'using': 622,\n",
              " 'japanese': 623,\n",
              " 'province': 624,\n",
              " 'park': 625,\n",
              " 'price': 626,\n",
              " 'commission': 627,\n",
              " 'california': 628,\n",
              " 'father': 629,\n",
              " 'son': 630,\n",
              " 'education': 631,\n",
              " '7': 632,\n",
              " 'village': 633,\n",
              " 'energy': 634,\n",
              " 'shot': 635,\n",
              " 'short': 636,\n",
              " 'africa': 637,\n",
              " 'key': 638,\n",
              " 'red': 639,\n",
              " 'association': 640,\n",
              " 'average': 641,\n",
              " 'pay': 642,\n",
              " 'exchange': 643,\n",
              " 'eu': 644,\n",
              " 'something': 645,\n",
              " 'gave': 646,\n",
              " 'likely': 647,\n",
              " 'player': 648,\n",
              " 'george': 649,\n",
              " '2007': 650,\n",
              " 'victory': 651,\n",
              " '8': 652,\n",
              " 'low': 653,\n",
              " 'things': 654,\n",
              " '2010': 655,\n",
              " 'pakistan': 656,\n",
              " '14': 657,\n",
              " 'post': 658,\n",
              " 'social': 659,\n",
              " 'continue': 660,\n",
              " 'ever': 661,\n",
              " 'look': 662,\n",
              " 'chairman': 663,\n",
              " 'job': 664,\n",
              " '2000': 665,\n",
              " 'soldiers': 666,\n",
              " 'able': 667,\n",
              " 'parliament': 668,\n",
              " 'front': 669,\n",
              " 'himself': 670,\n",
              " 'problems': 671,\n",
              " 'private': 672,\n",
              " 'lower': 673,\n",
              " 'list': 674,\n",
              " 'built': 675,\n",
              " '13': 676,\n",
              " 'efforts': 677,\n",
              " 'dollar': 678,\n",
              " 'miles': 679,\n",
              " 'included': 680,\n",
              " 'radio': 681,\n",
              " 'live': 682,\n",
              " 'form': 683,\n",
              " 'david': 684,\n",
              " 'african': 685,\n",
              " 'increase': 686,\n",
              " 'reports': 687,\n",
              " 'sent': 688,\n",
              " 'fourth': 689,\n",
              " 'always': 690,\n",
              " 'king': 691,\n",
              " '50': 692,\n",
              " 'tax': 693,\n",
              " 'taiwan': 694,\n",
              " 'britain': 695,\n",
              " '16': 696,\n",
              " 'playing': 697,\n",
              " 'title': 698,\n",
              " 'middle': 699,\n",
              " 'meet': 700,\n",
              " 'global': 701,\n",
              " 'wife': 702,\n",
              " '2009': 703,\n",
              " 'position': 704,\n",
              " 'located': 705,\n",
              " 'clear': 706,\n",
              " 'ahead': 707,\n",
              " '2004': 708,\n",
              " '2005': 709,\n",
              " 'iraqi': 710,\n",
              " 'english': 711,\n",
              " 'result': 712,\n",
              " 'release': 713,\n",
              " 'violence': 714,\n",
              " 'goal': 715,\n",
              " 'project': 716,\n",
              " 'closed': 717,\n",
              " 'border': 718,\n",
              " 'body': 719,\n",
              " 'soon': 720,\n",
              " 'crisis': 721,\n",
              " 'division': 722,\n",
              " '&amp;': 723,\n",
              " 'served': 724,\n",
              " 'tour': 725,\n",
              " 'hospital': 726,\n",
              " 'kong': 727,\n",
              " 'test': 728,\n",
              " 'hong': 729,\n",
              " 'u.n.': 730,\n",
              " 'inc.': 731,\n",
              " 'technology': 732,\n",
              " 'believe': 733,\n",
              " 'organization': 734,\n",
              " 'published': 735,\n",
              " 'weapons': 736,\n",
              " 'agreed': 737,\n",
              " 'why': 738,\n",
              " 'nine': 739,\n",
              " 'summer': 740,\n",
              " 'wanted': 741,\n",
              " 'republican': 742,\n",
              " 'act': 743,\n",
              " 'recently': 744,\n",
              " 'texas': 745,\n",
              " 'course': 746,\n",
              " 'problem': 747,\n",
              " 'senate': 748,\n",
              " 'medical': 749,\n",
              " 'un': 750,\n",
              " 'done': 751,\n",
              " 'reached': 752,\n",
              " 'star': 753,\n",
              " 'continued': 754,\n",
              " 'investors': 755,\n",
              " 'living': 756,\n",
              " 'care': 757,\n",
              " 'signed': 758,\n",
              " '17': 759,\n",
              " 'art': 760,\n",
              " 'provide': 761,\n",
              " 'worked': 762,\n",
              " 'presidential': 763,\n",
              " 'gold': 764,\n",
              " 'obama': 765,\n",
              " 'morning': 766,\n",
              " 'dead': 767,\n",
              " 'opened': 768,\n",
              " \"'ll\": 769,\n",
              " 'event': 770,\n",
              " 'previous': 771,\n",
              " 'cost': 772,\n",
              " 'instead': 773,\n",
              " 'canada': 774,\n",
              " 'band': 775,\n",
              " 'teams': 776,\n",
              " 'daily': 777,\n",
              " '2001': 778,\n",
              " 'available': 779,\n",
              " 'drug': 780,\n",
              " 'coming': 781,\n",
              " '2003': 782,\n",
              " 'investment': 783,\n",
              " '’s': 784,\n",
              " 'michael': 785,\n",
              " 'civil': 786,\n",
              " 'woman': 787,\n",
              " 'training': 788,\n",
              " 'appeared': 789,\n",
              " '9': 790,\n",
              " 'involved': 791,\n",
              " 'indian': 792,\n",
              " 'similar': 793,\n",
              " 'situation': 794,\n",
              " '24': 795,\n",
              " 'los': 796,\n",
              " 'running': 797,\n",
              " 'fighting': 798,\n",
              " 'mark': 799,\n",
              " '40': 800,\n",
              " 'trial': 801,\n",
              " 'hold': 802,\n",
              " 'australian': 803,\n",
              " 'thought': 804,\n",
              " '!': 805,\n",
              " 'study': 806,\n",
              " 'fall': 807,\n",
              " 'mother': 808,\n",
              " 'met': 809,\n",
              " 'relations': 810,\n",
              " 'anti': 811,\n",
              " '2002': 812,\n",
              " 'song': 813,\n",
              " 'popular': 814,\n",
              " 'base': 815,\n",
              " 'tv': 816,\n",
              " 'ground': 817,\n",
              " 'markets': 818,\n",
              " 'ii': 819,\n",
              " 'newspaper': 820,\n",
              " 'staff': 821,\n",
              " 'saw': 822,\n",
              " 'hand': 823,\n",
              " 'hope': 824,\n",
              " 'operations': 825,\n",
              " 'pressure': 826,\n",
              " 'americans': 827,\n",
              " 'eastern': 828,\n",
              " 'st.': 829,\n",
              " 'legal': 830,\n",
              " 'asia': 831,\n",
              " 'budget': 832,\n",
              " 'returned': 833,\n",
              " 'considered': 834,\n",
              " 'love': 835,\n",
              " 'wrote': 836,\n",
              " 'stop': 837,\n",
              " 'fight': 838,\n",
              " 'currently': 839,\n",
              " 'charges': 840,\n",
              " 'try': 841,\n",
              " 'aid': 842,\n",
              " 'ended': 843,\n",
              " 'management': 844,\n",
              " 'brought': 845,\n",
              " 'cases': 846,\n",
              " 'decided': 847,\n",
              " 'failed': 848,\n",
              " 'network': 849,\n",
              " 'works': 850,\n",
              " 'gas': 851,\n",
              " 'turned': 852,\n",
              " 'fact': 853,\n",
              " 'vice': 854,\n",
              " 'ca': 855,\n",
              " 'mexico': 856,\n",
              " 'trading': 857,\n",
              " 'especially': 858,\n",
              " 'reporters': 859,\n",
              " 'afghanistan': 860,\n",
              " 'common': 861,\n",
              " 'looking': 862,\n",
              " 'space': 863,\n",
              " 'rates': 864,\n",
              " 'manager': 865,\n",
              " 'loss': 866,\n",
              " '2011': 867,\n",
              " 'justice': 868,\n",
              " 'thousands': 869,\n",
              " 'james': 870,\n",
              " 'rather': 871,\n",
              " 'fund': 872,\n",
              " 'thing': 873,\n",
              " 'republic': 874,\n",
              " 'opening': 875,\n",
              " 'accused': 876,\n",
              " 'winning': 877,\n",
              " 'scored': 878,\n",
              " 'championship': 879,\n",
              " 'example': 880,\n",
              " 'getting': 881,\n",
              " 'biggest': 882,\n",
              " 'performance': 883,\n",
              " 'sports': 884,\n",
              " '1998': 885,\n",
              " 'let': 886,\n",
              " 'allowed': 887,\n",
              " 'schools': 888,\n",
              " 'means': 889,\n",
              " 'turn': 890,\n",
              " 'leave': 891,\n",
              " 'no.': 892,\n",
              " 'robert': 893,\n",
              " 'personal': 894,\n",
              " 'stocks': 895,\n",
              " 'showed': 896,\n",
              " 'light': 897,\n",
              " 'arrested': 898,\n",
              " 'person': 899,\n",
              " 'either': 900,\n",
              " 'offer': 901,\n",
              " 'majority': 902,\n",
              " 'battle': 903,\n",
              " '19': 904,\n",
              " 'class': 905,\n",
              " 'evidence': 906,\n",
              " 'makes': 907,\n",
              " 'society': 908,\n",
              " 'products': 909,\n",
              " 'regional': 910,\n",
              " 'needed': 911,\n",
              " 'stage': 912,\n",
              " 'am': 913,\n",
              " 'doing': 914,\n",
              " 'families': 915,\n",
              " 'construction': 916,\n",
              " 'various': 917,\n",
              " '1996': 918,\n",
              " 'sold': 919,\n",
              " 'independent': 920,\n",
              " 'kind': 921,\n",
              " 'airport': 922,\n",
              " 'paul': 923,\n",
              " 'judge': 924,\n",
              " 'internet': 925,\n",
              " 'movement': 926,\n",
              " 'room': 927,\n",
              " 'followed': 928,\n",
              " 'original': 929,\n",
              " 'angeles': 930,\n",
              " 'italy': 931,\n",
              " '`': 932,\n",
              " 'data': 933,\n",
              " 'comes': 934,\n",
              " 'parties': 935,\n",
              " 'nothing': 936,\n",
              " 'sea': 937,\n",
              " 'bring': 938,\n",
              " '2012': 939,\n",
              " 'annual': 940,\n",
              " 'officer': 941,\n",
              " 'beijing': 942,\n",
              " 'present': 943,\n",
              " 'remain': 944,\n",
              " 'nato': 945,\n",
              " '1999': 946,\n",
              " '22': 947,\n",
              " 'remains': 948,\n",
              " 'allow': 949,\n",
              " 'florida': 950,\n",
              " 'computer': 951,\n",
              " '21': 952,\n",
              " 'contract': 953,\n",
              " 'coast': 954,\n",
              " 'created': 955,\n",
              " 'demand': 956,\n",
              " 'operation': 957,\n",
              " 'events': 958,\n",
              " 'islamic': 959,\n",
              " 'beat': 960,\n",
              " 'analysts': 961,\n",
              " 'interview': 962,\n",
              " 'helped': 963,\n",
              " 'child': 964,\n",
              " 'probably': 965,\n",
              " 'spent': 966,\n",
              " 'asian': 967,\n",
              " 'effort': 968,\n",
              " 'cooperation': 969,\n",
              " 'shows': 970,\n",
              " 'calls': 971,\n",
              " 'investigation': 972,\n",
              " 'lives': 973,\n",
              " 'video': 974,\n",
              " 'yen': 975,\n",
              " 'runs': 976,\n",
              " 'tried': 977,\n",
              " 'bad': 978,\n",
              " 'described': 979,\n",
              " '1994': 980,\n",
              " 'toward': 981,\n",
              " 'written': 982,\n",
              " 'throughout': 983,\n",
              " 'established': 984,\n",
              " 'mission': 985,\n",
              " 'associated': 986,\n",
              " 'buy': 987,\n",
              " 'growing': 988,\n",
              " 'green': 989,\n",
              " 'forward': 990,\n",
              " 'competition': 991,\n",
              " 'poor': 992,\n",
              " 'latest': 993,\n",
              " 'banks': 994,\n",
              " 'question': 995,\n",
              " '1997': 996,\n",
              " 'prison': 997,\n",
              " 'feel': 998,\n",
              " 'attention': 999,\n",
              " ...}"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_model.key_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True False\n"
          ]
        }
      ],
      "source": [
        "weights_matrix, words_found = create_input_embedding(embed_model, pos = True, ner = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33204"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words_found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36119"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2index['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yes\n"
          ]
        }
      ],
      "source": [
        "if 'yes' in embed_model.key_to_index:\n",
        "    print('yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
              "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
              "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
              "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
              "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
              "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
              "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
              "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
              "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
              "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_model['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(39552, 77)\n"
          ]
        }
      ],
      "source": [
        "print(weights_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.25933999 -0.07604     0.37358001 -1.03009999  0.21642999  0.85126001\n",
            " -0.60244    -0.31900001 -0.33676001  0.60354    -0.10049    -1.13909996\n",
            "  0.43123001  0.0043515   0.72029001 -0.41692999 -0.42166999  0.27452001\n",
            " -0.077404   -0.057161    0.13315     1.3369      0.56550002  1.5079\n",
            "  0.049057   -0.72298002  0.082693   -0.61088002  0.26642999 -0.53430003\n",
            "  1.66630006  0.93687999  0.12852    -0.88971001  0.53053999  0.33317\n",
            " -0.23241    -0.63954002 -0.021537   -0.026442   -0.57050002 -0.017097\n",
            "  0.78815001  0.58354998  1.44850004 -0.53219998  0.047523   -0.65527999\n",
            "  0.37983    -0.075898    0.          0.          0.          1.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.        ]\n"
          ]
        }
      ],
      "source": [
        "print(weights_matrix[31499])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save('weights.matrix', weights_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "39550"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(word2index)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Concatenate Word Embeddings and Features** (unused)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "def concat_inputs(data):\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for index, sentence in enumerate(data):\n",
        "        sent_embedding = []\n",
        "\n",
        "        # Iterate through tokens\n",
        "        for token in sentence:\n",
        "            token_embedding = []\n",
        "\n",
        "            # If the word is in the word embedding model's vocabulary then append its embedding,\n",
        "            # otherwise an OOV vector of [0.0] * embedding dimension is appended (for simplicity)\n",
        "            if token in embed_model.key_to_index:\n",
        "                token_embedding.append([embed_model[token]])\n",
        "            else:\n",
        "                token_embedding.append(np.zeros(50))\n",
        "\n",
        "            # Look up and append tf_idf score to the embedding using the question number and word as the key \n",
        "            token_embedding.append(tf_idfs[(index, token)])\n",
        "\n",
        "            # Append the pos tag to the input embedding\n",
        "            token_embedding.append(pos_tag([token])[0][-1])\n",
        "\n",
        "            # Append the token embedding to the sentence embedding\n",
        "            sent_embedding.append(token_embedding)\n",
        "\n",
        "        # Append the sentence embedding to the list of all embeddings\n",
        "        embeddings.append(sent_embedding)\n",
        "    \n",
        "    return np.array(embeddings, dtype=object)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 - Layers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Feature Embedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AlignQuestionEmbedding(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim):        \n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.linear = nn.Linear(input_dim, input_dim)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, context, question, question_mask):\n",
        "        \n",
        "        # context = [bs, ctx_len, emb_dim]\n",
        "        # question = [bs, qtn_len, emb_dim]\n",
        "        # question_mask = [bs, qtn_len]\n",
        "    \n",
        "        ctx_ = self.linear(context)\n",
        "        ctx_ = self.relu(ctx_)\n",
        "        # ctx_ = [bs, ctx_len, emb_dim]\n",
        "        \n",
        "        qtn_ = self.linear(question)\n",
        "        qtn_ = self.relu(qtn_)\n",
        "        # qtn_ = [bs, qtn_len, emb_dim]\n",
        "        \n",
        "        qtn_transpose = qtn_.permute(0,2,1)\n",
        "        # qtn_transpose = [bs, emb_dim, qtn_len]\n",
        "        \n",
        "        align_scores = torch.bmm(ctx_, qtn_transpose)\n",
        "        # align_scores = [bs, ctx_len, qtn_len]\n",
        "        \n",
        "        qtn_mask = question_mask.unsqueeze(1).expand(align_scores.size())\n",
        "        # qtn_mask = [bs, 1, qtn_len] => [bs, ctx_len, qtn_len]\n",
        "        \n",
        "        # Fills elements of self tensor(align_scores) with value(-float(inf)) where mask is True. \n",
        "        # The shape of mask must be broadcastable with the shape of the underlying tensor.\n",
        "        align_scores = align_scores.masked_fill(qtn_mask == 1, -float('inf'))\n",
        "        # align_scores = [bs, ctx_len, qtn_len]\n",
        "        \n",
        "        align_scores_flat = align_scores.view(-1, question.size(1))\n",
        "        # align_scores = [bs*ctx_len, qtn_len]\n",
        "        \n",
        "        alpha = F.softmax(align_scores_flat, dim=1)\n",
        "        alpha = alpha.view(-1, context.shape[1], question.shape[1])\n",
        "        # alpha = [bs, ctx_len, qtn_len]\n",
        "        \n",
        "        align_embedding = torch.bmm(alpha, question)\n",
        "        # align = [bs, ctx_len, emb_dim]\n",
        "        \n",
        "        return align_embedding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Bi-LSTM Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StackedBiLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstms = nn.ModuleList()\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            \n",
        "            if i == 0:\n",
        "                input_dim = input_dim \n",
        "            else:\n",
        "                input_dim = hidden_dim * 2\n",
        "            \n",
        "            self.lstms.append(nn.LSTM(input_dim, hidden_dim,\n",
        "                                      batch_first=True, bidirectional=True))\n",
        "           \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x = [bs, seq_len, feature_dim]\n",
        "\n",
        "        outputs = [x]\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            lstm_input = outputs[-1]\n",
        "            lstm_out = F.dropout(lstm_input, p=self.dropout)\n",
        "            lstm_out, (hidden, cell) = self.lstms[i](lstm_input)\n",
        "           \n",
        "            outputs.append(lstm_out)\n",
        "\n",
        "    \n",
        "        output = torch.cat(outputs[1:], dim=2)\n",
        "        # [bs, seq_len, num_layers*num_dir*hidden_dim]\n",
        "        \n",
        "        output = F.dropout(output, p=self.dropout)\n",
        "      \n",
        "        return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Linear Attention Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearAttentionLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "        \n",
        "    def forward(self, question, question_mask):\n",
        "        \n",
        "        # question = [bs, qtn_len, input_dim] = [bs, qtn_len, bi_lstm_hid_dim]\n",
        "        # question_mask = [bs,  qtn_len]\n",
        "        \n",
        "        qtn = question.view(-1, question.shape[-1])\n",
        "        # qtn = [bs*qtn_len, hid_dim]\n",
        "        \n",
        "        attn_scores = self.linear(qtn)\n",
        "        # attn_scores = [bs*qtn_len, 1]\n",
        "        \n",
        "        attn_scores = attn_scores.view(question.shape[0], question.shape[1])\n",
        "        # attn_scores = [bs, qtn_len]\n",
        "        \n",
        "        attn_scores = attn_scores.masked_fill(question_mask == 1, -float('inf'))\n",
        "        \n",
        "        alpha = F.softmax(attn_scores, dim=1)\n",
        "        # alpha = [bs, qtn_len]\n",
        "        \n",
        "        return alpha\n",
        "        \n",
        "\n",
        "def weighted_average(x, weights):\n",
        "    # x = [bs, len, dim]\n",
        "    # weights = [bs, len]\n",
        "    \n",
        "    weights = weights.unsqueeze(1)\n",
        "    # weights = [bs, 1, len]\n",
        "    \n",
        "    w = weights.bmm(x).squeeze(1)\n",
        "    # w = [bs, 1, dim] => [bs, dim]\n",
        "    \n",
        "    return w"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Attention Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, context_dim, question_dim, attn_method):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.linear = nn.Linear(question_dim, context_dim)\n",
        "\n",
        "        self.attn_method = attn_method\n",
        "        \n",
        "    def forward(self, context, question, context_mask):\n",
        "        \n",
        "        # context = [bs, ctx_len, ctx_hid_dim] = [bs, ctx_len, hid_dim*6] = [bs, ctx_len, 768]\n",
        "        # question = [bs, qtn_hid_dim] = [bs, qtn_len, 768]\n",
        "        # context_mask = [bs, ctx_len]\n",
        "        \n",
        "        # print(self.attn_method)\n",
        "\n",
        "        if self.attn_method == 'bilinear':\n",
        "            qtn_proj = self.linear(question)\n",
        "            qtn_proj = qtn_proj.unsqueeze(2)\n",
        "            # scores = context.bmm(qtn_proj)\n",
        "            scores = context.bmm(qtn_proj)\n",
        "        # qtn_proj = [bs, ctx_hid_dim]\n",
        "        elif self.attn_method == 'cosine sim':\n",
        "            qtn_norm = F.normalize(question, dim=-1)  # Normalize the question vectors\n",
        "            ctx_norm = F.normalize(context, dim=-1)  # Normalize the context vectors\n",
        "            scores = ctx_norm.bmm(qtn_norm.unsqueeze(2))\n",
        "        elif self.attn_method == 'scaled dot product':\n",
        "        # qtn_proj = [bs, ctx_hid_dim, 1]\n",
        "            scores = context.bmm(question.unsqueeze(2))\n",
        "        # scores = [bs, ctx_len, 1]\n",
        "            d_k = question.size(-1)  # Dimension of the query/key vectors\n",
        "            scores = scores / np.sqrt(d_k)  # Apply scaling by square root of the dimension\n",
        "        \n",
        "        scores = scores.squeeze(2)\n",
        "        # scores = [bs, ctx_len]\n",
        "        \n",
        "        scores = scores.masked_fill(context_mask == 1, -float('inf'))\n",
        "        \n",
        "        # alpha = nn.LogSoftmax(scores, dim=1)\n",
        "        # alpha = [bs, ctx_len]\n",
        "        \n",
        "        return scores"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 - Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QA_model(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_dim, embedding_dim, num_layers, num_directions, dropout, device, attn_method):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        #self.embedding = self.get_glove_embedding()\n",
        "        \n",
        "        self.context_bilstm = StackedBiLSTM(embedding_dim * 2, hidden_dim, num_layers, dropout)\n",
        "        \n",
        "        self.question_bilstm = StackedBiLSTM(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        \n",
        "        self.glove_embedding = self.get_glove_embedding()\n",
        "        \n",
        "        def tune_embedding(grad, words=1000):\n",
        "            grad[words:] = 0\n",
        "            return grad\n",
        "        \n",
        "        self.glove_embedding.weight.register_hook(tune_embedding)\n",
        "        \n",
        "        self.align_embedding = AlignQuestionEmbedding(embedding_dim)\n",
        "        \n",
        "        self.linear_attn_question = LinearAttentionLayer(hidden_dim*num_layers*num_directions) \n",
        "\n",
        "        self.attn_start = AttentionLayer(hidden_dim*num_layers*num_directions, \n",
        "                                                          hidden_dim*num_layers*num_directions, attn_method)\n",
        "        \n",
        "        self.attn_end = AttentionLayer(hidden_dim*num_layers*num_directions,\n",
        "                                                        hidden_dim*num_layers*num_directions, attn_method)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        \n",
        "    def get_glove_embedding(self):\n",
        "        \n",
        "        weights_matrix = np.load('weights.matrix.npy')\n",
        "        num_embeddings, embedding_dim = weights_matrix.shape\n",
        "        print(num_embeddings, embedding_dim)\n",
        "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=False)\n",
        "\n",
        "        return embedding\n",
        "    \n",
        "    \n",
        "    def forward(self, context, question, context_mask, question_mask):\n",
        "       \n",
        "        # context = [bs, len_c]\n",
        "        # question = [bs, len_q]\n",
        "        # context_mask = [bs, len_c]\n",
        "        # question_mask = [bs, len_q]\n",
        "        \n",
        "        \n",
        "        ctx_embed = self.glove_embedding(context)\n",
        "        # ctx_embed = [bs, len_c, emb_dim]\n",
        "        \n",
        "        ques_embed = self.glove_embedding(question)\n",
        "        # ques_embed = [bs, len_q, emb_dim]\n",
        "        \n",
        "\n",
        "        ctx_embed = self.dropout(ctx_embed)\n",
        "     \n",
        "        ques_embed = self.dropout(ques_embed)\n",
        "             \n",
        "        align_embed = self.align_embedding(ctx_embed, ques_embed, question_mask)\n",
        "        # align_embed = [bs, len_c, emb_dim]  \n",
        "        \n",
        "        ctx_bilstm_input = torch.cat([ctx_embed, align_embed], dim=2)\n",
        "        # ctx_bilstm_input = [bs, len_c, emb_dim*2]\n",
        "                \n",
        "        ctx_outputs = self.context_bilstm(ctx_bilstm_input)\n",
        "        # ctx_outputs = [bs, len_c, hid_dim*layers*dir] = [bs, len_c, hid_dim*6]\n",
        "       \n",
        "        qtn_outputs = self.question_bilstm(ques_embed)\n",
        "        # qtn_outputs = [bs, len_q, hid_dim*6]\n",
        "    \n",
        "        qtn_weights = self.linear_attn_question(qtn_outputs, question_mask)\n",
        "        # qtn_weights = [bs, len_q]\n",
        "            \n",
        "        qtn_weighted = weighted_average(qtn_outputs, qtn_weights)\n",
        "        # qtn_weighted = [bs, hid_dim*6]\n",
        "        \n",
        "        start_scores = self.attn_start(ctx_outputs, qtn_weighted, context_mask)\n",
        "        # start_scores = [bs, len_c]\n",
        "         \n",
        "        end_scores = self.attn_end(ctx_outputs, qtn_weighted, context_mask)\n",
        "        # end_scores = [bs, len_c]\n",
        "        \n",
        "      \n",
        "        return start_scores, end_scores"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "39552 77\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "HIDDEN_DIM = 4\n",
        "EMB_DIM = 77\n",
        "NUM_LAYERS = 3\n",
        "NUM_DIRECTIONS = 2\n",
        "DROPOUT = 0 # vastly different predictions are made each test if dropout is used\n",
        "attn_method = 'bilinear'\n",
        "\n",
        "model = QA_model(HIDDEN_DIM,\n",
        "                       EMB_DIM, \n",
        "                       NUM_LAYERS, \n",
        "                       NUM_DIRECTIONS, \n",
        "                       DROPOUT, \n",
        "                       device,\n",
        "                       attn_method).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adamax(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 3,062,303 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    '''Returns the number of trainable parameters in the model.'''\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_dataset):\n",
        "    '''\n",
        "    Trains the model.\n",
        "    '''\n",
        "    \n",
        "    print(\"Starting training ........\")\n",
        "    \n",
        "    train_loss = 0.\n",
        "    batch_count = 0\n",
        "    \n",
        "    # put the model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    # iterate through training data\n",
        "    for batch in train_dataset:\n",
        "\n",
        "        # print(batch_count)\n",
        "\n",
        "        if batch_count % 500 == 0:\n",
        "            print(f\"Starting batch: {batch_count}\")\n",
        "        batch_count += 1\n",
        "\n",
        "        context, question, context_mask, question_mask, answer_span, ctx, ids = batch\n",
        "        # context, question, context_mask, question_mask, label, ctx, ans, ids = batch\n",
        "        # print(context)\n",
        "        # print(question)\n",
        "        # print(context_mask)\n",
        "        # print(question_mask)\n",
        "        # print(answer_span)\n",
        "        \n",
        "        # place the tensors on GPU/CPU\n",
        "        context, context_mask, question, question_mask, answer_span = context.to(device), context_mask.to(device),\\\n",
        "                                    question.to(device), question_mask.to(device), answer_span.to(device)\n",
        "        \n",
        "        # forward pass, get the predictions\n",
        "        preds = model(context, question, context_mask, question_mask)\n",
        "\n",
        "        p1, p2 = preds\n",
        "        # print(p1,p2)\n",
        "        \n",
        "        # separate labels for start and end position\n",
        "        y1, y2 = answer_span[:,0], answer_span[:,1]\n",
        "        # print(answer_span, ids)\n",
        "        # print(y1,y2)\n",
        "        # print(y1, y2)\n",
        "        # print(p2,y2)\n",
        "        # for i in range(len(p1[0])):\n",
        "        #     print(i)\n",
        "        #     print(max(p1[i]))\n",
        "        #     print(p1[y1[i]])\n",
        "        #     break\n",
        "\n",
        "        # calculate loss\n",
        "        loss = F.cross_entropy(p1, y1) + F.cross_entropy(p2, y2)\n",
        "        \n",
        "        # backward pass, calculates the gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
        "        \n",
        "        # update the gradients\n",
        "        optimizer.step()\n",
        "        \n",
        "        # zero the gradients to prevent them from accumulating\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    \n",
        "\n",
        "    return train_loss/len(train_dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    '''\n",
        "    Helper function to record epoch time.\n",
        "    '''\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def calculate_precision(predicted_span, ground_truth_span):\n",
        "    \"\"\"\n",
        "    Calculate precision for a predicted span given the ground truth span.\n",
        "\n",
        "    Args:\n",
        "        predicted_span (list): Predicted span [start_token, end_token]\n",
        "        ground_truth_span (list): Ground truth span [start_token, end_token]\n",
        "\n",
        "    Returns:\n",
        "        float: Precision score\n",
        "    \"\"\"\n",
        "    # Calculate the number of tokens in the predicted span that are in the ground truth span\n",
        "    predicted_tokens = set(range(predicted_span[0], predicted_span[1] + 1))\n",
        "    ground_truth_tokens = set(range(ground_truth_span[0], ground_truth_span[1] + 1))\n",
        "    intersection = predicted_tokens.intersection(ground_truth_tokens)\n",
        "    \n",
        "    # Calculate the precision score\n",
        "    precision = len(intersection) / len(predicted_tokens)\n",
        "    \n",
        "    return precision\n",
        "\n",
        "def calculate_recall(predicted_span, ground_truth_span):\n",
        "    \"\"\"\n",
        "    Calculate recall for a predicted span given the ground truth span.\n",
        "\n",
        "    Args:\n",
        "        predicted_span (list): Predicted span [start_token, end_token]\n",
        "        ground_truth_span (list): Ground truth span [start_token, end_token]\n",
        "\n",
        "    Returns:\n",
        "        float: Recall score\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate the number of tokens in the ground truth span that are in the predicted span\n",
        "    predicted_tokens = set(range(predicted_span[0], predicted_span[1] + 1))\n",
        "    ground_truth_tokens = set(range(ground_truth_span[0], ground_truth_span[1] + 1))\n",
        "    intersection = predicted_tokens.intersection(ground_truth_tokens)\n",
        "    \n",
        "    # Calculate the recall score\n",
        "    recall = len(intersection) / len(ground_truth_tokens)\n",
        "    \n",
        "    return recall\n",
        "\n",
        "def calculate_f1(precision, recall):\n",
        "    \"\"\"\n",
        "    Calculate F1 score given precision and recall.\n",
        "\n",
        "    Args:\n",
        "        precision (float): Precision score\n",
        "        recall (float): Recall score\n",
        "\n",
        "    Returns:\n",
        "        float: F1 score\n",
        "    \"\"\"\n",
        "    if precision == 0.0 or recall == 0.0:\n",
        "        return 0.0\n",
        "    \n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def evaluate(predicted_spans, ground_truth_spans):\n",
        "    \"\"\"\n",
        "    Calculate precision, recall, and F1 score for predicted spans compared to ground truth spans.\n",
        "\n",
        "    Args:\n",
        "        predicted_spans (dict): Predicted spans with question IDs as keys and lists of predicted spans as values\n",
        "        ground_truth_spans (dict): Ground truth spans with question IDs as keys and lists of ground truth spans as values\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average precision, recall, and F1 score\n",
        "    \"\"\"\n",
        "    total_precision = 0.0\n",
        "    total_recall = 0.0\n",
        "    total_f1 = 0.0\n",
        "    \n",
        "    for question_id, predicted_span in predicted_spans.items():\n",
        "        ground_truth_span = ground_truth_spans[question_id]\n",
        "        \n",
        "        best_precision = 0.0\n",
        "        best_recall = 0.0\n",
        "        best_f1 = 0.0\n",
        "        \n",
        "        # Iterate over each ground truth span\n",
        "        for gt_span in ground_truth_span:\n",
        "            # Calculate precision, recall, and F1 score\n",
        "            precision = calculate_precision(predicted_span, gt_span)\n",
        "            recall = calculate_recall(predicted_span, gt_span)\n",
        "            f1 = calculate_f1(precision, recall)\n",
        "            \n",
        "            # Update best metrics if necessary\n",
        "            if precision > best_precision:\n",
        "                best_precision = precision\n",
        "            if recall > best_recall:\n",
        "                best_recall = recall\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "        \n",
        "        # Add the best metrics for this question to the totals\n",
        "        total_precision += best_precision\n",
        "        total_recall += best_recall\n",
        "        total_f1 += best_f1\n",
        "    \n",
        "    # Calculate average precision, recall, and F1 score\n",
        "    avg_precision = total_precision / len(predicted_spans)\n",
        "    avg_recall = total_recall / len(predicted_spans)\n",
        "    avg_f1 = total_f1 / len(predicted_spans)\n",
        "    \n",
        "    return avg_precision, avg_recall, avg_f1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Validation**\n",
        "\n",
        "Create dictionary containing answer spans of the test data for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Q0': [[110, 150]], 'Q3': [[0, 0]], 'Q4': [[58, 78]], 'Q20': [[86, 100]], 'Q33': [[15, 50], [50, 63], [63, 104], [104, 150]], 'Q47': [[0, 0]], 'Q49': [[0, 0]], 'Q54': [[0, 0]], 'Q57': [[0, 0]], 'Q59': [[80, 89]], 'Q64': [[117, 156]], 'Q68': [[0, 0]], 'Q69': [[0, 0]], 'Q72': [[0, 0]], 'Q79': [[0, 0]], 'Q81': [[0, 0]], 'Q86': [[0, 0]], 'Q88': [[0, 0]], 'Q91': [[0, 0]], 'Q102': [[0, 36], [36, 73]], 'Q105': [[10, 27]], 'Q113': [[207, 231]], 'Q115': [[0, 0]], 'Q117': [[85, 97]], 'Q123': [[0, 0]], 'Q131': [[0, 0]], 'Q132': [[32, 61], [61, 70]], 'Q144': [[25, 45]], 'Q146': [[47, 62]], 'Q147': [[1, 32], [42, 55]], 'Q149': [[0, 0]], 'Q152': [[0, 0]], 'Q153': [[0, 0]], 'Q157': [[0, 0]], 'Q165': [[12, 54]], 'Q182': [[0, 0]], 'Q186': [[0, 0]], 'Q194': [[0, 0]], 'Q196': [[0, 0]], 'Q198': [[0, 0]], 'Q202': [[0, 0]], 'Q205': [[0, 0]], 'Q216': [[0, 0]], 'Q221': [[0, 17], [17, 57]], 'Q222': [[0, 0]], 'Q224': [[0, 0]], 'Q232': [[0, 0]], 'Q236': [[0, 0]], 'Q242': [[0, 28], [28, 68], [68, 95]], 'Q243': [[0, 0]], 'Q248': [[0, 0]], 'Q254': [[31, 63]], 'Q262': [[0, 0]], 'Q265': [[0, 0]], 'Q269': [[0, 0]], 'Q271': [[0, 0]], 'Q279': [[0, 0]], 'Q285': [[0, 0]], 'Q292': [[0, 0]], 'Q297': [[0, 0]], 'Q300': [[4, 23], [86, 112]], 'Q309': [[9, 34], [48, 60]], 'Q312': [[0, 0]], 'Q318': [[0, 14]], 'Q322': [[0, 0]], 'Q323': [[0, 0]], 'Q324': [[32, 43]], 'Q329': [[0, 0]], 'Q331': [[0, 26]], 'Q333': [[0, 0]], 'Q337': [[0, 0]], 'Q340': [[26, 48]], 'Q344': [[0, 0]], 'Q345': [[29, 49], [49, 67]], 'Q355': [[0, 0]], 'Q361': [[52, 90]], 'Q362': [[42, 61]], 'Q367': [[0, 0]], 'Q376': [[0, 0]], 'Q383': [[20, 54]], 'Q384': [[34, 73]], 'Q386': [[0, 0]], 'Q388': [[28, 44]], 'Q391': [[0, 10]], 'Q392': [[0, 0]], 'Q394': [[0, 0]], 'Q397': [[0, 0]], 'Q404': [[0, 0]], 'Q410': [[0, 0]], 'Q411': [[0, 0]], 'Q414': [[42, 81]], 'Q415': [[0, 0]], 'Q430': [[0, 0]], 'Q432': [[0, 0]], 'Q434': [[0, 0]], 'Q436': [[0, 0]], 'Q448': [[23, 36]], 'Q451': [[0, 0]], 'Q452': [[17, 59]], 'Q453': [[0, 0]], 'Q455': [[0, 0]], 'Q461': [[0, 0]], 'Q462': [[1, 35]], 'Q463': [[25, 47], [47, 62]], 'Q486': [[0, 0]], 'Q498': [[0, 0]], 'Q506': [[0, 0]], 'Q508': [[0, 0]], 'Q519': [[0, 0]], 'Q523': [[0, 0]], 'Q526': [[0, 0]], 'Q530': [[0, 0]], 'Q540': [[0, 0]], 'Q542': [[0, 0]], 'Q547': [[0, 0]], 'Q553': [[0, 0]], 'Q554': [[0, 0]], 'Q565': [[0, 0]], 'Q568': [[177, 206]], 'Q575': [[0, 15]], 'Q581': [[83, 108]], 'Q583': [[109, 143]], 'Q595': [[0, 0]], 'Q597': [[0, 21]], 'Q599': [[0, 0]], 'Q607': [[0, 0]], 'Q611': [[21, 37]], 'Q615': [[112, 126]], 'Q638': [[0, 34]], 'Q656': [[0, 0]], 'Q658': [[0, 0]], 'Q664': [[0, 0]], 'Q676': [[254, 264]], 'Q677': [[165, 177]], 'Q680': [[0, 0]], 'Q682': [[51, 83]], 'Q683': [[0, 19], [19, 43], [122, 179]], 'Q685': [[22, 58]], 'Q688': [[0, 0]], 'Q692': [[211, 221]], 'Q698': [[0, 23]], 'Q701': [[0, 0]], 'Q706': [[0, 0]], 'Q710': [[0, 0]], 'Q711': [[0, 0]], 'Q715': [[124, 148]], 'Q724': [[0, 0]], 'Q735': [[0, 0]], 'Q740': [[0, 0]], 'Q743': [[0, 16]], 'Q744': [[0, 0]], 'Q750': [[143, 157]], 'Q753': [[0, 0]], 'Q755': [[25, 60], [60, 115]], 'Q756': [[0, 0]], 'Q757': [[13, 48]], 'Q762': [[0, 0]], 'Q763': [[0, 37]], 'Q769': [[0, 0]], 'Q800': [[335, 361]], 'Q802': [[0, 0]], 'Q817': [[0, 0]], 'Q820': [[0, 0]], 'Q821': [[0, 0]], 'Q825': [[0, 0]], 'Q838': [[0, 45]], 'Q844': [[0, 0]], 'Q850': [[126, 150], [150, 173], [173, 205], [233, 240]], 'Q855': [[0, 0]], 'Q857': [[0, 0]], 'Q858': [[0, 0]], 'Q864': [[0, 0]], 'Q865': [[0, 0]], 'Q870': [[239, 276]], 'Q871': [[0, 0]], 'Q878': [[0, 0]], 'Q882': [[0, 0]], 'Q889': [[0, 0]], 'Q894': [[0, 0]], 'Q897': [[0, 0]], 'Q900': [[81, 99], [114, 134], [201, 218]], 'Q904': [[0, 0]], 'Q906': [[0, 0]], 'Q907': [[0, 0]], 'Q909': [[0, 0]], 'Q910': [[8, 25], [25, 53]], 'Q911': [[73, 78]], 'Q917': [[0, 0]], 'Q926': [[0, 0]], 'Q929': [[0, 0]], 'Q930': [[0, 0]], 'Q933': [[0, 0]], 'Q934': [[0, 0]], 'Q935': [[0, 0]], 'Q947': [[0, 0]], 'Q952': [[0, 0]], 'Q956': [[0, 0]], 'Q964': [[0, 0]], 'Q967': [[0, 12]], 'Q971': [[0, 0]], 'Q980': [[0, 0]], 'Q983': [[0, 0]], 'Q984': [[0, 0]], 'Q990': [[0, 26]], 'Q992': [[0, 0]], 'Q995': [[0, 0]], 'Q1012': [[0, 13]], 'Q1018': [[0, 0]], 'Q1021': [[0, 0]], 'Q1027': [[0, 25]], 'Q1032': [[3, 37]], 'Q1033': [[11, 49], [49, 116]], 'Q1064': [[28, 51]], 'Q1065': [[0, 16]], 'Q1067': [[21, 55], [55, 92]], 'Q1070': [[0, 0]], 'Q1074': [[0, 0]], 'Q1077': [[0, 0]], 'Q1078': [[0, 32]], 'Q1089': [[0, 0]], 'Q1093': [[0, 0]], 'Q1100': [[20, 34]], 'Q1102': [[0, 12]], 'Q1103': [[21, 39]], 'Q1115': [[0, 0]], 'Q1116': [[0, 0]], 'Q1118': [[0, 0]], 'Q1126': [[0, 0]], 'Q1131': [[0, 0]], 'Q1132': [[0, 0]], 'Q1135': [[0, 0]], 'Q1146': [[0, 0]], 'Q1147': [[0, 0]], 'Q1148': [[0, 0]], 'Q1151': [[0, 0]], 'Q1152': [[0, 0]], 'Q1154': [[0, 22]], 'Q1157': [[15, 47], [99, 132]], 'Q1159': [[0, 20], [20, 52]], 'Q1163': [[0, 37]], 'Q1175': [[0, 0]], 'Q1184': [[0, 37]], 'Q1191': [[0, 24]], 'Q1195': [[0, 0]], 'Q1196': [[0, 19]], 'Q1197': [[0, 0]], 'Q1198': [[0, 0]], 'Q1203': [[0, 0]], 'Q1206': [[50, 80]], 'Q1211': [[10, 49]], 'Q1212': [[29, 43]], 'Q1213': [[0, 15]], 'Q1216': [[0, 0]], 'Q1223': [[0, 27]], 'Q1226': [[159, 178]], 'Q1228': [[0, 0]], 'Q1230': [[9, 33], [33, 60]], 'Q1233': [[0, 19]], 'Q1236': [[9, 34]], 'Q1240': [[0, 0]], 'Q1246': [[0, 0]], 'Q1261': [[0, 0]], 'Q1264': [[0, 0]], 'Q1268': [[0, 26]], 'Q1269': [[0, 0]], 'Q1275': [[31, 58]], 'Q1277': [[0, 0]], 'Q1288': [[6, 19]], 'Q1297': [[0, 22]], 'Q1311': [[0, 0]], 'Q1312': [[0, 0]], 'Q1316': [[0, 48]], 'Q1325': [[0, 29]], 'Q1326': [[0, 23]], 'Q1338': [[0, 0]], 'Q1345': [[143, 164], [242, 256]], 'Q1346': [[0, 0]], 'Q1355': [[10, 37]], 'Q1358': [[5, 39]], 'Q1379': [[0, 0]], 'Q1383': [[0, 0]], 'Q1389': [[21, 49]], 'Q1391': [[5, 32]], 'Q1406': [[0, 0]], 'Q1409': [[0, 0]], 'Q1416': [[378, 402]], 'Q1418': [[0, 22], [22, 49]], 'Q1434': [[0, 21]], 'Q1436': [[0, 0]], 'Q1440': [[0, 0]], 'Q1442': [[0, 21]], 'Q1444': [[0, 23]], 'Q1446': [[0, 0]], 'Q1460': [[0, 0]], 'Q1465': [[0, 31]], 'Q1466': [[142, 161]], 'Q1477': [[0, 0]], 'Q1478': [[341, 352]], 'Q1485': [[0, 30]], 'Q1486': [[82, 119]], 'Q1487': [[0, 0]], 'Q1499': [[47, 72]], 'Q1506': [[30, 55], [65, 80], [80, 115]], 'Q1510': [[0, 0]], 'Q1511': [[0, 25], [59, 75]], 'Q1515': [[0, 0]], 'Q1516': [[0, 23]], 'Q1517': [[0, 0]], 'Q1533': [[0, 0]], 'Q1550': [[0, 26]], 'Q1551': [[0, 17]], 'Q1552': [[0, 0]], 'Q1554': [[0, 0]], 'Q1556': [[0, 0]], 'Q1559': [[30, 53]], 'Q1561': [[0, 19]], 'Q1562': [[5, 70]], 'Q1563': [[8, 36]], 'Q1564': [[0, 0]], 'Q1569': [[0, 0]], 'Q1572': [[0, 0]], 'Q1580': [[84, 113]], 'Q1582': [[0, 0]], 'Q1583': [[0, 28]], 'Q1585': [[0, 0]], 'Q1586': [[0, 0]], 'Q1597': [[0, 0]], 'Q1599': [[0, 0]], 'Q1602': [[0, 0]], 'Q1608': [[0, 0]], 'Q1613': [[0, 0]], 'Q1616': [[232, 250]], 'Q1619': [[0, 33]], 'Q1631': [[0, 0]], 'Q1633': [[0, 21]], 'Q1644': [[0, 0]], 'Q1645': [[0, 0]], 'Q1650': [[0, 0]], 'Q1655': [[0, 0]], 'Q1656': [[0, 22]], 'Q1658': [[0, 0]], 'Q1661': [[0, 0]], 'Q1662': [[0, 35]], 'Q1665': [[0, 0]], 'Q1673': [[28, 56]], 'Q1675': [[0, 48]], 'Q1685': [[0, 0]], 'Q1687': [[0, 0]], 'Q1688': [[0, 0]], 'Q1689': [[18, 41]], 'Q1695': [[95, 114]], 'Q1696': [[0, 0]], 'Q1697': [[0, 0]], 'Q1707': [[130, 167]], 'Q1712': [[16, 29]], 'Q1714': [[32, 45]], 'Q1722': [[0, 17], [17, 37]], 'Q1733': [[0, 0]], 'Q1736': [[0, 0]], 'Q1739': [[137, 176]], 'Q1749': [[35, 55]], 'Q1758': [[0, 0]], 'Q1759': [[0, 0]], 'Q1760': [[0, 16]], 'Q1772': [[0, 22]], 'Q1780': [[0, 0]], 'Q1792': [[26, 48]], 'Q1799': [[0, 0]], 'Q1803': [[0, 0]], 'Q1805': [[68, 77]], 'Q1806': [[0, 13]], 'Q1813': [[0, 27], [27, 45]], 'Q1814': [[0, 36]], 'Q1815': [[0, 0]], 'Q1817': [[0, 0]], 'Q1821': [[0, 0]], 'Q1823': [[0, 0]], 'Q1841': [[0, 0]], 'Q1842': [[0, 0]], 'Q1857': [[26, 46]], 'Q1860': [[0, 32]], 'Q1863': [[0, 29]], 'Q1865': [[0, 0]], 'Q1867': [[0, 0]], 'Q1868': [[0, 0]], 'Q1870': [[0, 0]], 'Q1871': [[0, 0]], 'Q1879': [[0, 0]], 'Q1882': [[0, 0]], 'Q1884': [[0, 0]], 'Q1892': [[0, 0]], 'Q1898': [[0, 0]], 'Q1899': [[0, 30], [30, 51], [51, 77], [77, 93]], 'Q1905': [[0, 14]], 'Q1913': [[0, 0]], 'Q1916': [[0, 0]], 'Q1917': [[0, 0]], 'Q1918': [[12, 54]], 'Q1922': [[0, 0]], 'Q1928': [[0, 0]], 'Q1931': [[0, 18]], 'Q1933': [[0, 0]], 'Q1935': [[32, 48]], 'Q1937': [[29, 61]], 'Q1944': [[15, 21]], 'Q1955': [[0, 0]], 'Q1957': [[0, 0]], 'Q1963': [[0, 0]], 'Q1966': [[0, 17]], 'Q1970': [[0, 0]], 'Q1973': [[0, 0]], 'Q1983': [[16, 46]], 'Q1984': [[0, 25]], 'Q1987': [[0, 0]], 'Q1992': [[49, 60]], 'Q1994': [[0, 0]], 'Q1995': [[0, 0]], 'Q2004': [[0, 12], [31, 50], [50, 67], [67, 77]], 'Q2009': [[0, 24]], 'Q2011': [[0, 0]], 'Q2028': [[0, 0]], 'Q2029': [[0, 0]], 'Q2034': [[0, 0]], 'Q2035': [[0, 25]], 'Q2041': [[0, 27]], 'Q2047': [[0, 0]], 'Q2051': [[0, 0]], 'Q2058': [[0, 9]], 'Q2061': [[49, 81]], 'Q2062': [[19, 45]], 'Q2065': [[0, 22], [22, 40]], 'Q2073': [[0, 0]], 'Q2078': [[0, 0]], 'Q2080': [[133, 146]], 'Q2095': [[0, 0]], 'Q2100': [[96, 133]], 'Q2101': [[0, 0]], 'Q2116': [[0, 0]], 'Q2121': [[0, 0]], 'Q2122': [[0, 0]], 'Q2128': [[0, 32]], 'Q2131': [[0, 0]], 'Q2140': [[0, 43]], 'Q2144': [[0, 0]], 'Q2148': [[0, 0]], 'Q2153': [[0, 0]], 'Q2169': [[0, 40]], 'Q2180': [[0, 0]], 'Q2183': [[0, 0]], 'Q2184': [[0, 0]], 'Q2185': [[0, 0]], 'Q2189': [[0, 0]], 'Q2198': [[0, 0]], 'Q2199': [[0, 0]], 'Q2200': [[0, 14]], 'Q2201': [[0, 0]], 'Q2203': [[0, 0]], 'Q2214': [[0, 28]], 'Q2216': [[0, 0]], 'Q2226': [[123, 134]], 'Q2227': [[0, 13]], 'Q2228': [[0, 0]], 'Q2236': [[0, 0]], 'Q2240': [[0, 33]], 'Q2245': [[0, 0]], 'Q2259': [[13, 31], [66, 98]], 'Q2261': [[0, 15]], 'Q2263': [[32, 54]], 'Q2274': [[0, 0]], 'Q2275': [[0, 0]], 'Q2279': [[0, 21], [39, 63], [63, 77]], 'Q2283': [[0, 26]], 'Q2286': [[0, 9]], 'Q2304': [[0, 0]], 'Q2308': [[116, 141]], 'Q2312': [[0, 0]], 'Q2323': [[0, 0]], 'Q2326': [[0, 18]], 'Q2346': [[0, 0]], 'Q2349': [[0, 19]], 'Q2352': [[0, 33]], 'Q2359': [[0, 0]], 'Q2365': [[0, 0]], 'Q2374': [[0, 0]], 'Q2375': [[49, 69]], 'Q2377': [[20, 40]], 'Q2382': [[0, 0]], 'Q2383': [[0, 24], [24, 49]], 'Q2385': [[0, 24]], 'Q2386': [[0, 0]], 'Q2395': [[0, 8]], 'Q2408': [[26, 44]], 'Q2411': [[0, 0]], 'Q2416': [[0, 0]], 'Q2417': [[0, 0]], 'Q2421': [[0, 0]], 'Q2424': [[0, 0]], 'Q2425': [[0, 17]], 'Q2428': [[0, 0]], 'Q2433': [[0, 29]], 'Q2435': [[0, 0]], 'Q2438': [[0, 35]], 'Q2443': [[0, 0]], 'Q2445': [[0, 0]], 'Q2447': [[0, 27]], 'Q2453': [[0, 0]], 'Q2462': [[7, 48]], 'Q2466': [[0, 0]], 'Q2487': [[0, 0]], 'Q2495': [[0, 0]], 'Q2498': [[0, 19]], 'Q2499': [[0, 0]], 'Q2500': [[0, 0]], 'Q2509': [[0, 0]], 'Q2511': [[0, 36]], 'Q2521': [[0, 0]], 'Q2537': [[0, 23]], 'Q2539': [[0, 17]], 'Q2549': [[0, 0]], 'Q2551': [[0, 0]], 'Q2555': [[0, 0]], 'Q2561': [[0, 0]], 'Q2567': [[0, 0]], 'Q2574': [[0, 0]], 'Q2575': [[0, 0]], 'Q2582': [[0, 0]], 'Q2590': [[5, 24]], 'Q2592': [[0, 10], [10, 30]], 'Q2603': [[0, 0]], 'Q2605': [[0, 0]], 'Q2606': [[0, 0]], 'Q2608': [[0, 0]], 'Q2610': [[0, 0]], 'Q2613': [[0, 0]], 'Q2615': [[0, 0]], 'Q2618': [[0, 0]], 'Q2621': [[0, 0]], 'Q2622': [[0, 0]], 'Q2623': [[0, 0]], 'Q2634': [[0, 0]], 'Q2636': [[0, 0]], 'Q2637': [[138, 161]], 'Q2640': [[23, 39]], 'Q2658': [[0, 0]], 'Q2662': [[0, 0]], 'Q2663': [[0, 0]], 'Q2675': [[0, 22]], 'Q2677': [[0, 0]], 'Q2679': [[0, 0]], 'Q2682': [[97, 123]], 'Q2684': [[0, 0]], 'Q2685': [[0, 24]], 'Q2687': [[0, 12]], 'Q2688': [[171, 190]], 'Q2698': [[12, 37]], 'Q2704': [[0, 0]], 'Q2709': [[0, 0]], 'Q2714': [[0, 0]], 'Q2720': [[0, 0]], 'Q2723': [[0, 0]], 'Q2727': [[0, 0]], 'Q2730': [[0, 0]], 'Q2734': [[0, 35]], 'Q2737': [[0, 0]], 'Q2738': [[0, 36]], 'Q2739': [[0, 0]], 'Q2741': [[308, 318]], 'Q2753': [[0, 0]], 'Q2766': [[0, 0]], 'Q2780': [[0, 0]], 'Q2788': [[0, 0]], 'Q2801': [[0, 0]], 'Q2802': [[0, 10]], 'Q2803': [[0, 0]], 'Q2806': [[0, 0]], 'Q2810': [[81, 105]], 'Q2811': [[0, 0]], 'Q2812': [[0, 0]], 'Q2815': [[0, 0]], 'Q2822': [[0, 34]], 'Q2823': [[0, 0]], 'Q2834': [[0, 0]], 'Q2836': [[0, 0]], 'Q2839': [[0, 0]], 'Q2841': [[16, 38]], 'Q2842': [[0, 0]], 'Q2843': [[0, 0]], 'Q2848': [[0, 17]], 'Q2859': [[0, 0]], 'Q2860': [[0, 0]], 'Q2868': [[0, 0]], 'Q2880': [[0, 35]], 'Q2884': [[0, 23]], 'Q2892': [[0, 0]], 'Q2896': [[0, 0]], 'Q2897': [[0, 0]], 'Q2902': [[0, 0]], 'Q2903': [[36, 59]], 'Q2921': [[0, 0]], 'Q2924': [[28, 46]], 'Q2931': [[3, 49]], 'Q2933': [[0, 32], [32, 90], [90, 103], [103, 115]], 'Q2934': [[0, 0]], 'Q2935': [[0, 34]], 'Q2939': [[525, 544]], 'Q2940': [[0, 22], [36, 62]], 'Q2942': [[0, 0]], 'Q2945': [[0, 0]], 'Q2947': [[0, 0]], 'Q2952': [[0, 11]], 'Q2953': [[0, 0]], 'Q2954': [[0, 0]], 'Q2960': [[0, 0]], 'Q2964': [[0, 0]], 'Q2965': [[0, 0]], 'Q2970': [[0, 0]], 'Q2973': [[2, 25]], 'Q2980': [[0, 0]], 'Q2981': [[0, 0]], 'Q2986': [[213, 222]], 'Q2990': [[59, 96]], 'Q2994': [[0, 27]], 'Q3004': [[9, 43]], 'Q3006': [[0, 0]], 'Q3008': [[82, 91]], 'Q3012': [[7, 23]], 'Q3013': [[0, 0]], 'Q3015': [[0, 0]], 'Q3026': [[0, 0]], 'Q3045': [[0, 0]]}\n"
          ]
        }
      ],
      "source": [
        "answer_span_dict = {}\n",
        "\n",
        "# Iterate over the rows of the DataFrame\n",
        "for _, row in wrangled_test_data.iterrows():\n",
        "    question_id = row['question_id']\n",
        "    answer_span = row['answer span']\n",
        "    \n",
        "    # If the question ID is already present in the dictionary, append the answer span\n",
        "    if question_id in answer_span_dict:\n",
        "        answer_span_dict[question_id].append(answer_span)\n",
        "    # If the question ID is not present, create a new list with the answer span\n",
        "    else:\n",
        "        answer_span_dict[question_id] = [answer_span]\n",
        "\n",
        "print(answer_span_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "def valid(model, valid_dataset):\n",
        "    '''\n",
        "    Performs validation.\n",
        "    '''\n",
        "    \n",
        "    print(\"Starting validation .........\")\n",
        "   \n",
        "    valid_loss = 0.\n",
        "\n",
        "    batch_count = 0\n",
        "    \n",
        "    # puts the model in eval mode. Turns off dropout\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = {}\n",
        "    \n",
        "    for batch in valid_dataset:\n",
        "\n",
        "        if batch_count % 500 == 0:\n",
        "            print(f\"Starting batch {batch_count}\")\n",
        "        batch_count += 1\n",
        "\n",
        "        context, question, context_mask, question_mask, answer_span, context_text, ids = batch\n",
        "        # context, question, context_mask, question_mask, label, context_text, answers, ids = batch\n",
        "\n",
        "        context, context_mask, question, question_mask, answer_span = context.to(device), context_mask.to(device),\\\n",
        "                                    question.to(device), question_mask.to(device), answer_span.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            preds = model(context, question, context_mask, question_mask)\n",
        "\n",
        "            p1, p2 = preds\n",
        "\n",
        "            y1, y2 = answer_span[:,0], answer_span[:,1]\n",
        "            # print(y1, y2)\n",
        "\n",
        "            # cross_entropy function combines the softmax operation with the computation of the cross-entropy loss in a single step\n",
        "            loss = F.cross_entropy(p1, y1) + F.cross_entropy(p2, y2)\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            \n",
        "            # get the start and end index positions from the model preds\n",
        "            \n",
        "            batch_size, c_len = p1.size()\n",
        "            ls = nn.LogSoftmax(dim=1)\n",
        "            # mask = torch.ones(c_len, c_len).tril(-1).to(device)  # Lower triangular matrix\n",
        "            # mask = mask.fill_(float('-inf'))  # Set all elements to negative infinity\n",
        "            # mask = mask.unsqueeze(0).expand(batch_size, -1, -1)  # Expand mask for each sample in the batch\n",
        "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            \n",
        "            # score = (p1.unsqueeze(2) + p2.unsqueeze(1)) + mask\n",
        "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
        "            score, s_idx = score.max(dim=1)\n",
        "            score, e_idx = score.max(dim=1)\n",
        "            # s_scores, s_idx = score.max(dim=1)\n",
        "            # e_scores, e_idx = score.topk(k=1, dim=1)  # Retrieve the top 1 scores and indices along the second dimension\n",
        "            # e_idx = e_idx.squeeze(1)\n",
        "            # print(s_idx, e_idx)\n",
        "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
        "            # e_idx = torch.gather(e_idx, 1, s_idx.view(-1, 1)).squeeze()\n",
        "            # print(s_idx, e_idx)\n",
        "            # stack predictions\n",
        "            for i in range(batch_size):\n",
        "                id = ids[i]\n",
        "                # print(id, [p1, p2])\n",
        "                # pred = context[i][s_idx[i]:e_idx[i]+1]\n",
        "\n",
        "                # pred = ' '.join([index2word[idx.item()] for idx in pred])\n",
        "                predictions[id] = [s_idx[i].item(), e_idx[i].item()]\n",
        "                # predictions[id] = [p1, p2]\n",
        "            \n",
        "    print(predictions)\n",
        "    precision, recall, f1 = evaluate(predictions, answer_span_dict)            \n",
        "    return valid_loss/len(valid_dataset), precision, recall, f1\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def valid(model, valid_dataset):\n",
        "    '''\n",
        "    Performs validation.\n",
        "    '''\n",
        "    \n",
        "    print(\"Starting validation .........\")\n",
        "   \n",
        "    valid_loss = 0.\n",
        "\n",
        "    batch_count = 0\n",
        "    \n",
        "    f1, em = 0., 0.\n",
        "    \n",
        "    # puts the model in eval mode. Turns off dropout\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = {}\n",
        "    \n",
        "    for batch in valid_dataset:\n",
        "\n",
        "        if batch_count % 500 == 0:\n",
        "            print(f\"Starting batch {batch_count}\")\n",
        "        batch_count += 1\n",
        "\n",
        "        context, question, context_mask, question_mask, answer_span, context_text, ids = batch\n",
        "        # context, question, context_mask, question_mask, label, context_text, answers, ids = batch\n",
        "\n",
        "        context, context_mask, question, question_mask, answer_span = context.to(device), context_mask.to(device),\\\n",
        "                                    question.to(device), question_mask.to(device), answer_span.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            preds = model(context, question, context_mask, question_mask)\n",
        "\n",
        "            p1, p2 = preds\n",
        "\n",
        "            y1, y2 = answer_span[:,0], answer_span[:,1]\n",
        "\n",
        "            # cross_entropy function combines the softmax operation with the computation of the cross-entropy loss in a single step\n",
        "            loss = F.cross_entropy(p1, y1) + F.cross_entropy(p2, y2)\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            \n",
        "            # get the start and end index positions from the model preds\n",
        "            \n",
        "            batch_size, c_len = p1.size()\n",
        "            ls = nn.LogSoftmax(dim=1)\n",
        "            # mask = torch.ones(c_len, c_len).tril(-1).to(device)  # Lower triangular matrix\n",
        "            # mask = mask.fill_(float('-inf'))  # Set all elements to negative infinity\n",
        "            # mask = mask.unsqueeze(0).expand(batch_size, -1, -1)  # Expand mask for each sample in the batch\n",
        "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            \n",
        "            # score = (p1.unsqueeze(2) + p2.unsqueeze(1)) + mask\n",
        "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
        "            score, s_idx = score.max(dim=1)\n",
        "            score, e_idx = score.max(dim=1)\n",
        "            # s_scores, s_idx = score.max(dim=1)\n",
        "            # e_scores, e_idx = score.topk(k=1, dim=1)  # Retrieve the top 1 scores and indices along the second dimension\n",
        "            # e_idx = e_idx.squeeze(1)\n",
        "            print(s_idx, e_idx)\n",
        "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
        "            # e_idx = torch.gather(e_idx, 1, s_idx.view(-1, 1)).squeeze()\n",
        "            print(s_idx, e_idx)\n",
        "            # stack predictions\n",
        "            for i in range(batch_size):\n",
        "                id = ids[i]\n",
        "                # print(id, [p1, p2])\n",
        "                # pred = context[i][s_idx[i]:e_idx[i]+1]\n",
        "\n",
        "                # pred = ' '.join([index2word[idx.item()] for idx in pred])\n",
        "                predictions[id] = [s_idx[i].item(), e_idx[i].item()]\n",
        "                # predictions[id] = [p1, p2]\n",
        "            \n",
        "    print(predictions)\n",
        "    # em, f1 = evaluate(predictions)            \n",
        "    # return valid_loss/len(valid_dataset), em, f1\n",
        "                "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Model Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Starting training ........\n",
            "Starting batch: 0\n",
            "Starting validation .........\n",
            "Starting batch 0\n",
            "{'Q0': [97, 127], 'Q3': [149, 187], 'Q4': [25, 66], 'Q20': [31, 37], 'Q33': [67, 267], 'Q47': [217, 227], 'Q49': [159, 265], 'Q54': [21, 50], 'Q57': [131, 159], 'Q59': [32, 109], 'Q64': [165, 251], 'Q68': [291, 326], 'Q69': [135, 168], 'Q72': [56, 56], 'Q79': [10, 19], 'Q81': [160, 197], 'Q86': [111, 217], 'Q88': [49, 57], 'Q91': [33, 301], 'Q102': [64, 69], 'Q105': [25, 26], 'Q113': [291, 291], 'Q115': [14, 30], 'Q117': [24, 187], 'Q123': [2, 2], 'Q131': [27, 303], 'Q132': [62, 79], 'Q144': [0, 0], 'Q146': [92, 112], 'Q147': [125, 145], 'Q149': [225, 277], 'Q152': [22, 50], 'Q153': [42, 107], 'Q157': [40, 40], 'Q165': [0, 0], 'Q182': [0, 121], 'Q186': [38, 88], 'Q194': [17, 130], 'Q196': [0, 5], 'Q198': [0, 58], 'Q202': [75, 78], 'Q205': [133, 137], 'Q216': [79, 159], 'Q221': [0, 59], 'Q222': [81, 401], 'Q224': [162, 310], 'Q232': [72, 267], 'Q236': [239, 245], 'Q242': [22, 23], 'Q243': [227, 269], 'Q248': [0, 9], 'Q254': [66, 83], 'Q262': [107, 195], 'Q265': [24, 43], 'Q269': [185, 193], 'Q271': [19, 107], 'Q279': [247, 253], 'Q285': [22, 216], 'Q292': [61, 178], 'Q297': [8, 117], 'Q300': [294, 357], 'Q309': [151, 269], 'Q312': [139, 139], 'Q318': [34, 123], 'Q322': [71, 75], 'Q323': [32, 39], 'Q324': [52, 74], 'Q329': [50, 62], 'Q331': [52, 110], 'Q333': [160, 194], 'Q337': [45, 49], 'Q340': [74, 75], 'Q344': [3, 5], 'Q345': [20, 29], 'Q355': [15, 136], 'Q361': [38, 61], 'Q362': [167, 171], 'Q367': [6, 6], 'Q376': [208, 230], 'Q383': [31, 65], 'Q384': [17, 109], 'Q386': [80, 91], 'Q388': [62, 177], 'Q391': [15, 19], 'Q392': [96, 162], 'Q394': [158, 159], 'Q397': [0, 103], 'Q404': [135, 145], 'Q410': [111, 143], 'Q411': [107, 112], 'Q414': [30, 89], 'Q415': [24, 25], 'Q430': [6, 8], 'Q432': [207, 211], 'Q434': [2, 85], 'Q436': [30, 35], 'Q448': [7, 28], 'Q451': [180, 287], 'Q452': [54, 86], 'Q453': [0, 2], 'Q455': [151, 240], 'Q461': [160, 306], 'Q462': [1, 89], 'Q463': [37, 72], 'Q486': [60, 67], 'Q498': [95, 327], 'Q506': [8, 16], 'Q508': [67, 81], 'Q519': [1, 19], 'Q523': [21, 124], 'Q526': [118, 122], 'Q530': [82, 252], 'Q540': [19, 124], 'Q542': [2, 27], 'Q547': [3, 148], 'Q553': [1, 1], 'Q554': [4, 36], 'Q565': [30, 31], 'Q568': [198, 227], 'Q575': [22, 54], 'Q581': [8, 83], 'Q583': [69, 79], 'Q595': [191, 192], 'Q597': [78, 180], 'Q599': [216, 218], 'Q607': [1, 189], 'Q611': [35, 72], 'Q615': [111, 149], 'Q638': [20, 49], 'Q656': [78, 85], 'Q658': [194, 196], 'Q664': [57, 68], 'Q676': [360, 419], 'Q677': [113, 194], 'Q680': [202, 228], 'Q682': [47, 156], 'Q683': [97, 161], 'Q685': [28, 32], 'Q688': [61, 67], 'Q692': [122, 140], 'Q698': [4, 82], 'Q701': [396, 483], 'Q706': [32, 56], 'Q710': [72, 81], 'Q711': [164, 194], 'Q715': [25, 226], 'Q724': [210, 266], 'Q735': [172, 190], 'Q740': [132, 142], 'Q743': [2, 8], 'Q744': [94, 244], 'Q750': [174, 203], 'Q753': [30, 31], 'Q755': [22, 24], 'Q756': [83, 111], 'Q757': [97, 142], 'Q762': [147, 194], 'Q763': [131, 142], 'Q769': [1118, 1333], 'Q800': [241, 423], 'Q802': [3, 134], 'Q817': [28, 28], 'Q820': [99, 209], 'Q821': [28, 79], 'Q825': [41, 65], 'Q838': [3, 29], 'Q844': [382, 405], 'Q850': [80, 180], 'Q855': [3, 128], 'Q857': [124, 142], 'Q858': [251, 258], 'Q864': [362, 467], 'Q865': [7, 7], 'Q870': [147, 264], 'Q871': [3, 19], 'Q878': [138, 263], 'Q882': [44, 114], 'Q889': [85, 156], 'Q894': [18, 89], 'Q897': [101, 394], 'Q900': [55, 118], 'Q904': [259, 259], 'Q906': [30, 30], 'Q907': [21, 66], 'Q909': [147, 173], 'Q910': [17, 24], 'Q911': [93, 93], 'Q917': [5, 12], 'Q926': [68, 303], 'Q929': [45, 201], 'Q930': [113, 150], 'Q933': [42, 155], 'Q934': [115, 121], 'Q935': [108, 113], 'Q947': [18, 20], 'Q952': [279, 282], 'Q956': [77, 80], 'Q964': [230, 299], 'Q967': [2, 5], 'Q971': [106, 140], 'Q980': [12, 13], 'Q983': [18, 27], 'Q984': [287, 487], 'Q990': [125, 134], 'Q992': [177, 177], 'Q995': [6, 42], 'Q1012': [386, 456], 'Q1018': [36, 51], 'Q1021': [33, 227], 'Q1027': [29, 185], 'Q1032': [48, 58], 'Q1033': [233, 243], 'Q1064': [14, 190], 'Q1065': [53, 79], 'Q1067': [169, 176], 'Q1070': [81, 94], 'Q1074': [106, 106], 'Q1077': [265, 277], 'Q1078': [45, 84], 'Q1089': [125, 126], 'Q1093': [134, 134], 'Q1100': [49, 62], 'Q1102': [19, 115], 'Q1103': [125, 242], 'Q1115': [28, 114], 'Q1116': [37, 54], 'Q1118': [12, 110], 'Q1126': [5, 107], 'Q1131': [28, 72], 'Q1132': [3, 3], 'Q1135': [111, 118], 'Q1146': [195, 234], 'Q1147': [16, 65], 'Q1148': [62, 77], 'Q1151': [42, 95], 'Q1152': [271, 386], 'Q1154': [57, 298], 'Q1157': [51, 58], 'Q1159': [105, 107], 'Q1163': [80, 336], 'Q1175': [5, 28], 'Q1184': [48, 56], 'Q1191': [61, 73], 'Q1195': [200, 250], 'Q1196': [34, 44], 'Q1197': [71, 79], 'Q1198': [34, 34], 'Q1203': [104, 169], 'Q1206': [27, 69], 'Q1211': [358, 380], 'Q1212': [49, 49], 'Q1213': [20, 27], 'Q1216': [16, 219], 'Q1223': [44, 100], 'Q1226': [105, 106], 'Q1228': [85, 141], 'Q1230': [1, 60], 'Q1233': [13, 218], 'Q1236': [13, 88], 'Q1240': [182, 241], 'Q1246': [88, 129], 'Q1261': [50, 63], 'Q1264': [227, 433], 'Q1268': [24, 106], 'Q1269': [8, 8], 'Q1275': [1, 25], 'Q1277': [16, 43], 'Q1288': [53, 89], 'Q1297': [236, 286], 'Q1311': [36, 36], 'Q1312': [42, 46], 'Q1316': [40, 74], 'Q1325': [22, 76], 'Q1326': [2, 6], 'Q1338': [239, 285], 'Q1345': [200, 420], 'Q1346': [0, 18], 'Q1355': [41, 52], 'Q1358': [26, 28], 'Q1379': [2, 12], 'Q1383': [26, 43], 'Q1389': [2, 80], 'Q1391': [39, 40], 'Q1406': [34, 193], 'Q1409': [128, 270], 'Q1416': [130, 155], 'Q1418': [236, 279], 'Q1434': [81, 82], 'Q1436': [57, 96], 'Q1440': [8, 15], 'Q1442': [35, 95], 'Q1444': [227, 271], 'Q1446': [262, 269], 'Q1460': [48, 76], 'Q1465': [178, 431], 'Q1466': [18, 52], 'Q1477': [0, 0], 'Q1478': [185, 196], 'Q1485': [56, 262], 'Q1486': [108, 118], 'Q1487': [63, 102], 'Q1499': [12, 99], 'Q1506': [20, 22], 'Q1510': [154, 238], 'Q1511': [1, 35], 'Q1515': [217, 314], 'Q1516': [386, 455], 'Q1517': [5, 97], 'Q1533': [25, 29], 'Q1550': [1, 61], 'Q1551': [0, 0], 'Q1552': [27, 143], 'Q1554': [12, 39], 'Q1556': [21, 21], 'Q1559': [122, 216], 'Q1561': [16, 23], 'Q1562': [58, 70], 'Q1563': [52, 52], 'Q1564': [58, 62], 'Q1569': [21, 116], 'Q1572': [81, 173], 'Q1580': [118, 185], 'Q1582': [65, 66], 'Q1583': [115, 169], 'Q1585': [13, 20], 'Q1586': [43, 58], 'Q1597': [1, 44], 'Q1599': [13, 114], 'Q1602': [74, 106], 'Q1608': [0, 55], 'Q1613': [36, 77], 'Q1616': [57, 198], 'Q1619': [291, 348], 'Q1631': [8, 60], 'Q1633': [158, 242], 'Q1644': [23, 74], 'Q1645': [60, 141], 'Q1650': [3, 5], 'Q1655': [43, 61], 'Q1656': [10, 18], 'Q1658': [135, 137], 'Q1661': [77, 117], 'Q1662': [10, 214], 'Q1665': [153, 199], 'Q1673': [5, 46], 'Q1675': [278, 420], 'Q1685': [123, 127], 'Q1687': [26, 75], 'Q1688': [123, 169], 'Q1689': [44, 79], 'Q1695': [60, 60], 'Q1696': [15, 20], 'Q1697': [202, 326], 'Q1707': [125, 129], 'Q1712': [34, 48], 'Q1714': [98, 157], 'Q1722': [6, 62], 'Q1733': [54, 253], 'Q1736': [24, 37], 'Q1739': [127, 174], 'Q1749': [0, 63], 'Q1758': [393, 479], 'Q1759': [37, 41], 'Q1760': [187, 188], 'Q1772': [6, 40], 'Q1780': [72, 86], 'Q1792': [24, 167], 'Q1799': [89, 109], 'Q1803': [75, 94], 'Q1805': [15, 27], 'Q1806': [0, 155], 'Q1813': [13, 30], 'Q1814': [178, 310], 'Q1815': [19, 172], 'Q1817': [42, 101], 'Q1821': [53, 79], 'Q1823': [20, 175], 'Q1841': [268, 339], 'Q1842': [125, 156], 'Q1857': [137, 137], 'Q1860': [30, 33], 'Q1863': [45, 47], 'Q1865': [24, 115], 'Q1867': [12, 29], 'Q1868': [0, 115], 'Q1870': [246, 246], 'Q1871': [39, 46], 'Q1879': [55, 65], 'Q1882': [4, 97], 'Q1884': [80, 97], 'Q1892': [88, 127], 'Q1898': [21, 70], 'Q1899': [329, 359], 'Q1905': [15, 67], 'Q1913': [101, 101], 'Q1916': [160, 174], 'Q1917': [9, 9], 'Q1918': [41, 43], 'Q1922': [90, 90], 'Q1928': [158, 183], 'Q1931': [59, 145], 'Q1933': [187, 258], 'Q1935': [164, 229], 'Q1937': [12, 29], 'Q1944': [4, 14], 'Q1955': [14, 19], 'Q1957': [74, 86], 'Q1963': [28, 29], 'Q1966': [399, 663], 'Q1970': [62, 93], 'Q1973': [35, 46], 'Q1983': [153, 164], 'Q1984': [5, 7], 'Q1987': [10, 12], 'Q1992': [35, 115], 'Q1994': [99, 193], 'Q1995': [33, 71], 'Q2004': [23, 66], 'Q2009': [77, 208], 'Q2011': [10, 268], 'Q2028': [91, 118], 'Q2029': [33, 94], 'Q2034': [486, 488], 'Q2035': [2, 4], 'Q2041': [16, 58], 'Q2047': [135, 138], 'Q2051': [40, 40], 'Q2058': [8, 136], 'Q2061': [24, 148], 'Q2062': [37, 42], 'Q2065': [23, 31], 'Q2073': [39, 89], 'Q2078': [104, 243], 'Q2080': [123, 141], 'Q2095': [169, 238], 'Q2100': [0, 245], 'Q2101': [153, 160], 'Q2116': [73, 89], 'Q2121': [1, 21], 'Q2122': [2, 2], 'Q2128': [44, 71], 'Q2131': [0, 0], 'Q2140': [95, 152], 'Q2144': [63, 63], 'Q2148': [53, 67], 'Q2153': [0, 5], 'Q2169': [0, 73], 'Q2180': [115, 125], 'Q2183': [79, 101], 'Q2184': [411, 434], 'Q2185': [95, 193], 'Q2189': [230, 236], 'Q2198': [75, 89], 'Q2199': [102, 103], 'Q2200': [11, 30], 'Q2201': [228, 378], 'Q2203': [28, 421], 'Q2214': [5, 7], 'Q2216': [87, 111], 'Q2226': [99, 99], 'Q2227': [3, 9], 'Q2228': [6, 11], 'Q2236': [175, 188], 'Q2240': [31, 98], 'Q2245': [233, 243], 'Q2259': [57, 102], 'Q2261': [22, 63], 'Q2263': [15, 15], 'Q2274': [64, 64], 'Q2275': [21, 44], 'Q2279': [36, 50], 'Q2283': [85, 86], 'Q2286': [54, 83], 'Q2304': [180, 242], 'Q2308': [11, 91], 'Q2312': [1, 54], 'Q2323': [44, 174], 'Q2326': [0, 7], 'Q2346': [22, 55], 'Q2349': [52, 57], 'Q2352': [17, 87], 'Q2359': [179, 179], 'Q2365': [18, 49], 'Q2374': [45, 108], 'Q2375': [14, 35], 'Q2377': [48, 56], 'Q2382': [61, 259], 'Q2383': [41, 168], 'Q2385': [19, 28], 'Q2386': [71, 72], 'Q2395': [27, 91], 'Q2408': [130, 179], 'Q2411': [41, 93], 'Q2416': [5, 107], 'Q2417': [22, 35], 'Q2421': [305, 337], 'Q2424': [52, 92], 'Q2425': [23, 40], 'Q2428': [13, 58], 'Q2433': [28, 50], 'Q2435': [32, 86], 'Q2438': [42, 45], 'Q2443': [6, 6], 'Q2445': [425, 457], 'Q2447': [12, 57], 'Q2453': [167, 207], 'Q2462': [173, 180], 'Q2466': [43, 43], 'Q2487': [18, 392], 'Q2495': [222, 222], 'Q2498': [29, 74], 'Q2499': [215, 271], 'Q2500': [41, 175], 'Q2509': [127, 193], 'Q2511': [28, 40], 'Q2521': [10, 21], 'Q2537': [44, 226], 'Q2539': [47, 96], 'Q2549': [13, 286], 'Q2551': [1, 11], 'Q2555': [10, 35], 'Q2561': [2, 18], 'Q2567': [3, 14], 'Q2574': [46, 89], 'Q2575': [190, 247], 'Q2582': [11, 14], 'Q2590': [48, 61], 'Q2592': [29, 29], 'Q2603': [184, 230], 'Q2605': [39, 52], 'Q2606': [83, 226], 'Q2608': [65, 90], 'Q2610': [100, 149], 'Q2613': [12, 36], 'Q2615': [100, 150], 'Q2618': [205, 226], 'Q2621': [126, 344], 'Q2622': [20, 29], 'Q2623': [8, 9], 'Q2634': [22, 24], 'Q2636': [109, 126], 'Q2637': [75, 147], 'Q2640': [8, 103], 'Q2658': [45, 52], 'Q2662': [32, 79], 'Q2663': [78, 124], 'Q2675': [93, 111], 'Q2677': [221, 242], 'Q2679': [44, 95], 'Q2682': [35, 121], 'Q2684': [39, 112], 'Q2685': [73, 74], 'Q2687': [147, 173], 'Q2688': [32, 78], 'Q2698': [81, 81], 'Q2704': [247, 258], 'Q2709': [69, 80], 'Q2714': [148, 153], 'Q2720': [8, 15], 'Q2723': [38, 95], 'Q2727': [61, 97], 'Q2730': [24, 42], 'Q2734': [19, 124], 'Q2737': [27, 54], 'Q2738': [206, 335], 'Q2739': [23, 95], 'Q2741': [251, 415], 'Q2753': [55, 59], 'Q2766': [20, 20], 'Q2780': [6, 34], 'Q2788': [364, 373], 'Q2801': [210, 216], 'Q2802': [56, 104], 'Q2803': [75, 189], 'Q2806': [51, 66], 'Q2810': [469, 537], 'Q2811': [3, 7], 'Q2812': [0, 1], 'Q2815': [40, 44], 'Q2822': [73, 101], 'Q2823': [60, 63], 'Q2834': [26, 35], 'Q2836': [162, 257], 'Q2839': [24, 44], 'Q2841': [40, 395], 'Q2842': [199, 199], 'Q2843': [332, 365], 'Q2848': [24, 103], 'Q2859': [342, 420], 'Q2860': [79, 273], 'Q2868': [71, 94], 'Q2880': [186, 295], 'Q2884': [308, 312], 'Q2892': [21, 64], 'Q2896': [36, 46], 'Q2897': [116, 204], 'Q2902': [113, 140], 'Q2903': [182, 403], 'Q2921': [232, 340], 'Q2924': [102, 137], 'Q2931': [1, 17], 'Q2933': [38, 619], 'Q2934': [38, 55], 'Q2935': [82, 128], 'Q2939': [293, 329], 'Q2940': [86, 115], 'Q2942': [1, 8], 'Q2945': [11, 33], 'Q2947': [45, 58], 'Q2952': [30, 30], 'Q2953': [22, 28], 'Q2954': [61, 87], 'Q2960': [39, 60], 'Q2964': [5, 5], 'Q2965': [43, 47], 'Q2970': [87, 92], 'Q2973': [26, 84], 'Q2980': [275, 280], 'Q2981': [28, 39], 'Q2986': [54, 82], 'Q2990': [91, 136], 'Q2994': [6, 22], 'Q3004': [2, 9], 'Q3006': [54, 54], 'Q3008': [5, 61], 'Q3012': [45, 111], 'Q3013': [303, 322], 'Q3015': [71, 142], 'Q3026': [7, 8], 'Q3045': [65, 111]}\n",
            "Epoch train loss : inf| Time: 1m 8s\n",
            "Epoch valid loss: inf\n",
            "Epoch Precision: 0.08551190829996237\n",
            "Epoch Recall: 0.11626689154195645\n",
            "Epoch F1: 0.06922644645169189\n",
            "====================================================================================\n",
            "Epoch 2\n",
            "Starting training ........\n",
            "Starting batch: 0\n",
            "Starting validation .........\n",
            "Starting batch 0\n",
            "{'Q0': [97, 127], 'Q3': [149, 187], 'Q4': [25, 66], 'Q20': [31, 37], 'Q33': [67, 267], 'Q47': [217, 227], 'Q49': [159, 265], 'Q54': [21, 50], 'Q57': [131, 159], 'Q59': [32, 109], 'Q64': [165, 251], 'Q68': [291, 326], 'Q69': [135, 168], 'Q72': [56, 56], 'Q79': [10, 19], 'Q81': [160, 197], 'Q86': [111, 217], 'Q88': [49, 57], 'Q91': [33, 301], 'Q102': [64, 69], 'Q105': [25, 26], 'Q113': [291, 291], 'Q115': [14, 30], 'Q117': [24, 187], 'Q123': [2, 2], 'Q131': [27, 303], 'Q132': [62, 79], 'Q144': [0, 0], 'Q146': [92, 112], 'Q147': [125, 145], 'Q149': [225, 277], 'Q152': [22, 50], 'Q153': [42, 107], 'Q157': [40, 40], 'Q165': [0, 0], 'Q182': [0, 121], 'Q186': [38, 88], 'Q194': [17, 130], 'Q196': [0, 5], 'Q198': [0, 58], 'Q202': [75, 78], 'Q205': [133, 137], 'Q216': [79, 159], 'Q221': [0, 59], 'Q222': [81, 401], 'Q224': [162, 310], 'Q232': [72, 267], 'Q236': [239, 245], 'Q242': [22, 23], 'Q243': [227, 269], 'Q248': [0, 9], 'Q254': [66, 83], 'Q262': [107, 195], 'Q265': [24, 43], 'Q269': [185, 193], 'Q271': [19, 107], 'Q279': [247, 253], 'Q285': [22, 216], 'Q292': [61, 178], 'Q297': [8, 117], 'Q300': [294, 357], 'Q309': [151, 269], 'Q312': [139, 139], 'Q318': [34, 123], 'Q322': [71, 75], 'Q323': [32, 39], 'Q324': [52, 74], 'Q329': [50, 62], 'Q331': [52, 110], 'Q333': [160, 194], 'Q337': [45, 49], 'Q340': [74, 75], 'Q344': [3, 5], 'Q345': [20, 29], 'Q355': [15, 136], 'Q361': [38, 61], 'Q362': [167, 171], 'Q367': [6, 6], 'Q376': [208, 230], 'Q383': [31, 65], 'Q384': [17, 109], 'Q386': [80, 91], 'Q388': [62, 177], 'Q391': [15, 19], 'Q392': [96, 162], 'Q394': [158, 159], 'Q397': [0, 103], 'Q404': [135, 145], 'Q410': [111, 143], 'Q411': [107, 112], 'Q414': [30, 89], 'Q415': [24, 25], 'Q430': [6, 8], 'Q432': [207, 211], 'Q434': [2, 85], 'Q436': [30, 35], 'Q448': [7, 28], 'Q451': [180, 287], 'Q452': [54, 86], 'Q453': [0, 2], 'Q455': [151, 240], 'Q461': [160, 306], 'Q462': [1, 89], 'Q463': [37, 72], 'Q486': [60, 67], 'Q498': [95, 327], 'Q506': [8, 16], 'Q508': [67, 81], 'Q519': [1, 19], 'Q523': [21, 124], 'Q526': [118, 122], 'Q530': [82, 252], 'Q540': [19, 124], 'Q542': [2, 27], 'Q547': [3, 148], 'Q553': [1, 1], 'Q554': [4, 36], 'Q565': [30, 31], 'Q568': [198, 227], 'Q575': [22, 54], 'Q581': [8, 83], 'Q583': [69, 79], 'Q595': [191, 192], 'Q597': [78, 180], 'Q599': [216, 218], 'Q607': [1, 189], 'Q611': [35, 72], 'Q615': [111, 149], 'Q638': [20, 49], 'Q656': [78, 85], 'Q658': [194, 196], 'Q664': [57, 68], 'Q676': [360, 419], 'Q677': [113, 194], 'Q680': [202, 228], 'Q682': [47, 156], 'Q683': [97, 161], 'Q685': [28, 32], 'Q688': [61, 67], 'Q692': [122, 140], 'Q698': [4, 82], 'Q701': [396, 483], 'Q706': [32, 56], 'Q710': [72, 81], 'Q711': [164, 194], 'Q715': [25, 226], 'Q724': [210, 266], 'Q735': [172, 190], 'Q740': [132, 142], 'Q743': [2, 8], 'Q744': [94, 244], 'Q750': [174, 203], 'Q753': [30, 31], 'Q755': [22, 24], 'Q756': [83, 111], 'Q757': [97, 142], 'Q762': [147, 194], 'Q763': [131, 142], 'Q769': [1118, 1333], 'Q800': [241, 423], 'Q802': [3, 134], 'Q817': [28, 28], 'Q820': [99, 209], 'Q821': [28, 79], 'Q825': [41, 65], 'Q838': [3, 29], 'Q844': [382, 405], 'Q850': [80, 180], 'Q855': [3, 128], 'Q857': [124, 142], 'Q858': [251, 258], 'Q864': [362, 467], 'Q865': [7, 7], 'Q870': [147, 264], 'Q871': [3, 19], 'Q878': [138, 263], 'Q882': [44, 114], 'Q889': [85, 156], 'Q894': [18, 89], 'Q897': [101, 394], 'Q900': [55, 118], 'Q904': [259, 259], 'Q906': [30, 30], 'Q907': [21, 66], 'Q909': [147, 173], 'Q910': [17, 24], 'Q911': [93, 93], 'Q917': [5, 12], 'Q926': [68, 303], 'Q929': [45, 201], 'Q930': [113, 150], 'Q933': [42, 155], 'Q934': [115, 121], 'Q935': [108, 113], 'Q947': [18, 20], 'Q952': [279, 282], 'Q956': [77, 80], 'Q964': [230, 299], 'Q967': [2, 5], 'Q971': [106, 140], 'Q980': [12, 13], 'Q983': [18, 27], 'Q984': [287, 487], 'Q990': [125, 134], 'Q992': [177, 177], 'Q995': [6, 42], 'Q1012': [386, 456], 'Q1018': [36, 51], 'Q1021': [33, 227], 'Q1027': [29, 185], 'Q1032': [48, 58], 'Q1033': [233, 243], 'Q1064': [14, 190], 'Q1065': [53, 79], 'Q1067': [169, 176], 'Q1070': [81, 94], 'Q1074': [106, 106], 'Q1077': [265, 277], 'Q1078': [45, 84], 'Q1089': [125, 126], 'Q1093': [134, 134], 'Q1100': [49, 62], 'Q1102': [19, 115], 'Q1103': [125, 242], 'Q1115': [28, 114], 'Q1116': [37, 54], 'Q1118': [12, 110], 'Q1126': [5, 107], 'Q1131': [28, 72], 'Q1132': [3, 3], 'Q1135': [111, 118], 'Q1146': [195, 234], 'Q1147': [16, 65], 'Q1148': [62, 77], 'Q1151': [42, 95], 'Q1152': [271, 386], 'Q1154': [57, 298], 'Q1157': [51, 58], 'Q1159': [105, 107], 'Q1163': [80, 336], 'Q1175': [5, 28], 'Q1184': [48, 56], 'Q1191': [61, 73], 'Q1195': [200, 250], 'Q1196': [34, 44], 'Q1197': [71, 79], 'Q1198': [34, 34], 'Q1203': [104, 169], 'Q1206': [27, 69], 'Q1211': [358, 380], 'Q1212': [49, 49], 'Q1213': [20, 27], 'Q1216': [16, 219], 'Q1223': [44, 100], 'Q1226': [105, 106], 'Q1228': [85, 141], 'Q1230': [1, 60], 'Q1233': [13, 218], 'Q1236': [13, 88], 'Q1240': [182, 241], 'Q1246': [88, 129], 'Q1261': [50, 63], 'Q1264': [227, 433], 'Q1268': [24, 106], 'Q1269': [8, 8], 'Q1275': [1, 25], 'Q1277': [16, 43], 'Q1288': [53, 89], 'Q1297': [236, 286], 'Q1311': [36, 36], 'Q1312': [42, 46], 'Q1316': [40, 74], 'Q1325': [22, 76], 'Q1326': [2, 6], 'Q1338': [239, 285], 'Q1345': [200, 420], 'Q1346': [0, 18], 'Q1355': [41, 52], 'Q1358': [26, 28], 'Q1379': [2, 12], 'Q1383': [26, 43], 'Q1389': [2, 80], 'Q1391': [39, 40], 'Q1406': [34, 193], 'Q1409': [128, 270], 'Q1416': [130, 155], 'Q1418': [236, 279], 'Q1434': [81, 82], 'Q1436': [57, 96], 'Q1440': [8, 15], 'Q1442': [35, 95], 'Q1444': [227, 271], 'Q1446': [262, 269], 'Q1460': [48, 76], 'Q1465': [178, 431], 'Q1466': [18, 52], 'Q1477': [0, 0], 'Q1478': [185, 196], 'Q1485': [56, 262], 'Q1486': [108, 118], 'Q1487': [63, 102], 'Q1499': [12, 99], 'Q1506': [20, 22], 'Q1510': [154, 238], 'Q1511': [1, 35], 'Q1515': [217, 314], 'Q1516': [386, 455], 'Q1517': [5, 97], 'Q1533': [25, 29], 'Q1550': [1, 61], 'Q1551': [0, 0], 'Q1552': [27, 143], 'Q1554': [12, 39], 'Q1556': [21, 21], 'Q1559': [122, 216], 'Q1561': [16, 23], 'Q1562': [58, 70], 'Q1563': [52, 52], 'Q1564': [58, 62], 'Q1569': [21, 116], 'Q1572': [81, 173], 'Q1580': [118, 185], 'Q1582': [65, 66], 'Q1583': [115, 169], 'Q1585': [13, 20], 'Q1586': [43, 58], 'Q1597': [1, 44], 'Q1599': [13, 114], 'Q1602': [74, 106], 'Q1608': [0, 55], 'Q1613': [36, 77], 'Q1616': [57, 198], 'Q1619': [291, 348], 'Q1631': [8, 60], 'Q1633': [158, 242], 'Q1644': [23, 74], 'Q1645': [60, 141], 'Q1650': [3, 5], 'Q1655': [43, 61], 'Q1656': [10, 18], 'Q1658': [135, 137], 'Q1661': [77, 117], 'Q1662': [10, 214], 'Q1665': [153, 199], 'Q1673': [5, 46], 'Q1675': [278, 420], 'Q1685': [123, 127], 'Q1687': [26, 75], 'Q1688': [123, 169], 'Q1689': [44, 79], 'Q1695': [60, 60], 'Q1696': [15, 20], 'Q1697': [202, 326], 'Q1707': [125, 129], 'Q1712': [34, 48], 'Q1714': [98, 157], 'Q1722': [6, 62], 'Q1733': [54, 253], 'Q1736': [24, 37], 'Q1739': [127, 174], 'Q1749': [0, 63], 'Q1758': [393, 479], 'Q1759': [37, 41], 'Q1760': [187, 188], 'Q1772': [6, 40], 'Q1780': [72, 86], 'Q1792': [24, 167], 'Q1799': [89, 109], 'Q1803': [75, 94], 'Q1805': [15, 27], 'Q1806': [0, 155], 'Q1813': [13, 30], 'Q1814': [178, 310], 'Q1815': [19, 172], 'Q1817': [42, 101], 'Q1821': [53, 79], 'Q1823': [20, 175], 'Q1841': [268, 339], 'Q1842': [125, 156], 'Q1857': [137, 137], 'Q1860': [30, 33], 'Q1863': [45, 47], 'Q1865': [24, 115], 'Q1867': [12, 29], 'Q1868': [0, 115], 'Q1870': [246, 246], 'Q1871': [39, 46], 'Q1879': [55, 65], 'Q1882': [4, 97], 'Q1884': [80, 97], 'Q1892': [88, 127], 'Q1898': [21, 70], 'Q1899': [329, 359], 'Q1905': [15, 67], 'Q1913': [101, 101], 'Q1916': [160, 174], 'Q1917': [9, 9], 'Q1918': [41, 43], 'Q1922': [90, 90], 'Q1928': [158, 183], 'Q1931': [59, 145], 'Q1933': [187, 258], 'Q1935': [164, 229], 'Q1937': [12, 29], 'Q1944': [4, 14], 'Q1955': [14, 19], 'Q1957': [74, 86], 'Q1963': [28, 29], 'Q1966': [399, 663], 'Q1970': [62, 93], 'Q1973': [35, 46], 'Q1983': [153, 164], 'Q1984': [5, 7], 'Q1987': [10, 12], 'Q1992': [35, 115], 'Q1994': [99, 193], 'Q1995': [33, 71], 'Q2004': [23, 66], 'Q2009': [77, 208], 'Q2011': [10, 268], 'Q2028': [91, 118], 'Q2029': [33, 94], 'Q2034': [486, 488], 'Q2035': [2, 4], 'Q2041': [16, 58], 'Q2047': [135, 138], 'Q2051': [40, 40], 'Q2058': [8, 136], 'Q2061': [24, 148], 'Q2062': [37, 42], 'Q2065': [23, 31], 'Q2073': [39, 89], 'Q2078': [104, 243], 'Q2080': [123, 141], 'Q2095': [169, 238], 'Q2100': [0, 245], 'Q2101': [153, 160], 'Q2116': [73, 89], 'Q2121': [1, 21], 'Q2122': [2, 2], 'Q2128': [44, 71], 'Q2131': [0, 0], 'Q2140': [95, 152], 'Q2144': [63, 63], 'Q2148': [53, 67], 'Q2153': [0, 5], 'Q2169': [0, 73], 'Q2180': [115, 125], 'Q2183': [79, 101], 'Q2184': [411, 434], 'Q2185': [95, 193], 'Q2189': [230, 236], 'Q2198': [75, 89], 'Q2199': [102, 103], 'Q2200': [11, 30], 'Q2201': [228, 378], 'Q2203': [28, 421], 'Q2214': [5, 7], 'Q2216': [87, 111], 'Q2226': [99, 99], 'Q2227': [3, 9], 'Q2228': [6, 11], 'Q2236': [175, 188], 'Q2240': [31, 98], 'Q2245': [233, 243], 'Q2259': [57, 102], 'Q2261': [22, 63], 'Q2263': [15, 15], 'Q2274': [64, 64], 'Q2275': [21, 44], 'Q2279': [36, 50], 'Q2283': [85, 86], 'Q2286': [54, 83], 'Q2304': [180, 242], 'Q2308': [11, 91], 'Q2312': [1, 54], 'Q2323': [44, 174], 'Q2326': [0, 7], 'Q2346': [22, 55], 'Q2349': [52, 57], 'Q2352': [17, 87], 'Q2359': [179, 179], 'Q2365': [18, 49], 'Q2374': [45, 108], 'Q2375': [14, 35], 'Q2377': [48, 56], 'Q2382': [61, 259], 'Q2383': [41, 168], 'Q2385': [19, 28], 'Q2386': [71, 72], 'Q2395': [27, 91], 'Q2408': [130, 179], 'Q2411': [41, 93], 'Q2416': [5, 107], 'Q2417': [22, 35], 'Q2421': [305, 337], 'Q2424': [52, 92], 'Q2425': [23, 40], 'Q2428': [13, 58], 'Q2433': [28, 50], 'Q2435': [32, 86], 'Q2438': [42, 45], 'Q2443': [6, 6], 'Q2445': [425, 457], 'Q2447': [12, 57], 'Q2453': [167, 207], 'Q2462': [173, 180], 'Q2466': [43, 43], 'Q2487': [18, 392], 'Q2495': [222, 222], 'Q2498': [29, 74], 'Q2499': [215, 271], 'Q2500': [41, 175], 'Q2509': [127, 193], 'Q2511': [28, 40], 'Q2521': [10, 21], 'Q2537': [44, 226], 'Q2539': [47, 96], 'Q2549': [13, 286], 'Q2551': [1, 11], 'Q2555': [10, 35], 'Q2561': [2, 18], 'Q2567': [3, 14], 'Q2574': [46, 89], 'Q2575': [190, 247], 'Q2582': [11, 14], 'Q2590': [48, 61], 'Q2592': [29, 29], 'Q2603': [184, 230], 'Q2605': [39, 52], 'Q2606': [83, 226], 'Q2608': [65, 90], 'Q2610': [100, 149], 'Q2613': [12, 36], 'Q2615': [100, 150], 'Q2618': [205, 226], 'Q2621': [126, 344], 'Q2622': [20, 29], 'Q2623': [8, 9], 'Q2634': [22, 24], 'Q2636': [109, 126], 'Q2637': [75, 147], 'Q2640': [8, 103], 'Q2658': [45, 52], 'Q2662': [32, 79], 'Q2663': [78, 124], 'Q2675': [93, 111], 'Q2677': [221, 242], 'Q2679': [44, 95], 'Q2682': [35, 121], 'Q2684': [39, 112], 'Q2685': [73, 74], 'Q2687': [147, 173], 'Q2688': [32, 78], 'Q2698': [81, 81], 'Q2704': [247, 258], 'Q2709': [69, 80], 'Q2714': [148, 153], 'Q2720': [8, 15], 'Q2723': [38, 95], 'Q2727': [61, 97], 'Q2730': [24, 42], 'Q2734': [19, 124], 'Q2737': [27, 54], 'Q2738': [206, 335], 'Q2739': [23, 95], 'Q2741': [251, 415], 'Q2753': [55, 59], 'Q2766': [20, 20], 'Q2780': [6, 34], 'Q2788': [364, 373], 'Q2801': [210, 216], 'Q2802': [56, 104], 'Q2803': [75, 189], 'Q2806': [51, 66], 'Q2810': [469, 537], 'Q2811': [3, 7], 'Q2812': [0, 1], 'Q2815': [40, 44], 'Q2822': [73, 101], 'Q2823': [60, 63], 'Q2834': [26, 35], 'Q2836': [162, 257], 'Q2839': [24, 44], 'Q2841': [40, 395], 'Q2842': [199, 199], 'Q2843': [332, 365], 'Q2848': [24, 103], 'Q2859': [342, 420], 'Q2860': [79, 273], 'Q2868': [71, 94], 'Q2880': [186, 295], 'Q2884': [308, 312], 'Q2892': [21, 64], 'Q2896': [36, 46], 'Q2897': [116, 204], 'Q2902': [113, 140], 'Q2903': [182, 403], 'Q2921': [232, 340], 'Q2924': [102, 137], 'Q2931': [1, 17], 'Q2933': [38, 619], 'Q2934': [38, 55], 'Q2935': [82, 128], 'Q2939': [293, 329], 'Q2940': [86, 115], 'Q2942': [1, 8], 'Q2945': [11, 33], 'Q2947': [45, 58], 'Q2952': [30, 30], 'Q2953': [22, 28], 'Q2954': [61, 87], 'Q2960': [39, 60], 'Q2964': [5, 5], 'Q2965': [43, 47], 'Q2970': [87, 92], 'Q2973': [26, 84], 'Q2980': [275, 280], 'Q2981': [28, 39], 'Q2986': [54, 82], 'Q2990': [91, 136], 'Q2994': [6, 22], 'Q3004': [2, 9], 'Q3006': [54, 54], 'Q3008': [5, 61], 'Q3012': [45, 111], 'Q3013': [303, 322], 'Q3015': [71, 142], 'Q3026': [7, 8], 'Q3045': [65, 111]}\n",
            "Epoch train loss : inf| Time: 1m 9s\n",
            "Epoch valid loss: inf\n",
            "Epoch Precision: 0.08551190829996237\n",
            "Epoch Recall: 0.11626689154195645\n",
            "Epoch F1: 0.06922644645169189\n",
            "====================================================================================\n",
            "Epoch 3\n",
            "Starting training ........\n",
            "Starting batch: 0\n",
            "Starting validation .........\n",
            "Starting batch 0\n",
            "{'Q0': [97, 127], 'Q3': [149, 187], 'Q4': [25, 66], 'Q20': [31, 37], 'Q33': [67, 267], 'Q47': [217, 227], 'Q49': [159, 265], 'Q54': [21, 50], 'Q57': [131, 159], 'Q59': [32, 109], 'Q64': [165, 251], 'Q68': [291, 326], 'Q69': [135, 168], 'Q72': [56, 56], 'Q79': [10, 19], 'Q81': [160, 197], 'Q86': [111, 217], 'Q88': [49, 57], 'Q91': [33, 301], 'Q102': [64, 69], 'Q105': [25, 26], 'Q113': [291, 291], 'Q115': [14, 30], 'Q117': [24, 187], 'Q123': [2, 2], 'Q131': [27, 303], 'Q132': [62, 79], 'Q144': [0, 0], 'Q146': [92, 112], 'Q147': [125, 145], 'Q149': [225, 277], 'Q152': [22, 50], 'Q153': [42, 107], 'Q157': [40, 40], 'Q165': [0, 0], 'Q182': [0, 121], 'Q186': [38, 88], 'Q194': [17, 130], 'Q196': [0, 5], 'Q198': [0, 58], 'Q202': [75, 78], 'Q205': [133, 137], 'Q216': [79, 159], 'Q221': [0, 59], 'Q222': [81, 401], 'Q224': [162, 310], 'Q232': [72, 267], 'Q236': [239, 245], 'Q242': [22, 23], 'Q243': [227, 269], 'Q248': [0, 9], 'Q254': [66, 83], 'Q262': [107, 195], 'Q265': [24, 43], 'Q269': [185, 193], 'Q271': [19, 107], 'Q279': [247, 253], 'Q285': [22, 216], 'Q292': [61, 178], 'Q297': [8, 117], 'Q300': [294, 357], 'Q309': [151, 269], 'Q312': [139, 139], 'Q318': [34, 123], 'Q322': [71, 75], 'Q323': [32, 39], 'Q324': [52, 74], 'Q329': [50, 62], 'Q331': [52, 110], 'Q333': [160, 194], 'Q337': [45, 49], 'Q340': [74, 75], 'Q344': [3, 5], 'Q345': [20, 29], 'Q355': [15, 136], 'Q361': [38, 61], 'Q362': [167, 171], 'Q367': [6, 6], 'Q376': [208, 230], 'Q383': [31, 65], 'Q384': [17, 109], 'Q386': [80, 91], 'Q388': [62, 177], 'Q391': [15, 19], 'Q392': [96, 162], 'Q394': [158, 159], 'Q397': [0, 103], 'Q404': [135, 145], 'Q410': [111, 143], 'Q411': [107, 112], 'Q414': [30, 89], 'Q415': [24, 25], 'Q430': [6, 8], 'Q432': [207, 211], 'Q434': [2, 85], 'Q436': [30, 35], 'Q448': [7, 28], 'Q451': [180, 287], 'Q452': [54, 86], 'Q453': [0, 2], 'Q455': [151, 240], 'Q461': [160, 306], 'Q462': [1, 89], 'Q463': [37, 72], 'Q486': [60, 67], 'Q498': [95, 327], 'Q506': [8, 16], 'Q508': [67, 81], 'Q519': [1, 19], 'Q523': [21, 124], 'Q526': [118, 122], 'Q530': [82, 252], 'Q540': [19, 124], 'Q542': [2, 27], 'Q547': [3, 148], 'Q553': [1, 1], 'Q554': [4, 36], 'Q565': [30, 31], 'Q568': [198, 227], 'Q575': [22, 54], 'Q581': [8, 83], 'Q583': [69, 79], 'Q595': [191, 192], 'Q597': [78, 180], 'Q599': [216, 218], 'Q607': [1, 189], 'Q611': [35, 72], 'Q615': [111, 149], 'Q638': [20, 49], 'Q656': [78, 85], 'Q658': [194, 196], 'Q664': [57, 68], 'Q676': [360, 419], 'Q677': [113, 194], 'Q680': [202, 228], 'Q682': [47, 156], 'Q683': [97, 161], 'Q685': [28, 32], 'Q688': [61, 67], 'Q692': [122, 140], 'Q698': [4, 82], 'Q701': [396, 483], 'Q706': [32, 56], 'Q710': [72, 81], 'Q711': [164, 194], 'Q715': [25, 226], 'Q724': [210, 266], 'Q735': [172, 190], 'Q740': [132, 142], 'Q743': [2, 8], 'Q744': [94, 244], 'Q750': [174, 203], 'Q753': [30, 31], 'Q755': [22, 24], 'Q756': [83, 111], 'Q757': [97, 142], 'Q762': [147, 194], 'Q763': [131, 142], 'Q769': [1118, 1333], 'Q800': [241, 423], 'Q802': [3, 134], 'Q817': [28, 28], 'Q820': [99, 209], 'Q821': [28, 79], 'Q825': [41, 65], 'Q838': [3, 29], 'Q844': [382, 405], 'Q850': [80, 180], 'Q855': [3, 128], 'Q857': [124, 142], 'Q858': [251, 258], 'Q864': [362, 467], 'Q865': [7, 7], 'Q870': [147, 264], 'Q871': [3, 19], 'Q878': [138, 263], 'Q882': [44, 114], 'Q889': [85, 156], 'Q894': [18, 89], 'Q897': [101, 394], 'Q900': [55, 118], 'Q904': [259, 259], 'Q906': [30, 30], 'Q907': [21, 66], 'Q909': [147, 173], 'Q910': [17, 24], 'Q911': [93, 93], 'Q917': [5, 12], 'Q926': [68, 303], 'Q929': [45, 201], 'Q930': [113, 150], 'Q933': [42, 155], 'Q934': [115, 121], 'Q935': [108, 113], 'Q947': [18, 20], 'Q952': [279, 282], 'Q956': [77, 80], 'Q964': [230, 299], 'Q967': [2, 5], 'Q971': [106, 140], 'Q980': [12, 13], 'Q983': [18, 27], 'Q984': [287, 487], 'Q990': [125, 134], 'Q992': [177, 177], 'Q995': [6, 42], 'Q1012': [386, 456], 'Q1018': [36, 51], 'Q1021': [33, 227], 'Q1027': [29, 185], 'Q1032': [48, 58], 'Q1033': [233, 243], 'Q1064': [14, 190], 'Q1065': [53, 79], 'Q1067': [169, 176], 'Q1070': [81, 94], 'Q1074': [106, 106], 'Q1077': [265, 277], 'Q1078': [45, 84], 'Q1089': [125, 126], 'Q1093': [134, 134], 'Q1100': [49, 62], 'Q1102': [19, 115], 'Q1103': [125, 242], 'Q1115': [28, 114], 'Q1116': [37, 54], 'Q1118': [12, 110], 'Q1126': [5, 107], 'Q1131': [28, 72], 'Q1132': [3, 3], 'Q1135': [111, 118], 'Q1146': [195, 234], 'Q1147': [16, 65], 'Q1148': [62, 77], 'Q1151': [42, 95], 'Q1152': [271, 386], 'Q1154': [57, 298], 'Q1157': [51, 58], 'Q1159': [105, 107], 'Q1163': [80, 336], 'Q1175': [5, 28], 'Q1184': [48, 56], 'Q1191': [61, 73], 'Q1195': [200, 250], 'Q1196': [34, 44], 'Q1197': [71, 79], 'Q1198': [34, 34], 'Q1203': [104, 169], 'Q1206': [27, 69], 'Q1211': [358, 380], 'Q1212': [49, 49], 'Q1213': [20, 27], 'Q1216': [16, 219], 'Q1223': [44, 100], 'Q1226': [105, 106], 'Q1228': [85, 141], 'Q1230': [1, 60], 'Q1233': [13, 218], 'Q1236': [13, 88], 'Q1240': [182, 241], 'Q1246': [88, 129], 'Q1261': [50, 63], 'Q1264': [227, 433], 'Q1268': [24, 106], 'Q1269': [8, 8], 'Q1275': [1, 25], 'Q1277': [16, 43], 'Q1288': [53, 89], 'Q1297': [236, 286], 'Q1311': [36, 36], 'Q1312': [42, 46], 'Q1316': [40, 74], 'Q1325': [22, 76], 'Q1326': [2, 6], 'Q1338': [239, 285], 'Q1345': [200, 420], 'Q1346': [0, 18], 'Q1355': [41, 52], 'Q1358': [26, 28], 'Q1379': [2, 12], 'Q1383': [26, 43], 'Q1389': [2, 80], 'Q1391': [39, 40], 'Q1406': [34, 193], 'Q1409': [128, 270], 'Q1416': [130, 155], 'Q1418': [236, 279], 'Q1434': [81, 82], 'Q1436': [57, 96], 'Q1440': [8, 15], 'Q1442': [35, 95], 'Q1444': [227, 271], 'Q1446': [262, 269], 'Q1460': [48, 76], 'Q1465': [178, 431], 'Q1466': [18, 52], 'Q1477': [0, 0], 'Q1478': [185, 196], 'Q1485': [56, 262], 'Q1486': [108, 118], 'Q1487': [63, 102], 'Q1499': [12, 99], 'Q1506': [20, 22], 'Q1510': [154, 238], 'Q1511': [1, 35], 'Q1515': [217, 314], 'Q1516': [386, 455], 'Q1517': [5, 97], 'Q1533': [25, 29], 'Q1550': [1, 61], 'Q1551': [0, 0], 'Q1552': [27, 143], 'Q1554': [12, 39], 'Q1556': [21, 21], 'Q1559': [122, 216], 'Q1561': [16, 23], 'Q1562': [58, 70], 'Q1563': [52, 52], 'Q1564': [58, 62], 'Q1569': [21, 116], 'Q1572': [81, 173], 'Q1580': [118, 185], 'Q1582': [65, 66], 'Q1583': [115, 169], 'Q1585': [13, 20], 'Q1586': [43, 58], 'Q1597': [1, 44], 'Q1599': [13, 114], 'Q1602': [74, 106], 'Q1608': [0, 55], 'Q1613': [36, 77], 'Q1616': [57, 198], 'Q1619': [291, 348], 'Q1631': [8, 60], 'Q1633': [158, 242], 'Q1644': [23, 74], 'Q1645': [60, 141], 'Q1650': [3, 5], 'Q1655': [43, 61], 'Q1656': [10, 18], 'Q1658': [135, 137], 'Q1661': [77, 117], 'Q1662': [10, 214], 'Q1665': [153, 199], 'Q1673': [5, 46], 'Q1675': [278, 420], 'Q1685': [123, 127], 'Q1687': [26, 75], 'Q1688': [123, 169], 'Q1689': [44, 79], 'Q1695': [60, 60], 'Q1696': [15, 20], 'Q1697': [202, 326], 'Q1707': [125, 129], 'Q1712': [34, 48], 'Q1714': [98, 157], 'Q1722': [6, 62], 'Q1733': [54, 253], 'Q1736': [24, 37], 'Q1739': [127, 174], 'Q1749': [0, 63], 'Q1758': [393, 479], 'Q1759': [37, 41], 'Q1760': [187, 188], 'Q1772': [6, 40], 'Q1780': [72, 86], 'Q1792': [24, 167], 'Q1799': [89, 109], 'Q1803': [75, 94], 'Q1805': [15, 27], 'Q1806': [0, 155], 'Q1813': [13, 30], 'Q1814': [178, 310], 'Q1815': [19, 172], 'Q1817': [42, 101], 'Q1821': [53, 79], 'Q1823': [20, 175], 'Q1841': [268, 339], 'Q1842': [125, 156], 'Q1857': [137, 137], 'Q1860': [30, 33], 'Q1863': [45, 47], 'Q1865': [24, 115], 'Q1867': [12, 29], 'Q1868': [0, 115], 'Q1870': [246, 246], 'Q1871': [39, 46], 'Q1879': [55, 65], 'Q1882': [4, 97], 'Q1884': [80, 97], 'Q1892': [88, 127], 'Q1898': [21, 70], 'Q1899': [329, 359], 'Q1905': [15, 67], 'Q1913': [101, 101], 'Q1916': [160, 174], 'Q1917': [9, 9], 'Q1918': [41, 43], 'Q1922': [90, 90], 'Q1928': [158, 183], 'Q1931': [59, 145], 'Q1933': [187, 258], 'Q1935': [164, 229], 'Q1937': [12, 29], 'Q1944': [4, 14], 'Q1955': [14, 19], 'Q1957': [74, 86], 'Q1963': [28, 29], 'Q1966': [399, 663], 'Q1970': [62, 93], 'Q1973': [35, 46], 'Q1983': [153, 164], 'Q1984': [5, 7], 'Q1987': [10, 12], 'Q1992': [35, 115], 'Q1994': [99, 193], 'Q1995': [33, 71], 'Q2004': [23, 66], 'Q2009': [77, 208], 'Q2011': [10, 268], 'Q2028': [91, 118], 'Q2029': [33, 94], 'Q2034': [486, 488], 'Q2035': [2, 4], 'Q2041': [16, 58], 'Q2047': [135, 138], 'Q2051': [40, 40], 'Q2058': [8, 136], 'Q2061': [24, 148], 'Q2062': [37, 42], 'Q2065': [23, 31], 'Q2073': [39, 89], 'Q2078': [104, 243], 'Q2080': [123, 141], 'Q2095': [169, 238], 'Q2100': [0, 245], 'Q2101': [153, 160], 'Q2116': [73, 89], 'Q2121': [1, 21], 'Q2122': [2, 2], 'Q2128': [44, 71], 'Q2131': [0, 0], 'Q2140': [95, 152], 'Q2144': [63, 63], 'Q2148': [53, 67], 'Q2153': [0, 5], 'Q2169': [0, 73], 'Q2180': [115, 125], 'Q2183': [79, 101], 'Q2184': [411, 434], 'Q2185': [95, 193], 'Q2189': [230, 236], 'Q2198': [75, 89], 'Q2199': [102, 103], 'Q2200': [11, 30], 'Q2201': [228, 378], 'Q2203': [28, 421], 'Q2214': [5, 7], 'Q2216': [87, 111], 'Q2226': [99, 99], 'Q2227': [3, 9], 'Q2228': [6, 11], 'Q2236': [175, 188], 'Q2240': [31, 98], 'Q2245': [233, 243], 'Q2259': [57, 102], 'Q2261': [22, 63], 'Q2263': [15, 15], 'Q2274': [64, 64], 'Q2275': [21, 44], 'Q2279': [36, 50], 'Q2283': [85, 86], 'Q2286': [54, 83], 'Q2304': [180, 242], 'Q2308': [11, 91], 'Q2312': [1, 54], 'Q2323': [44, 174], 'Q2326': [0, 7], 'Q2346': [22, 55], 'Q2349': [52, 57], 'Q2352': [17, 87], 'Q2359': [179, 179], 'Q2365': [18, 49], 'Q2374': [45, 108], 'Q2375': [14, 35], 'Q2377': [48, 56], 'Q2382': [61, 259], 'Q2383': [41, 168], 'Q2385': [19, 28], 'Q2386': [71, 72], 'Q2395': [27, 91], 'Q2408': [130, 179], 'Q2411': [41, 93], 'Q2416': [5, 107], 'Q2417': [22, 35], 'Q2421': [305, 337], 'Q2424': [52, 92], 'Q2425': [23, 40], 'Q2428': [13, 58], 'Q2433': [28, 50], 'Q2435': [32, 86], 'Q2438': [42, 45], 'Q2443': [6, 6], 'Q2445': [425, 457], 'Q2447': [12, 57], 'Q2453': [167, 207], 'Q2462': [173, 180], 'Q2466': [43, 43], 'Q2487': [18, 392], 'Q2495': [222, 222], 'Q2498': [29, 74], 'Q2499': [215, 271], 'Q2500': [41, 175], 'Q2509': [127, 193], 'Q2511': [28, 40], 'Q2521': [10, 21], 'Q2537': [44, 226], 'Q2539': [47, 96], 'Q2549': [13, 286], 'Q2551': [1, 11], 'Q2555': [10, 35], 'Q2561': [2, 18], 'Q2567': [3, 14], 'Q2574': [46, 89], 'Q2575': [190, 247], 'Q2582': [11, 14], 'Q2590': [48, 61], 'Q2592': [29, 29], 'Q2603': [184, 230], 'Q2605': [39, 52], 'Q2606': [83, 226], 'Q2608': [65, 90], 'Q2610': [100, 149], 'Q2613': [12, 36], 'Q2615': [100, 150], 'Q2618': [205, 226], 'Q2621': [126, 344], 'Q2622': [20, 29], 'Q2623': [8, 9], 'Q2634': [22, 24], 'Q2636': [109, 126], 'Q2637': [75, 147], 'Q2640': [8, 103], 'Q2658': [45, 52], 'Q2662': [32, 79], 'Q2663': [78, 124], 'Q2675': [93, 111], 'Q2677': [221, 242], 'Q2679': [44, 95], 'Q2682': [35, 121], 'Q2684': [39, 112], 'Q2685': [73, 74], 'Q2687': [147, 173], 'Q2688': [32, 78], 'Q2698': [81, 81], 'Q2704': [247, 258], 'Q2709': [69, 80], 'Q2714': [148, 153], 'Q2720': [8, 15], 'Q2723': [38, 95], 'Q2727': [61, 97], 'Q2730': [24, 42], 'Q2734': [19, 124], 'Q2737': [27, 54], 'Q2738': [206, 335], 'Q2739': [23, 95], 'Q2741': [251, 415], 'Q2753': [55, 59], 'Q2766': [20, 20], 'Q2780': [6, 34], 'Q2788': [364, 373], 'Q2801': [210, 216], 'Q2802': [56, 104], 'Q2803': [75, 189], 'Q2806': [51, 66], 'Q2810': [469, 537], 'Q2811': [3, 7], 'Q2812': [0, 1], 'Q2815': [40, 44], 'Q2822': [73, 101], 'Q2823': [60, 63], 'Q2834': [26, 35], 'Q2836': [162, 257], 'Q2839': [24, 44], 'Q2841': [40, 395], 'Q2842': [199, 199], 'Q2843': [332, 365], 'Q2848': [24, 103], 'Q2859': [342, 420], 'Q2860': [79, 273], 'Q2868': [71, 94], 'Q2880': [186, 295], 'Q2884': [308, 312], 'Q2892': [21, 64], 'Q2896': [36, 46], 'Q2897': [116, 204], 'Q2902': [113, 140], 'Q2903': [182, 403], 'Q2921': [232, 340], 'Q2924': [102, 137], 'Q2931': [1, 17], 'Q2933': [38, 619], 'Q2934': [38, 55], 'Q2935': [82, 128], 'Q2939': [293, 329], 'Q2940': [86, 115], 'Q2942': [1, 8], 'Q2945': [11, 33], 'Q2947': [45, 58], 'Q2952': [30, 30], 'Q2953': [22, 28], 'Q2954': [61, 87], 'Q2960': [39, 60], 'Q2964': [5, 5], 'Q2965': [43, 47], 'Q2970': [87, 92], 'Q2973': [26, 84], 'Q2980': [275, 280], 'Q2981': [28, 39], 'Q2986': [54, 82], 'Q2990': [91, 136], 'Q2994': [6, 22], 'Q3004': [2, 9], 'Q3006': [54, 54], 'Q3008': [5, 61], 'Q3012': [45, 111], 'Q3013': [303, 322], 'Q3015': [71, 142], 'Q3026': [7, 8], 'Q3045': [65, 111]}\n",
            "Epoch train loss : inf| Time: 1m 9s\n",
            "Epoch valid loss: inf\n",
            "Epoch Precision: 0.08551190829996237\n",
            "Epoch Recall: 0.11626689154195645\n",
            "Epoch F1: 0.06922644645169189\n",
            "====================================================================================\n",
            "Epoch 4\n",
            "Starting training ........\n",
            "Starting batch: 0\n",
            "Starting validation .........\n",
            "Starting batch 0\n",
            "{'Q0': [97, 127], 'Q3': [149, 187], 'Q4': [25, 66], 'Q20': [31, 37], 'Q33': [67, 267], 'Q47': [217, 227], 'Q49': [159, 265], 'Q54': [21, 50], 'Q57': [131, 159], 'Q59': [32, 109], 'Q64': [165, 251], 'Q68': [291, 326], 'Q69': [135, 168], 'Q72': [56, 56], 'Q79': [10, 19], 'Q81': [160, 197], 'Q86': [111, 217], 'Q88': [49, 57], 'Q91': [33, 301], 'Q102': [64, 69], 'Q105': [25, 26], 'Q113': [291, 291], 'Q115': [14, 30], 'Q117': [24, 187], 'Q123': [2, 2], 'Q131': [27, 303], 'Q132': [62, 79], 'Q144': [0, 0], 'Q146': [92, 112], 'Q147': [125, 145], 'Q149': [225, 277], 'Q152': [22, 50], 'Q153': [42, 107], 'Q157': [40, 40], 'Q165': [0, 0], 'Q182': [0, 121], 'Q186': [38, 88], 'Q194': [17, 130], 'Q196': [0, 5], 'Q198': [0, 58], 'Q202': [75, 78], 'Q205': [133, 137], 'Q216': [79, 159], 'Q221': [0, 59], 'Q222': [81, 401], 'Q224': [162, 310], 'Q232': [72, 267], 'Q236': [239, 245], 'Q242': [22, 23], 'Q243': [227, 269], 'Q248': [0, 9], 'Q254': [66, 83], 'Q262': [107, 195], 'Q265': [24, 43], 'Q269': [185, 193], 'Q271': [19, 107], 'Q279': [247, 253], 'Q285': [22, 216], 'Q292': [61, 178], 'Q297': [8, 117], 'Q300': [294, 357], 'Q309': [151, 269], 'Q312': [139, 139], 'Q318': [34, 123], 'Q322': [71, 75], 'Q323': [32, 39], 'Q324': [52, 74], 'Q329': [50, 62], 'Q331': [52, 110], 'Q333': [160, 194], 'Q337': [45, 49], 'Q340': [74, 75], 'Q344': [3, 5], 'Q345': [20, 29], 'Q355': [15, 136], 'Q361': [38, 61], 'Q362': [167, 171], 'Q367': [6, 6], 'Q376': [208, 230], 'Q383': [31, 65], 'Q384': [17, 109], 'Q386': [80, 91], 'Q388': [62, 177], 'Q391': [15, 19], 'Q392': [96, 162], 'Q394': [158, 159], 'Q397': [0, 103], 'Q404': [135, 145], 'Q410': [111, 143], 'Q411': [107, 112], 'Q414': [30, 89], 'Q415': [24, 25], 'Q430': [6, 8], 'Q432': [207, 211], 'Q434': [2, 85], 'Q436': [30, 35], 'Q448': [7, 28], 'Q451': [180, 287], 'Q452': [54, 86], 'Q453': [0, 2], 'Q455': [151, 240], 'Q461': [160, 306], 'Q462': [1, 89], 'Q463': [37, 72], 'Q486': [60, 67], 'Q498': [95, 327], 'Q506': [8, 16], 'Q508': [67, 81], 'Q519': [1, 19], 'Q523': [21, 124], 'Q526': [118, 122], 'Q530': [82, 252], 'Q540': [19, 124], 'Q542': [2, 27], 'Q547': [3, 148], 'Q553': [1, 1], 'Q554': [4, 36], 'Q565': [30, 31], 'Q568': [198, 227], 'Q575': [22, 54], 'Q581': [8, 83], 'Q583': [69, 79], 'Q595': [191, 192], 'Q597': [78, 180], 'Q599': [216, 218], 'Q607': [1, 189], 'Q611': [35, 72], 'Q615': [111, 149], 'Q638': [20, 49], 'Q656': [78, 85], 'Q658': [194, 196], 'Q664': [57, 68], 'Q676': [360, 419], 'Q677': [113, 194], 'Q680': [202, 228], 'Q682': [47, 156], 'Q683': [97, 161], 'Q685': [28, 32], 'Q688': [61, 67], 'Q692': [122, 140], 'Q698': [4, 82], 'Q701': [396, 483], 'Q706': [32, 56], 'Q710': [72, 81], 'Q711': [164, 194], 'Q715': [25, 226], 'Q724': [210, 266], 'Q735': [172, 190], 'Q740': [132, 142], 'Q743': [2, 8], 'Q744': [94, 244], 'Q750': [174, 203], 'Q753': [30, 31], 'Q755': [22, 24], 'Q756': [83, 111], 'Q757': [97, 142], 'Q762': [147, 194], 'Q763': [131, 142], 'Q769': [1118, 1333], 'Q800': [241, 423], 'Q802': [3, 134], 'Q817': [28, 28], 'Q820': [99, 209], 'Q821': [28, 79], 'Q825': [41, 65], 'Q838': [3, 29], 'Q844': [382, 405], 'Q850': [80, 180], 'Q855': [3, 128], 'Q857': [124, 142], 'Q858': [251, 258], 'Q864': [362, 467], 'Q865': [7, 7], 'Q870': [147, 264], 'Q871': [3, 19], 'Q878': [138, 263], 'Q882': [44, 114], 'Q889': [85, 156], 'Q894': [18, 89], 'Q897': [101, 394], 'Q900': [55, 118], 'Q904': [259, 259], 'Q906': [30, 30], 'Q907': [21, 66], 'Q909': [147, 173], 'Q910': [17, 24], 'Q911': [93, 93], 'Q917': [5, 12], 'Q926': [68, 303], 'Q929': [45, 201], 'Q930': [113, 150], 'Q933': [42, 155], 'Q934': [115, 121], 'Q935': [108, 113], 'Q947': [18, 20], 'Q952': [279, 282], 'Q956': [77, 80], 'Q964': [230, 299], 'Q967': [2, 5], 'Q971': [106, 140], 'Q980': [12, 13], 'Q983': [18, 27], 'Q984': [287, 487], 'Q990': [125, 134], 'Q992': [177, 177], 'Q995': [6, 42], 'Q1012': [386, 456], 'Q1018': [36, 51], 'Q1021': [33, 227], 'Q1027': [29, 185], 'Q1032': [48, 58], 'Q1033': [233, 243], 'Q1064': [14, 190], 'Q1065': [53, 79], 'Q1067': [169, 176], 'Q1070': [81, 94], 'Q1074': [106, 106], 'Q1077': [265, 277], 'Q1078': [45, 84], 'Q1089': [125, 126], 'Q1093': [134, 134], 'Q1100': [49, 62], 'Q1102': [19, 115], 'Q1103': [125, 242], 'Q1115': [28, 114], 'Q1116': [37, 54], 'Q1118': [12, 110], 'Q1126': [5, 107], 'Q1131': [28, 72], 'Q1132': [3, 3], 'Q1135': [111, 118], 'Q1146': [195, 234], 'Q1147': [16, 65], 'Q1148': [62, 77], 'Q1151': [42, 95], 'Q1152': [271, 386], 'Q1154': [57, 298], 'Q1157': [51, 58], 'Q1159': [105, 107], 'Q1163': [80, 336], 'Q1175': [5, 28], 'Q1184': [48, 56], 'Q1191': [61, 73], 'Q1195': [200, 250], 'Q1196': [34, 44], 'Q1197': [71, 79], 'Q1198': [34, 34], 'Q1203': [104, 169], 'Q1206': [27, 69], 'Q1211': [358, 380], 'Q1212': [49, 49], 'Q1213': [20, 27], 'Q1216': [16, 219], 'Q1223': [44, 100], 'Q1226': [105, 106], 'Q1228': [85, 141], 'Q1230': [1, 60], 'Q1233': [13, 218], 'Q1236': [13, 88], 'Q1240': [182, 241], 'Q1246': [88, 129], 'Q1261': [50, 63], 'Q1264': [227, 433], 'Q1268': [24, 106], 'Q1269': [8, 8], 'Q1275': [1, 25], 'Q1277': [16, 43], 'Q1288': [53, 89], 'Q1297': [236, 286], 'Q1311': [36, 36], 'Q1312': [42, 46], 'Q1316': [40, 74], 'Q1325': [22, 76], 'Q1326': [2, 6], 'Q1338': [239, 285], 'Q1345': [200, 420], 'Q1346': [0, 18], 'Q1355': [41, 52], 'Q1358': [26, 28], 'Q1379': [2, 12], 'Q1383': [26, 43], 'Q1389': [2, 80], 'Q1391': [39, 40], 'Q1406': [34, 193], 'Q1409': [128, 270], 'Q1416': [130, 155], 'Q1418': [236, 279], 'Q1434': [81, 82], 'Q1436': [57, 96], 'Q1440': [8, 15], 'Q1442': [35, 95], 'Q1444': [227, 271], 'Q1446': [262, 269], 'Q1460': [48, 76], 'Q1465': [178, 431], 'Q1466': [18, 52], 'Q1477': [0, 0], 'Q1478': [185, 196], 'Q1485': [56, 262], 'Q1486': [108, 118], 'Q1487': [63, 102], 'Q1499': [12, 99], 'Q1506': [20, 22], 'Q1510': [154, 238], 'Q1511': [1, 35], 'Q1515': [217, 314], 'Q1516': [386, 455], 'Q1517': [5, 97], 'Q1533': [25, 29], 'Q1550': [1, 61], 'Q1551': [0, 0], 'Q1552': [27, 143], 'Q1554': [12, 39], 'Q1556': [21, 21], 'Q1559': [122, 216], 'Q1561': [16, 23], 'Q1562': [58, 70], 'Q1563': [52, 52], 'Q1564': [58, 62], 'Q1569': [21, 116], 'Q1572': [81, 173], 'Q1580': [118, 185], 'Q1582': [65, 66], 'Q1583': [115, 169], 'Q1585': [13, 20], 'Q1586': [43, 58], 'Q1597': [1, 44], 'Q1599': [13, 114], 'Q1602': [74, 106], 'Q1608': [0, 55], 'Q1613': [36, 77], 'Q1616': [57, 198], 'Q1619': [291, 348], 'Q1631': [8, 60], 'Q1633': [158, 242], 'Q1644': [23, 74], 'Q1645': [60, 141], 'Q1650': [3, 5], 'Q1655': [43, 61], 'Q1656': [10, 18], 'Q1658': [135, 137], 'Q1661': [77, 117], 'Q1662': [10, 214], 'Q1665': [153, 199], 'Q1673': [5, 46], 'Q1675': [278, 420], 'Q1685': [123, 127], 'Q1687': [26, 75], 'Q1688': [123, 169], 'Q1689': [44, 79], 'Q1695': [60, 60], 'Q1696': [15, 20], 'Q1697': [202, 326], 'Q1707': [125, 129], 'Q1712': [34, 48], 'Q1714': [98, 157], 'Q1722': [6, 62], 'Q1733': [54, 253], 'Q1736': [24, 37], 'Q1739': [127, 174], 'Q1749': [0, 63], 'Q1758': [393, 479], 'Q1759': [37, 41], 'Q1760': [187, 188], 'Q1772': [6, 40], 'Q1780': [72, 86], 'Q1792': [24, 167], 'Q1799': [89, 109], 'Q1803': [75, 94], 'Q1805': [15, 27], 'Q1806': [0, 155], 'Q1813': [13, 30], 'Q1814': [178, 310], 'Q1815': [19, 172], 'Q1817': [42, 101], 'Q1821': [53, 79], 'Q1823': [20, 175], 'Q1841': [268, 339], 'Q1842': [125, 156], 'Q1857': [137, 137], 'Q1860': [30, 33], 'Q1863': [45, 47], 'Q1865': [24, 115], 'Q1867': [12, 29], 'Q1868': [0, 115], 'Q1870': [246, 246], 'Q1871': [39, 46], 'Q1879': [55, 65], 'Q1882': [4, 97], 'Q1884': [80, 97], 'Q1892': [88, 127], 'Q1898': [21, 70], 'Q1899': [329, 359], 'Q1905': [15, 67], 'Q1913': [101, 101], 'Q1916': [160, 174], 'Q1917': [9, 9], 'Q1918': [41, 43], 'Q1922': [90, 90], 'Q1928': [158, 183], 'Q1931': [59, 145], 'Q1933': [187, 258], 'Q1935': [164, 229], 'Q1937': [12, 29], 'Q1944': [4, 14], 'Q1955': [14, 19], 'Q1957': [74, 86], 'Q1963': [28, 29], 'Q1966': [399, 663], 'Q1970': [62, 93], 'Q1973': [35, 46], 'Q1983': [153, 164], 'Q1984': [5, 7], 'Q1987': [10, 12], 'Q1992': [35, 115], 'Q1994': [99, 193], 'Q1995': [33, 71], 'Q2004': [23, 66], 'Q2009': [77, 208], 'Q2011': [10, 268], 'Q2028': [91, 118], 'Q2029': [33, 94], 'Q2034': [486, 488], 'Q2035': [2, 4], 'Q2041': [16, 58], 'Q2047': [135, 138], 'Q2051': [40, 40], 'Q2058': [8, 136], 'Q2061': [24, 148], 'Q2062': [37, 42], 'Q2065': [23, 31], 'Q2073': [39, 89], 'Q2078': [104, 243], 'Q2080': [123, 141], 'Q2095': [169, 238], 'Q2100': [0, 245], 'Q2101': [153, 160], 'Q2116': [73, 89], 'Q2121': [1, 21], 'Q2122': [2, 2], 'Q2128': [44, 71], 'Q2131': [0, 0], 'Q2140': [95, 152], 'Q2144': [63, 63], 'Q2148': [53, 67], 'Q2153': [0, 5], 'Q2169': [0, 73], 'Q2180': [115, 125], 'Q2183': [79, 101], 'Q2184': [411, 434], 'Q2185': [95, 193], 'Q2189': [230, 236], 'Q2198': [75, 89], 'Q2199': [102, 103], 'Q2200': [11, 30], 'Q2201': [228, 378], 'Q2203': [28, 421], 'Q2214': [5, 7], 'Q2216': [87, 111], 'Q2226': [99, 99], 'Q2227': [3, 9], 'Q2228': [6, 11], 'Q2236': [175, 188], 'Q2240': [31, 98], 'Q2245': [233, 243], 'Q2259': [57, 102], 'Q2261': [22, 63], 'Q2263': [15, 15], 'Q2274': [64, 64], 'Q2275': [21, 44], 'Q2279': [36, 50], 'Q2283': [85, 86], 'Q2286': [54, 83], 'Q2304': [180, 242], 'Q2308': [11, 91], 'Q2312': [1, 54], 'Q2323': [44, 174], 'Q2326': [0, 7], 'Q2346': [22, 55], 'Q2349': [52, 57], 'Q2352': [17, 87], 'Q2359': [179, 179], 'Q2365': [18, 49], 'Q2374': [45, 108], 'Q2375': [14, 35], 'Q2377': [48, 56], 'Q2382': [61, 259], 'Q2383': [41, 168], 'Q2385': [19, 28], 'Q2386': [71, 72], 'Q2395': [27, 91], 'Q2408': [130, 179], 'Q2411': [41, 93], 'Q2416': [5, 107], 'Q2417': [22, 35], 'Q2421': [305, 337], 'Q2424': [52, 92], 'Q2425': [23, 40], 'Q2428': [13, 58], 'Q2433': [28, 50], 'Q2435': [32, 86], 'Q2438': [42, 45], 'Q2443': [6, 6], 'Q2445': [425, 457], 'Q2447': [12, 57], 'Q2453': [167, 207], 'Q2462': [173, 180], 'Q2466': [43, 43], 'Q2487': [18, 392], 'Q2495': [222, 222], 'Q2498': [29, 74], 'Q2499': [215, 271], 'Q2500': [41, 175], 'Q2509': [127, 193], 'Q2511': [28, 40], 'Q2521': [10, 21], 'Q2537': [44, 226], 'Q2539': [47, 96], 'Q2549': [13, 286], 'Q2551': [1, 11], 'Q2555': [10, 35], 'Q2561': [2, 18], 'Q2567': [3, 14], 'Q2574': [46, 89], 'Q2575': [190, 247], 'Q2582': [11, 14], 'Q2590': [48, 61], 'Q2592': [29, 29], 'Q2603': [184, 230], 'Q2605': [39, 52], 'Q2606': [83, 226], 'Q2608': [65, 90], 'Q2610': [100, 149], 'Q2613': [12, 36], 'Q2615': [100, 150], 'Q2618': [205, 226], 'Q2621': [126, 344], 'Q2622': [20, 29], 'Q2623': [8, 9], 'Q2634': [22, 24], 'Q2636': [109, 126], 'Q2637': [75, 147], 'Q2640': [8, 103], 'Q2658': [45, 52], 'Q2662': [32, 79], 'Q2663': [78, 124], 'Q2675': [93, 111], 'Q2677': [221, 242], 'Q2679': [44, 95], 'Q2682': [35, 121], 'Q2684': [39, 112], 'Q2685': [73, 74], 'Q2687': [147, 173], 'Q2688': [32, 78], 'Q2698': [81, 81], 'Q2704': [247, 258], 'Q2709': [69, 80], 'Q2714': [148, 153], 'Q2720': [8, 15], 'Q2723': [38, 95], 'Q2727': [61, 97], 'Q2730': [24, 42], 'Q2734': [19, 124], 'Q2737': [27, 54], 'Q2738': [206, 335], 'Q2739': [23, 95], 'Q2741': [251, 415], 'Q2753': [55, 59], 'Q2766': [20, 20], 'Q2780': [6, 34], 'Q2788': [364, 373], 'Q2801': [210, 216], 'Q2802': [56, 104], 'Q2803': [75, 189], 'Q2806': [51, 66], 'Q2810': [469, 537], 'Q2811': [3, 7], 'Q2812': [0, 1], 'Q2815': [40, 44], 'Q2822': [73, 101], 'Q2823': [60, 63], 'Q2834': [26, 35], 'Q2836': [162, 257], 'Q2839': [24, 44], 'Q2841': [40, 395], 'Q2842': [199, 199], 'Q2843': [332, 365], 'Q2848': [24, 103], 'Q2859': [342, 420], 'Q2860': [79, 273], 'Q2868': [71, 94], 'Q2880': [186, 295], 'Q2884': [308, 312], 'Q2892': [21, 64], 'Q2896': [36, 46], 'Q2897': [116, 204], 'Q2902': [113, 140], 'Q2903': [182, 403], 'Q2921': [232, 340], 'Q2924': [102, 137], 'Q2931': [1, 17], 'Q2933': [38, 619], 'Q2934': [38, 55], 'Q2935': [82, 128], 'Q2939': [293, 329], 'Q2940': [86, 115], 'Q2942': [1, 8], 'Q2945': [11, 33], 'Q2947': [45, 58], 'Q2952': [30, 30], 'Q2953': [22, 28], 'Q2954': [61, 87], 'Q2960': [39, 60], 'Q2964': [5, 5], 'Q2965': [43, 47], 'Q2970': [87, 92], 'Q2973': [26, 84], 'Q2980': [275, 280], 'Q2981': [28, 39], 'Q2986': [54, 82], 'Q2990': [91, 136], 'Q2994': [6, 22], 'Q3004': [2, 9], 'Q3006': [54, 54], 'Q3008': [5, 61], 'Q3012': [45, 111], 'Q3013': [303, 322], 'Q3015': [71, 142], 'Q3026': [7, 8], 'Q3045': [65, 111]}\n",
            "Epoch train loss : inf| Time: 1m 7s\n",
            "Epoch valid loss: inf\n",
            "Epoch Precision: 0.08551190829996237\n",
            "Epoch Recall: 0.11626689154195645\n",
            "Epoch F1: 0.06922644645169189\n",
            "====================================================================================\n",
            "Epoch 5\n",
            "Starting training ........\n",
            "Starting batch: 0\n",
            "Starting validation .........\n",
            "Starting batch 0\n",
            "{'Q0': [97, 127], 'Q3': [149, 187], 'Q4': [25, 66], 'Q20': [31, 37], 'Q33': [67, 267], 'Q47': [217, 227], 'Q49': [159, 265], 'Q54': [21, 50], 'Q57': [131, 159], 'Q59': [32, 109], 'Q64': [165, 251], 'Q68': [291, 326], 'Q69': [135, 168], 'Q72': [56, 56], 'Q79': [10, 19], 'Q81': [160, 197], 'Q86': [111, 217], 'Q88': [49, 57], 'Q91': [33, 301], 'Q102': [64, 69], 'Q105': [25, 26], 'Q113': [291, 291], 'Q115': [14, 30], 'Q117': [24, 187], 'Q123': [2, 2], 'Q131': [27, 303], 'Q132': [62, 79], 'Q144': [0, 0], 'Q146': [92, 112], 'Q147': [125, 145], 'Q149': [225, 277], 'Q152': [22, 50], 'Q153': [42, 107], 'Q157': [40, 40], 'Q165': [0, 0], 'Q182': [0, 121], 'Q186': [38, 88], 'Q194': [17, 130], 'Q196': [0, 5], 'Q198': [0, 58], 'Q202': [75, 78], 'Q205': [133, 137], 'Q216': [79, 159], 'Q221': [0, 59], 'Q222': [81, 401], 'Q224': [162, 310], 'Q232': [72, 267], 'Q236': [239, 245], 'Q242': [22, 23], 'Q243': [227, 269], 'Q248': [0, 9], 'Q254': [66, 83], 'Q262': [107, 195], 'Q265': [24, 43], 'Q269': [185, 193], 'Q271': [19, 107], 'Q279': [247, 253], 'Q285': [22, 216], 'Q292': [61, 178], 'Q297': [8, 117], 'Q300': [294, 357], 'Q309': [151, 269], 'Q312': [139, 139], 'Q318': [34, 123], 'Q322': [71, 75], 'Q323': [32, 39], 'Q324': [52, 74], 'Q329': [50, 62], 'Q331': [52, 110], 'Q333': [160, 194], 'Q337': [45, 49], 'Q340': [74, 75], 'Q344': [3, 5], 'Q345': [20, 29], 'Q355': [15, 136], 'Q361': [38, 61], 'Q362': [167, 171], 'Q367': [6, 6], 'Q376': [208, 230], 'Q383': [31, 65], 'Q384': [17, 109], 'Q386': [80, 91], 'Q388': [62, 177], 'Q391': [15, 19], 'Q392': [96, 162], 'Q394': [158, 159], 'Q397': [0, 103], 'Q404': [135, 145], 'Q410': [111, 143], 'Q411': [107, 112], 'Q414': [30, 89], 'Q415': [24, 25], 'Q430': [6, 8], 'Q432': [207, 211], 'Q434': [2, 85], 'Q436': [30, 35], 'Q448': [7, 28], 'Q451': [180, 287], 'Q452': [54, 86], 'Q453': [0, 2], 'Q455': [151, 240], 'Q461': [160, 306], 'Q462': [1, 89], 'Q463': [37, 72], 'Q486': [60, 67], 'Q498': [95, 327], 'Q506': [8, 16], 'Q508': [67, 81], 'Q519': [1, 19], 'Q523': [21, 124], 'Q526': [118, 122], 'Q530': [82, 252], 'Q540': [19, 124], 'Q542': [2, 27], 'Q547': [3, 148], 'Q553': [1, 1], 'Q554': [4, 36], 'Q565': [30, 31], 'Q568': [198, 227], 'Q575': [22, 54], 'Q581': [8, 83], 'Q583': [69, 79], 'Q595': [191, 192], 'Q597': [78, 180], 'Q599': [216, 218], 'Q607': [1, 189], 'Q611': [35, 72], 'Q615': [111, 149], 'Q638': [20, 49], 'Q656': [78, 85], 'Q658': [194, 196], 'Q664': [57, 68], 'Q676': [360, 419], 'Q677': [113, 194], 'Q680': [202, 228], 'Q682': [47, 156], 'Q683': [97, 161], 'Q685': [28, 32], 'Q688': [61, 67], 'Q692': [122, 140], 'Q698': [4, 82], 'Q701': [396, 483], 'Q706': [32, 56], 'Q710': [72, 81], 'Q711': [164, 194], 'Q715': [25, 226], 'Q724': [210, 266], 'Q735': [172, 190], 'Q740': [132, 142], 'Q743': [2, 8], 'Q744': [94, 244], 'Q750': [174, 203], 'Q753': [30, 31], 'Q755': [22, 24], 'Q756': [83, 111], 'Q757': [97, 142], 'Q762': [147, 194], 'Q763': [131, 142], 'Q769': [1118, 1333], 'Q800': [241, 423], 'Q802': [3, 134], 'Q817': [28, 28], 'Q820': [99, 209], 'Q821': [28, 79], 'Q825': [41, 65], 'Q838': [3, 29], 'Q844': [382, 405], 'Q850': [80, 180], 'Q855': [3, 128], 'Q857': [124, 142], 'Q858': [251, 258], 'Q864': [362, 467], 'Q865': [7, 7], 'Q870': [147, 264], 'Q871': [3, 19], 'Q878': [138, 263], 'Q882': [44, 114], 'Q889': [85, 156], 'Q894': [18, 89], 'Q897': [101, 394], 'Q900': [55, 118], 'Q904': [259, 259], 'Q906': [30, 30], 'Q907': [21, 66], 'Q909': [147, 173], 'Q910': [17, 24], 'Q911': [93, 93], 'Q917': [5, 12], 'Q926': [68, 303], 'Q929': [45, 201], 'Q930': [113, 150], 'Q933': [42, 155], 'Q934': [115, 121], 'Q935': [108, 113], 'Q947': [18, 20], 'Q952': [279, 282], 'Q956': [77, 80], 'Q964': [230, 299], 'Q967': [2, 5], 'Q971': [106, 140], 'Q980': [12, 13], 'Q983': [18, 27], 'Q984': [287, 487], 'Q990': [125, 134], 'Q992': [177, 177], 'Q995': [6, 42], 'Q1012': [386, 456], 'Q1018': [36, 51], 'Q1021': [33, 227], 'Q1027': [29, 185], 'Q1032': [48, 58], 'Q1033': [233, 243], 'Q1064': [14, 190], 'Q1065': [53, 79], 'Q1067': [169, 176], 'Q1070': [81, 94], 'Q1074': [106, 106], 'Q1077': [265, 277], 'Q1078': [45, 84], 'Q1089': [125, 126], 'Q1093': [134, 134], 'Q1100': [49, 62], 'Q1102': [19, 115], 'Q1103': [125, 242], 'Q1115': [28, 114], 'Q1116': [37, 54], 'Q1118': [12, 110], 'Q1126': [5, 107], 'Q1131': [28, 72], 'Q1132': [3, 3], 'Q1135': [111, 118], 'Q1146': [195, 234], 'Q1147': [16, 65], 'Q1148': [62, 77], 'Q1151': [42, 95], 'Q1152': [271, 386], 'Q1154': [57, 298], 'Q1157': [51, 58], 'Q1159': [105, 107], 'Q1163': [80, 336], 'Q1175': [5, 28], 'Q1184': [48, 56], 'Q1191': [61, 73], 'Q1195': [200, 250], 'Q1196': [34, 44], 'Q1197': [71, 79], 'Q1198': [34, 34], 'Q1203': [104, 169], 'Q1206': [27, 69], 'Q1211': [358, 380], 'Q1212': [49, 49], 'Q1213': [20, 27], 'Q1216': [16, 219], 'Q1223': [44, 100], 'Q1226': [105, 106], 'Q1228': [85, 141], 'Q1230': [1, 60], 'Q1233': [13, 218], 'Q1236': [13, 88], 'Q1240': [182, 241], 'Q1246': [88, 129], 'Q1261': [50, 63], 'Q1264': [227, 433], 'Q1268': [24, 106], 'Q1269': [8, 8], 'Q1275': [1, 25], 'Q1277': [16, 43], 'Q1288': [53, 89], 'Q1297': [236, 286], 'Q1311': [36, 36], 'Q1312': [42, 46], 'Q1316': [40, 74], 'Q1325': [22, 76], 'Q1326': [2, 6], 'Q1338': [239, 285], 'Q1345': [200, 420], 'Q1346': [0, 18], 'Q1355': [41, 52], 'Q1358': [26, 28], 'Q1379': [2, 12], 'Q1383': [26, 43], 'Q1389': [2, 80], 'Q1391': [39, 40], 'Q1406': [34, 193], 'Q1409': [128, 270], 'Q1416': [130, 155], 'Q1418': [236, 279], 'Q1434': [81, 82], 'Q1436': [57, 96], 'Q1440': [8, 15], 'Q1442': [35, 95], 'Q1444': [227, 271], 'Q1446': [262, 269], 'Q1460': [48, 76], 'Q1465': [178, 431], 'Q1466': [18, 52], 'Q1477': [0, 0], 'Q1478': [185, 196], 'Q1485': [56, 262], 'Q1486': [108, 118], 'Q1487': [63, 102], 'Q1499': [12, 99], 'Q1506': [20, 22], 'Q1510': [154, 238], 'Q1511': [1, 35], 'Q1515': [217, 314], 'Q1516': [386, 455], 'Q1517': [5, 97], 'Q1533': [25, 29], 'Q1550': [1, 61], 'Q1551': [0, 0], 'Q1552': [27, 143], 'Q1554': [12, 39], 'Q1556': [21, 21], 'Q1559': [122, 216], 'Q1561': [16, 23], 'Q1562': [58, 70], 'Q1563': [52, 52], 'Q1564': [58, 62], 'Q1569': [21, 116], 'Q1572': [81, 173], 'Q1580': [118, 185], 'Q1582': [65, 66], 'Q1583': [115, 169], 'Q1585': [13, 20], 'Q1586': [43, 58], 'Q1597': [1, 44], 'Q1599': [13, 114], 'Q1602': [74, 106], 'Q1608': [0, 55], 'Q1613': [36, 77], 'Q1616': [57, 198], 'Q1619': [291, 348], 'Q1631': [8, 60], 'Q1633': [158, 242], 'Q1644': [23, 74], 'Q1645': [60, 141], 'Q1650': [3, 5], 'Q1655': [43, 61], 'Q1656': [10, 18], 'Q1658': [135, 137], 'Q1661': [77, 117], 'Q1662': [10, 214], 'Q1665': [153, 199], 'Q1673': [5, 46], 'Q1675': [278, 420], 'Q1685': [123, 127], 'Q1687': [26, 75], 'Q1688': [123, 169], 'Q1689': [44, 79], 'Q1695': [60, 60], 'Q1696': [15, 20], 'Q1697': [202, 326], 'Q1707': [125, 129], 'Q1712': [34, 48], 'Q1714': [98, 157], 'Q1722': [6, 62], 'Q1733': [54, 253], 'Q1736': [24, 37], 'Q1739': [127, 174], 'Q1749': [0, 63], 'Q1758': [393, 479], 'Q1759': [37, 41], 'Q1760': [187, 188], 'Q1772': [6, 40], 'Q1780': [72, 86], 'Q1792': [24, 167], 'Q1799': [89, 109], 'Q1803': [75, 94], 'Q1805': [15, 27], 'Q1806': [0, 155], 'Q1813': [13, 30], 'Q1814': [178, 310], 'Q1815': [19, 172], 'Q1817': [42, 101], 'Q1821': [53, 79], 'Q1823': [20, 175], 'Q1841': [268, 339], 'Q1842': [125, 156], 'Q1857': [137, 137], 'Q1860': [30, 33], 'Q1863': [45, 47], 'Q1865': [24, 115], 'Q1867': [12, 29], 'Q1868': [0, 115], 'Q1870': [246, 246], 'Q1871': [39, 46], 'Q1879': [55, 65], 'Q1882': [4, 97], 'Q1884': [80, 97], 'Q1892': [88, 127], 'Q1898': [21, 70], 'Q1899': [329, 359], 'Q1905': [15, 67], 'Q1913': [101, 101], 'Q1916': [160, 174], 'Q1917': [9, 9], 'Q1918': [41, 43], 'Q1922': [90, 90], 'Q1928': [158, 183], 'Q1931': [59, 145], 'Q1933': [187, 258], 'Q1935': [164, 229], 'Q1937': [12, 29], 'Q1944': [4, 14], 'Q1955': [14, 19], 'Q1957': [74, 86], 'Q1963': [28, 29], 'Q1966': [399, 663], 'Q1970': [62, 93], 'Q1973': [35, 46], 'Q1983': [153, 164], 'Q1984': [5, 7], 'Q1987': [10, 12], 'Q1992': [35, 115], 'Q1994': [99, 193], 'Q1995': [33, 71], 'Q2004': [23, 66], 'Q2009': [77, 208], 'Q2011': [10, 268], 'Q2028': [91, 118], 'Q2029': [33, 94], 'Q2034': [486, 488], 'Q2035': [2, 4], 'Q2041': [16, 58], 'Q2047': [135, 138], 'Q2051': [40, 40], 'Q2058': [8, 136], 'Q2061': [24, 148], 'Q2062': [37, 42], 'Q2065': [23, 31], 'Q2073': [39, 89], 'Q2078': [104, 243], 'Q2080': [123, 141], 'Q2095': [169, 238], 'Q2100': [0, 245], 'Q2101': [153, 160], 'Q2116': [73, 89], 'Q2121': [1, 21], 'Q2122': [2, 2], 'Q2128': [44, 71], 'Q2131': [0, 0], 'Q2140': [95, 152], 'Q2144': [63, 63], 'Q2148': [53, 67], 'Q2153': [0, 5], 'Q2169': [0, 73], 'Q2180': [115, 125], 'Q2183': [79, 101], 'Q2184': [411, 434], 'Q2185': [95, 193], 'Q2189': [230, 236], 'Q2198': [75, 89], 'Q2199': [102, 103], 'Q2200': [11, 30], 'Q2201': [228, 378], 'Q2203': [28, 421], 'Q2214': [5, 7], 'Q2216': [87, 111], 'Q2226': [99, 99], 'Q2227': [3, 9], 'Q2228': [6, 11], 'Q2236': [175, 188], 'Q2240': [31, 98], 'Q2245': [233, 243], 'Q2259': [57, 102], 'Q2261': [22, 63], 'Q2263': [15, 15], 'Q2274': [64, 64], 'Q2275': [21, 44], 'Q2279': [36, 50], 'Q2283': [85, 86], 'Q2286': [54, 83], 'Q2304': [180, 242], 'Q2308': [11, 91], 'Q2312': [1, 54], 'Q2323': [44, 174], 'Q2326': [0, 7], 'Q2346': [22, 55], 'Q2349': [52, 57], 'Q2352': [17, 87], 'Q2359': [179, 179], 'Q2365': [18, 49], 'Q2374': [45, 108], 'Q2375': [14, 35], 'Q2377': [48, 56], 'Q2382': [61, 259], 'Q2383': [41, 168], 'Q2385': [19, 28], 'Q2386': [71, 72], 'Q2395': [27, 91], 'Q2408': [130, 179], 'Q2411': [41, 93], 'Q2416': [5, 107], 'Q2417': [22, 35], 'Q2421': [305, 337], 'Q2424': [52, 92], 'Q2425': [23, 40], 'Q2428': [13, 58], 'Q2433': [28, 50], 'Q2435': [32, 86], 'Q2438': [42, 45], 'Q2443': [6, 6], 'Q2445': [425, 457], 'Q2447': [12, 57], 'Q2453': [167, 207], 'Q2462': [173, 180], 'Q2466': [43, 43], 'Q2487': [18, 392], 'Q2495': [222, 222], 'Q2498': [29, 74], 'Q2499': [215, 271], 'Q2500': [41, 175], 'Q2509': [127, 193], 'Q2511': [28, 40], 'Q2521': [10, 21], 'Q2537': [44, 226], 'Q2539': [47, 96], 'Q2549': [13, 286], 'Q2551': [1, 11], 'Q2555': [10, 35], 'Q2561': [2, 18], 'Q2567': [3, 14], 'Q2574': [46, 89], 'Q2575': [190, 247], 'Q2582': [11, 14], 'Q2590': [48, 61], 'Q2592': [29, 29], 'Q2603': [184, 230], 'Q2605': [39, 52], 'Q2606': [83, 226], 'Q2608': [65, 90], 'Q2610': [100, 149], 'Q2613': [12, 36], 'Q2615': [100, 150], 'Q2618': [205, 226], 'Q2621': [126, 344], 'Q2622': [20, 29], 'Q2623': [8, 9], 'Q2634': [22, 24], 'Q2636': [109, 126], 'Q2637': [75, 147], 'Q2640': [8, 103], 'Q2658': [45, 52], 'Q2662': [32, 79], 'Q2663': [78, 124], 'Q2675': [93, 111], 'Q2677': [221, 242], 'Q2679': [44, 95], 'Q2682': [35, 121], 'Q2684': [39, 112], 'Q2685': [73, 74], 'Q2687': [147, 173], 'Q2688': [32, 78], 'Q2698': [81, 81], 'Q2704': [247, 258], 'Q2709': [69, 80], 'Q2714': [148, 153], 'Q2720': [8, 15], 'Q2723': [38, 95], 'Q2727': [61, 97], 'Q2730': [24, 42], 'Q2734': [19, 124], 'Q2737': [27, 54], 'Q2738': [206, 335], 'Q2739': [23, 95], 'Q2741': [251, 415], 'Q2753': [55, 59], 'Q2766': [20, 20], 'Q2780': [6, 34], 'Q2788': [364, 373], 'Q2801': [210, 216], 'Q2802': [56, 104], 'Q2803': [75, 189], 'Q2806': [51, 66], 'Q2810': [469, 537], 'Q2811': [3, 7], 'Q2812': [0, 1], 'Q2815': [40, 44], 'Q2822': [73, 101], 'Q2823': [60, 63], 'Q2834': [26, 35], 'Q2836': [162, 257], 'Q2839': [24, 44], 'Q2841': [40, 395], 'Q2842': [199, 199], 'Q2843': [332, 365], 'Q2848': [24, 103], 'Q2859': [342, 420], 'Q2860': [79, 273], 'Q2868': [71, 94], 'Q2880': [186, 295], 'Q2884': [308, 312], 'Q2892': [21, 64], 'Q2896': [36, 46], 'Q2897': [116, 204], 'Q2902': [113, 140], 'Q2903': [182, 403], 'Q2921': [232, 340], 'Q2924': [102, 137], 'Q2931': [1, 17], 'Q2933': [38, 619], 'Q2934': [38, 55], 'Q2935': [82, 128], 'Q2939': [293, 329], 'Q2940': [86, 115], 'Q2942': [1, 8], 'Q2945': [11, 33], 'Q2947': [45, 58], 'Q2952': [30, 30], 'Q2953': [22, 28], 'Q2954': [61, 87], 'Q2960': [39, 60], 'Q2964': [5, 5], 'Q2965': [43, 47], 'Q2970': [87, 92], 'Q2973': [26, 84], 'Q2980': [275, 280], 'Q2981': [28, 39], 'Q2986': [54, 82], 'Q2990': [91, 136], 'Q2994': [6, 22], 'Q3004': [2, 9], 'Q3006': [54, 54], 'Q3008': [5, 61], 'Q3012': [45, 111], 'Q3013': [303, 322], 'Q3015': [71, 142], 'Q3026': [7, 8], 'Q3045': [65, 111]}\n",
            "Epoch train loss : inf| Time: 1m 9s\n",
            "Epoch valid loss: inf\n",
            "Epoch Precision: 0.08551190829996237\n",
            "Epoch Recall: 0.11626689154195645\n",
            "Epoch F1: 0.06922644645169189\n",
            "====================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "ems = []\n",
        "f1s = []\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_loader)\n",
        "    valid_loss, precision, recall, f1 = valid(model, test_loader)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"Epoch valid loss: {valid_loss}\")\n",
        "    print(f\"Epoch Precision: {precision}\")\n",
        "    print(f\"Epoch Recall: {recall}\")\n",
        "    print(f\"Epoch F1: {f1}\")\n",
        "    print(\"====================================================================================\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cTNGfO0h9I3W"
      },
      "source": [
        "### 3.1. Input Embedding Ablation Study\n",
        "\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEVsyvrc9VHL"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uX7nFwMo9WBE"
      },
      "source": [
        "### 3.2. Attention Ablation Study\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfRK-BeiNSVi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0VAR8GF9hSD"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "llzGjUe6NDnB"
      },
      "source": [
        "### 3.3. Hyper Parameter Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xj4PNyrNDBH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
