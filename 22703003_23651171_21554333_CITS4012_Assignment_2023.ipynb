{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2023 CITS4012 Assignment\n",
        "*Make sure you change the file name with your student id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please check the bottom of the this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\New\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Necessary Library Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "# Read in data files\n",
        "test_data = pd.read_csv(\"WikiQA-test.tsv\", sep=\"\\t\")\n",
        "train_data = pd.read_csv(\"WikiQA-train.tsv\", sep=\"\\t\")\n",
        "\n",
        "def wrangle_data(df):\n",
        "    \"\"\"Handles data and splits dataframe into question, document and answer set\n",
        "\n",
        "    df: dataframe to split\n",
        "\n",
        "    return: question, document, and answer dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Split dataframe into relevant fields for Question data\n",
        "    question = df[['QuestionID','Question','DocumentID']].drop_duplicates()\n",
        "\n",
        "\n",
        "    # Initialise lists containing all documents and answers\n",
        "    documents = []\n",
        "    answers = {}\n",
        "\n",
        "    # Iterate through dataframe and form documents data\n",
        "    for index, row in df.iterrows():\n",
        "\n",
        "        # Initalise current document pairs list and previous doc number (only for first iteration)\n",
        "        if index == 0:\n",
        "            previous_doc_num = row['DocumentID']\n",
        "            current_doc_pairs = []\n",
        "\n",
        "        # If the current doc number is different to the previous doc number, then a new  \n",
        "        # document has been found so append the current document pairs to the list of documents     \n",
        "        # and reinitilise the current document pairs list\n",
        "        current_doc_num = row['DocumentID']\n",
        "        if current_doc_num != previous_doc_num:\n",
        "            documents.append(current_doc_pairs)\n",
        "            current_doc_pairs = []\n",
        "        \n",
        "        # Tokenise the current row's sentence\n",
        "        sent_tokens = word_tokenize(row['Sentence'])\n",
        "\n",
        "        # Assign token types according to 'Label' field (1 means the corresponding sentence\n",
        "        # is the answer/one of the answers)\n",
        "        # Token types:\n",
        "        #   0: token a part of the answer\n",
        "        #   1: start token of the answer\n",
        "        #   2: inner token of the answer\n",
        "        #   3: end token of the answer\n",
        "        if row['Label']:\n",
        "            # Answer to question is found\n",
        "            token_types = np.full(shape=len(sent_tokens), fill_value=2)\n",
        "            token_types[0] = 1\n",
        "            token_types[-1] = 3\n",
        "\n",
        "            # Add the tokenised answer to the answers dictionary. If an answer has already been found.\n",
        "            # add the tokens to the current answer.\n",
        "            if row['QuestionID'] in answers:\n",
        "                answers[row['QuestionID']] += sent_tokens \n",
        "            else:\n",
        "                answers[row['QuestionID']] = sent_tokens\n",
        "        else:\n",
        "            # Answer not found \n",
        "            token_types = np.zeros(len(sent_tokens), dtype=np.int)\n",
        "\n",
        "        # Combine token and corresponding types and append to current document list\n",
        "        token_type_pairs = list(zip(sent_tokens, token_types))\n",
        "        current_doc_pairs.append(token_type_pairs)\n",
        "\n",
        "        # Update previous document number \n",
        "        previous_doc_num = current_doc_num\n",
        "\n",
        "        # If the iterated row is the final row in the dataframe, append the current document\n",
        "        # to the documents array\n",
        "        if index == len(df)-1:\n",
        "            documents.append(current_doc_pairs)\n",
        "\n",
        "    return question, documents, answers\n",
        "\n",
        "train_questions, train_documents, train_answers = wrangle_data(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for debugging - find all labels equal to 1\n",
        "train_data.loc[train_data['Label'] == 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2.QA Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Model Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTNGfO0h9I3W"
      },
      "source": [
        "###3.1. Input Embedding Ablation Study\n",
        "\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEVsyvrc9VHL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX7nFwMo9WBE"
      },
      "source": [
        "###3.2. Attention Ablation Study\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfRK-BeiNSVi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0VAR8GF9hSD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llzGjUe6NDnB"
      },
      "source": [
        "###3.3. Hyper Parameter Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xj4PNyrNDBH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
